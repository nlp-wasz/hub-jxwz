import os
import re
import json
import time
import numpy as np
import pandas as pd
import warnings
import faiss
import markdown
import faiss
# å¯¼å…¥åº“
from sentence_transformers import SentenceTransformer
from openai import OpenAI

class RAGSystem:
    """RAGé—®ç­”ç³»ç»Ÿä¸»ç±»"""

    def __init__(self):
        self.chunks = []
        self.embeddings = None
        self.index = None
        self.embedding_model = None
        self.client = None

    def setup_environment(self):

        # åˆå§‹åŒ–æ¨¡å‹
        print("ğŸ“¥ æ­£åœ¨åŠ è½½BGE embeddingæ¨¡å‹...")
        self.embedding_model = SentenceTransformer('BAAI/bge-small-zh-v1.5')
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼å‘é‡ç»´åº¦: {self.embedding_model.get_sentence_embedding_dimension()}")

        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        if 'OPENAI_API_KEY' not in os.environ:
            api_key = input("è¯·è¾“å…¥DeepSeek APIå¯†é’¥: ")
            os.environ['OPENAI_API_KEY'] = api_key
            os.environ['OPENAI_BASE_URL'] = 'https://api.deepseek.com'

        self.client = OpenAI()
        print("âœ… DeepSeek APIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼")

        return True

    def load_document(self, file_path):
        """åŠ è½½æ–‡æ¡£"""
        print(f"ğŸ“„ æ­£åœ¨åŠ è½½æ–‡æ¡£: {file_path}")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # æ¸…ç†æ–‡æœ¬
            content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
            content = '\n'.join(line.strip() for line in content.split('\n'))
            content = content.strip()

            print(f"âœ… æ–‡æ¡£åŠ è½½æˆåŠŸï¼é•¿åº¦: {len(content)} å­—ç¬¦")
            return content

        except Exception as e:
            print(f"âŒ æ–‡æ¡£åŠ è½½å¤±è´¥: {e}")
            return None

    def chunk_text(self, text, chunk_size=512, chunk_overlap=50):
        """æ–‡æœ¬åˆ†å—"""
        print("ğŸ”ª æ­£åœ¨è¿›è¡Œæ™ºèƒ½æ–‡æœ¬åˆ†å—...")

        chunks = []
        paragraphs = text.split('\n\n')
        current_chunk = ""

        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            # æ£€æŸ¥æ·»åŠ æ®µè½åæ˜¯å¦ä¼šè¶…è¿‡å—å¤§å°
            test_chunk = current_chunk + "\n\n" + paragraph if current_chunk else paragraph

            if len(test_chunk) <= chunk_size:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
            else:
                # ä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append({
                        'text': current_chunk,
                        'metadata': {
                            'length': len(current_chunk),
                            'paragraph_count': len(current_chunk.split('\n\n'))
                        }
                    })

                # å¼€å§‹æ–°å—
                current_chunk = paragraph

        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append({
                'text': current_chunk,
                'metadata': {
                    'length': len(current_chunk),
                    'paragraph_count': len(current_chunk.split('\n\n'))
                }
            })

        print(f"âœ… åˆ†å—å®Œæˆï¼å…± {len(chunks)} ä¸ªæ–‡æ¡£å—")

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        chunk_lengths = [chunk['metadata']['length'] for chunk in chunks]
        print(f"ğŸ“Š åˆ†å—ç»Ÿè®¡:")
        print(f"   å¹³å‡é•¿åº¦: {np.mean(chunk_lengths):.1f} å­—ç¬¦")
        print(f"   æœ€å¤§é•¿åº¦: {max(chunk_lengths)} å­—ç¬¦")
        print(f"   æœ€å°é•¿åº¦: {min(chunk_lengths)} å­—ç¬¦")

        self.chunks = chunks
        return chunks

    def build_vector_index(self):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        print("ğŸ” æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•...")

        if not self.chunks:
            print("âŒ è¯·å…ˆè¿›è¡Œæ–‡æœ¬åˆ†å—ï¼")
            return False

        # æå–æ–‡æœ¬
        texts = [chunk['text'] for chunk in self.chunks]

        # ç”Ÿæˆå‘é‡
        start_time = time.time()
        print("ğŸ“Š æ­£åœ¨ç”Ÿæˆæ–‡æœ¬å‘é‡...")
        self.embeddings = self.embedding_model.encode(
            texts,
            batch_size=32,
            show_progress_bar=True,
            normalize_embeddings=True
        )
        end_time = time.time()

        print(f"âœ… å‘é‡ç”Ÿæˆå®Œæˆï¼è€—æ—¶: {end_time - start_time:.2f}ç§’")
        print(f"   å‘é‡ç»´åº¦: {self.embeddings.shape}")

        # åˆ›å»ºFAISSç´¢å¼•

        dimension = self.embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ç´¢å¼•
        self.index.add(self.embeddings.astype('float32'))

        print(f"âœ… FAISSç´¢å¼•æ„å»ºå®Œæˆï¼ç´¢å¼•å¤§å°: {self.index.ntotal} å‘é‡")
        return True

    def search_similar(self, query, k=5):
        """æœç´¢ç›¸ä¼¼å†…å®¹"""
        if self.index is None:
            print("âŒ è¯·å…ˆæ„å»ºå‘é‡ç´¢å¼•ï¼")
            return []

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_model.encode([query], normalize_embeddings=True)

        # æœç´¢
        scores, indices = self.index.search(query_embedding.astype('float32'), k)

        # æ„å»ºç»“æœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx >= 0 and idx < len(self.chunks):
                chunk = self.chunks[idx]
                results.append({
                    'chunk': chunk,
                    'score': float(score),
                    'index': int(idx)
                })

        return results

    def generate_answer(self, question, max_context_length=2000):
        """ç”Ÿæˆç­”æ¡ˆ"""
        print(f"ğŸ¤” æ­£åœ¨å›ç­”é—®é¢˜: {question}")

        # æ£€ç´¢ç›¸å…³å†…å®¹
        search_results = self.search_similar(question, k=5)

        if not search_results:
            return {
                'question': question,
                'answer': 'æŠ±æ­‰ï¼Œåœ¨æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚',
                'sources': [],
                'response_time': 0
            }

        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        current_length = 0

        for i, result in enumerate(search_results):
            chunk_text = result['chunk']['text']
            formatted_chunk = f"[æ¥æº{i+1}]\n{chunk_text}\n"

            if current_length + len(formatted_chunk) <= max_context_length:
                context_parts.append(formatted_chunk)
                current_length += len(formatted_chunk)
            else:
                break

        context = "\n".join(context_parts)

        # åˆ›å»ºæç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæŠ€æœ¯æ–‡æ¡£åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹æä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹:
{context}

ç”¨æˆ·é—®é¢˜: {question}

è¯·æ ¹æ®æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼Œè¦æ±‚:
1. ç­”æ¡ˆå¿…é¡»åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰æ¡ç†
4. é€‚å½“å¼•ç”¨æ–‡æ¡£ä¸­çš„å…·ä½“æ•°æ®å’ŒæŠ€æœ¯ç»†èŠ‚
5. ä½¿ç”¨ä¸­æ–‡å›ç­”

ç­”æ¡ˆ:"""

        # è°ƒç”¨APIç”Ÿæˆç­”æ¡ˆ
        start_time = time.time()

        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæŠ€æœ¯æ–‡æ¡£åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )

            answer = response.choices[0].message.content

        except Exception as e:
            answer = f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}"

        end_time = time.time()

        return {
            'question': question,
            'answer': answer,
            'sources': [
                {
                    'score': result['score'],
                    'snippet': result['chunk']['text'][:200] + "..."
                }
                for result in search_results[:3]
            ],
            'response_time': end_time - start_time
        }

    def run_test(self):
        """è¿è¡Œæµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸ§ª å¼€å§‹è¿è¡Œæµ‹è¯•ç”¨ä¾‹")
        print("="*60)

        test_questions = [
            "DeepSeek-V3çš„æ€»å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ",
            "DeepSeek-V3ä½¿ç”¨äº†å“ªäº›ä¼˜åŒ–æŠ€æœ¯ï¼Ÿ",
            "è®­ç»ƒæˆæœ¬å¦‚ä½•ï¼Ÿ",
            "æ¨¡å‹çš„æ€§èƒ½è¡¨ç°å¦‚ä½•ï¼Ÿ"
        ]

        results = []

        for i, question in enumerate(test_questions, 1):
            print(f"\nğŸ“ æµ‹è¯• {i}/{len(test_questions)}: {question}")
            print("-" * 50)

            result = self.generate_answer(question)
            results.append(result)

            print(f"ğŸ’¬ ç­”æ¡ˆ: {result['answer']}")
            print(f"â±ï¸  å“åº”æ—¶é—´: {result['response_time']:.2f}ç§’")

            if result['sources']:
                print(f"ğŸ“š ç›¸å…³æ¥æº:")
                for j, source in enumerate(result['sources']):
                    print(f"   {j+1}. ç›¸ä¼¼åº¦: {source['score']:.4f}")

        print(f"\n{'='*60}")
        print("âœ… æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {np.mean([r['response_time'] for r in results]):.2f}ç§’")
        print(f"   æœ€å¿«å“åº”æ—¶é—´: {min([r['response_time'] for r in results]):.2f}ç§’")
        print(f"   æœ€æ…¢å“åº”æ—¶é—´: {max([r['response_time'] for r in results]):.2f}ç§’")
        print("="*60)

        return results

    def interactive_mode(self):
        """äº¤äº’æ¨¡å¼"""
        print("\nğŸš€ DeepSeek-V3æŠ€æœ¯æŠ¥å‘Šé—®ç­”ç³»ç»Ÿ")
        print("="*50)
        print("è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œè¾“å…¥ 'quit' é€€å‡º")
        print("="*50)

        while True:
            try:
                query = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()

                if query.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼")
                    break

                elif not query:
                    print("è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ï¼")
                    continue

                result = self.generate_answer(query)

                print(f"\nğŸ’¬ ç­”æ¡ˆ: {result['answer']}")
                print(f"â±ï¸  å“åº”æ—¶é—´: {result['response_time']:.2f}ç§’")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ RAGé—®ç­”ç³»ç»Ÿ - DeepSeek-V3æŠ€æœ¯æŠ¥å‘Š")
    print("="*60)

    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag_system = RAGSystem()

    # ç¯å¢ƒé…ç½®
    if not rag_system.setup_environment():
        print("âŒ ç¯å¢ƒé…ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¾èµ–åŒ…å®‰è£…")
        return

    # åŠ è½½æ–‡æ¡£
    doc_path = "2412-DeepSeek-V3.md"
    if not os.path.exists(doc_path):
        print(f"âŒ æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨: {doc_path}")
        print("è¯·ç¡®ä¿DeepSeek-V3æŠ€æœ¯æŠ¥å‘Šæ–‡æ¡£åœ¨å½“å‰ç›®å½•ä¸‹")
        return

    content = rag_system.load_document(doc_path)
    if not content:
        return

    # æ–‡æœ¬åˆ†å—
    rag_system.chunk_text(content)

    # æ„å»ºå‘é‡ç´¢å¼•
    if not rag_system.build_vector_index():
        return

    # è¿è¡Œæµ‹è¯•
    test_results = rag_system.run_test()

    # è¯¢é—®æ˜¯å¦è¿›å…¥äº¤äº’æ¨¡å¼
    user_input = input("\næ˜¯å¦è¿›å…¥äº¤äº’é—®ç­”æ¨¡å¼ï¼Ÿ(y/n): ").strip().lower()
    if user_input in ['y', 'yes', 'æ˜¯']:
        rag_system.interactive_mode()

    print("\nğŸ‰ RAGé—®ç­”ç³»ç»Ÿè¿è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    main()