import streamlit as st
import hashlib
import os
# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥è§£å†³ Pydantic v2 å…¼å®¹æ€§é—®é¢˜
os.environ["LANGCHAIN_ALLOW_DUPLICATE_VALIDATORS"] = "true"
from dotenv import load_dotenv
from langchain_text_splitters import MarkdownHeaderTextSplitter
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
# ä» pymilvus å¯¼å…¥ MilvusClient
from pymilvus import MilvusClient
# ä½¿ç”¨ SentenceTransformer ä½œä¸ºæœ¬åœ°åµŒå…¥æ¨¡å‹
from sentence_transformers import SentenceTransformer
import tempfile
import re
import numpy as np

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# åˆå§‹åŒ–æ¨¡å‹å’ŒåµŒå…¥
@st.cache_resource
def init_resources():
    # ä½¿ç”¨æœ¬åœ° BGE æ¨¡å‹ä½œä¸ºåµŒå…¥æ¨¡å‹ï¼Œé¿å… API è´¹ç”¨
    embeddings = SentenceTransformer(r'D:\learning\å…«æ–—\models\bge-small-zh-v1.5')
    llm = ChatOpenAI(
        model="qwen-max",
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    return embeddings, llm


# åˆå§‹åŒ–Milvuså‘é‡æ•°æ®åº“è¿æ¥ï¼ˆå‚è€ƒWeek11é¡¹ç›®çš„æ–¹å¼ï¼‰
@st.cache_resource
def init_milvus():
    # ä½¿ç”¨ä¸Week11é¡¹ç›®ç›¸åŒçš„è¿æ¥é…ç½®
    client = MilvusClient(
        uri=os.getenv("MILVUS_URI"),
        token=os.getenv("MILVUS_TOKEN")
    )
    
    # ç¡®ä¿é›†åˆå­˜åœ¨
    collections = client.list_collections()
    if "w15" not in collections:
        # å®šä¹‰é›†åˆæ¨¡å¼
        schema = {
            "fields": [
                {"name": "id", "type": "INT64", "is_primary": True, "auto_id": True},
                {"name": "vector", "type": "FLOAT_VECTOR", "dim": 512},  # BGE æ¨¡å‹çš„ç»´åº¦
                {"name": "text", "type": "VARCHAR", "max_length": 65535},
                {"name": "source", "type": "VARCHAR", "max_length": 256},
                {"name": "seq_num", "type": "INT64"}
            ]
        }
        # åˆ›å»ºé›†åˆ
        client.create_collection(
            collection_name="w15",
            schema=schema
        )
        
        # åˆ›å»ºå‘é‡ç´¢å¼•
        index_params = {
            "field_name": "vector",
            "index_type": "AUTOINDEX",
            "metric_type": "COSINE"
        }
        client.create_index(
            collection_name="w15",
            index_params=index_params
        )
        
        # åŠ è½½é›†åˆ
        client.load_collection(collection_name="w15")
    
    return client


# å¤„ç†ä¸Šä¼ çš„Markdownæ–‡ä»¶
def process_markdown_file(file_content, filename):
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
        ("####", "Header 4"),
    ]

    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    md_header_splits = markdown_splitter.split_text(file_content)

    # ä¸ºæ¯ä¸ªåˆ†å‰²æ·»åŠ æ–‡ä»¶åå…ƒæ•°æ®
    docs = []
    for i, split in enumerate(md_header_splits):
        doc = Document(
            page_content=split.page_content,
            metadata={
                "source": filename,
                "seq_num": i,
                **split.metadata
            }
        )
        docs.append(doc)

    return docs


# ç”Ÿæˆæ–‡æ¡£å”¯ä¸€æ ‡è¯†ç¬¦
def generate_doc_id(content):
    return hashlib.md5(content.encode()).hexdigest()


# å°†æ–‡æ¡£æ·»åŠ åˆ°Milvusæ•°æ®åº“
def add_documents_to_milvus(client, embeddings, docs):
    # ä¸ºæ–‡æ¡£ç”ŸæˆåµŒå…¥å‘é‡
    texts = [doc.page_content for doc in docs]
    embeddings_list = embeddings.encode(texts)
    
    # å‡†å¤‡æ’å…¥åˆ°Milvusçš„æ•°æ®
    data = []
    for i, (doc, embedding) in enumerate(zip(docs, embeddings_list)):
        data.append({
            "vector": embedding.tolist(),  # è½¬æ¢ä¸ºåˆ—è¡¨
            "text": doc.page_content,
            "source": doc.metadata.get("source", ""),
            "seq_num": doc.metadata.get("seq_num", 0)
        })
    
    # æ’å…¥æ•°æ®åˆ°Milvus
    result = client.insert(
        collection_name="w15",
        data=data
    )
    
    # åˆ·æ–°é›†åˆä»¥ç¡®ä¿æ•°æ®å¯è§
    client.flush(collection_name="w15")
    
    return result


# ä»Milvusæ£€ç´¢ç›¸å…³æ–‡æ¡£
def search_documents(client, embeddings, query, top_k=4):
    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    query_embedding = embeddings.encode(query).tolist()
    
    # æ‰§è¡Œæœç´¢
    results = client.search(
        collection_name="w15",
        data=[query_embedding],
        limit=top_k,
        output_fields=["text", "source", "seq_num"]
    )
    
    # å¤„ç†æœç´¢ç»“æœ
    docs = []
    for result in results[0]:
        doc = Document(
            page_content=result["entity"]["text"],
            metadata={
                "source": result["entity"]["source"],
                "seq_num": result["entity"]["seq_num"]
            }
        )
        docs.append(doc)
    
    return docs


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


def main():
    st.set_page_config(page_title="Markdown RAGé—®ç­”ç³»ç»Ÿ", layout="wide")
    st.title("ğŸ“˜ Markdownæ–‡æ¡£RAGé—®ç­”ç³»ç»Ÿ")

    # ä¾§è¾¹æ 
    st.sidebar.header("æ“ä½œé¢æ¿")

    # åˆå§‹åŒ–èµ„æº
    embeddings, llm = init_resources()
    milvus_client = init_milvus()

    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_files = st.sidebar.file_uploader(
        "ä¸Šä¼ Markdownæ–‡ä»¶",
        type=["md"],
        accept_multiple_files=True
    )

    if uploaded_files:
        with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶..."):
            all_docs = []
            for uploaded_file in uploaded_files:
                content = uploaded_file.read().decode('utf-8')
                docs = process_markdown_file(content, uploaded_file.name)
                all_docs.extend(docs)

                st.success(f"å·²å¤„ç†æ–‡ä»¶: {uploaded_file.name} ({len(docs)} ä¸ªç‰‡æ®µ)")

            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            if all_docs:
                with st.spinner("æ­£åœ¨å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“..."):
                    result = add_documents_to_milvus(milvus_client, embeddings, all_docs)
                    st.success(f"å·²æˆåŠŸå­˜å‚¨ {len(all_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µåˆ°å‘é‡æ•°æ®åº“! æ’å…¥ID: {result['ids']}")

    # æ˜¾ç¤ºé›†åˆä¿¡æ¯
    try:
        collections = milvus_client.list_collections()
        if "w15" in collections:
            stats = milvus_client.get_collection_stats(collection_name="w15")
            st.sidebar.info(f"å½“å‰é›†åˆ: w15")
            st.sidebar.info(f"é›†åˆå®ä½“æ•°: {stats.get('row_count', 0)}")
        else:
            st.sidebar.info("é›†åˆ w15 å°šæœªåˆ›å»º")
    except Exception as e:
        st.sidebar.warning(f"æ— æ³•è·å–é›†åˆä¿¡æ¯: {str(e)}")

    # æŸ¥è¯¢è¾“å…¥
    st.subheader("ğŸ’¬ è¯¢é—®å…³äºæ–‡æ¡£çš„é—®é¢˜")
    query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜:", placeholder="ä¾‹å¦‚: è¿™äº›æ–‡æ¡£ä¸»è¦è®²äº†ä»€ä¹ˆå†…å®¹?")

    if query:
        with st.spinner("æ­£åœ¨æ£€ç´¢å’Œç”Ÿæˆç­”æ¡ˆ..."):
            try:
                # æ£€ç´¢ç›¸å…³æ–‡æ¡£
                retrieved_docs = search_documents(milvus_client, embeddings, query)

                # æ„å»ºä¸Šä¸‹æ–‡
                context = format_docs(retrieved_docs)

                # æ„å»ºæç¤ºè¯
                template = """
                ä½ æ˜¯é—®ç­”åŠ©æ‰‹ï¼ŒåŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
                å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•ä»æ–‡æ¡£ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚

                ä¸Šä¸‹æ–‡:
                {context}

                é—®é¢˜: {question}

                å›ç­”:
                """

                prompt = ChatPromptTemplate.from_template(template)

                # æ„å»ºRAGé“¾
                rag_chain = (
                    {"context": lambda x: context, "question": RunnablePassthrough()}
                    | prompt
                    | llm
                    | StrOutputParser()
                )

                # ç”Ÿæˆå›ç­”
                response = rag_chain.invoke(query)

                # æ˜¾ç¤ºå›ç­”
                st.markdown("### ğŸ¤– å›ç­”")
                st.write(response)

                # æ˜¾ç¤ºå‚è€ƒæ–‡æ¡£
                st.markdown("### ğŸ“š å‚è€ƒæ–‡æ¡£")
                for i, doc in enumerate(retrieved_docs):
                    with st.expander(f"å‚è€ƒæ–‡æ¡£ #{i + 1}"):
                        st.markdown(f"**æ¥æº:** {doc.metadata.get('source', 'Unknown')}")
                        st.markdown(f"**å†…å®¹:**\n\n{doc.page_content}")

            except Exception as e:
                st.error(f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}")

    # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“ ä½¿ç”¨è¯´æ˜")
    st.sidebar.markdown("""
    1. åœ¨å·¦ä¾§ä¸Šä¼ Markdownæ–‡ä»¶
    2. ç³»ç»Ÿä¼šè‡ªåŠ¨è§£æå¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
    3. è¾“å…¥é—®é¢˜è·å–åŸºäºæ–‡æ¡£çš„å›ç­”
    4. å¯ä»¥æŸ¥çœ‹å‚è€ƒçš„æ–‡æ¡£ç‰‡æ®µ
    """)


if __name__ == "__main__":
    main()