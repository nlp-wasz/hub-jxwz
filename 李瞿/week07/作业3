import json
import torch
from transformers import (
    BertTokenizerFast,
    BertForQuestionAnswering,
    TrainingArguments,
    Trainer,
    DefaultDataCollator
)
from datasets import Dataset
import numpy as np
from peft import LoraConfig, TaskType, get_peft_model

# 1.加载数据
train = json.load(open('./cmrc2018_public/train.json', 'r', encoding='utf-8'))
dev = json.load(open('./cmrc2018_public/dev.json', encoding='utf-8'))

# 2.加载模型
# 初始化tokenizer和模型、加载预训练模型。网络结构和参数都复用。
tokenizer = BertTokenizerFast.from_pretrained('../models/google-bert/bert-base-chinese')
model = BertForQuestionAnswering.from_pretrained('../models/google-bert/bert-base-chinese')

# 3.配置LoRA
def setup_lora(model):
    """设置LoRA配置并应用到模型"""
    config = LoraConfig(
        task_type=TaskType.QUESTION_ANS,  # 任务类型，问答任务
        target_modules=["query", "value"],  # 对注意力机制的query和value进行微调
        inference_mode=False,
        r=8,  # LoRA的秩
        lora_alpha=32,  # LoRA的缩放因子
        lora_dropout=0.1  # LoRA的丢弃率
    )

    model = get_peft_model(model, config)
    model.print_trainable_parameters()
    return model

# 应用LoRA到模型
model = setup_lora(model)

# 输入 11 token -》13 token -》 13*768 -》 13*2 对应回答在原文的位置（通过开头和结尾来确定）

# 准备训练数据
# 一个上下文，可以提多个问题，一个问题可以有多个答案和多个答案起始位置。
def prepare_dataset(data):
    paragraphs = []
    questions = []
    answers = []

    for paragraph in data['data']:
        context = paragraph['paragraphs'][0]['context']
        for qa in paragraph['paragraphs'][0]['qas']:
            paragraphs.append(context)
            questions.append(qa['question'])  # 一个问题
            answers.append({  # 多个答案
                'answer_start': [qa['answers'][0]['answer_start']],  # 答案的位置对应也可能包含多个
                'text': [qa['answers'][0]['text']]  # 答案可能包含多个
            })

    return paragraphs, questions, answers


# 3.准备训练和验证数据
train_paragraphs, train_questions, train_answers = prepare_dataset(train)
val_paragraphs, val_questions, val_answers = prepare_dataset(dev)

# 创建数据集字典
train_dataset_dict = {
    'context': train_paragraphs[:100],
    'question': train_questions[:100],
    'answers': train_answers[:100]
}

val_dataset_dict = {
    'context': val_paragraphs[:10],
    'question': val_questions[:10],
    'answers': val_answers[:10]
}

# 转换为Hugging Face Dataset
train_dataset = Dataset.from_dict(train_dataset_dict)
val_dataset = Dataset.from_dict(val_dataset_dict)


# 预处理函数
def preprocess_function(examples):
    questions = [q.strip() for q in examples["question"]]  # 问题
    contexts = [c.strip() for c in examples["context"]]  # 上下文

    # Tokenize
    # Tokenizer 处理后的结构
    # tokens = [
    #     '[CLS]',           # 特殊标记
    #     '中国', '的', '首都', '在', '哪里', '？',  # 问题内容
    #     '[SEP]',           # 特殊标记
    #     '中国', '是', '一个', '拥有', '五千年', '历史', '的', '国家', '。',
    #     '中国', '的', '首都', '是', '北京', '。',  # 上下文内容
    #     '[SEP]'            # 特殊标记
    # ]
    tokenized_examples = tokenizer(
        questions,
        contexts,
        truncation="only_second",
        max_length=512,
        stride=128,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    # 由于可能有溢出，需要重新映射样本
    # sample_mapping 一个数组，例如 [0, 0, 1, 1, 2] 表示每个token序列片段对应原始数据中的第几个样本
    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
    # offset_mapping 三维数组，例如 [[[0,2], [2,5], [5,8]], [[10,12], [12,15], ...]]
    # 表示每个token对应的原始文本字符范围
    offset_mapping = tokenized_examples.pop("offset_mapping")

    tokenized_examples["start_positions"] = []
    tokenized_examples["end_positions"] = []

    for i, offsets in enumerate(offset_mapping):
        # 获取对应的原始样本的索引
        sample_index = sample_mapping[i]
        # "answers": [
        #                 {
        #                   "text": "赤经16时54分，赤纬-41度48分",
        #                   "answer_start": 27
        #                 }
        #            ]
        # 这里的answers在处理过后，只保留了一个答案和答案起始位置
        answer = examples["answers"][sample_index]

        # 如果没有答案，设置默认值
        if len(answer["answer_start"]) == 0:
            tokenized_examples["start_positions"].append(0)
            tokenized_examples["end_positions"].append(0)
            continue

        # 获取答案的起始和结束位置
        start_char = answer["answer_start"][0]
        end_char = start_char + len(answer["text"][0])

        # # 对应的 sequence_ids
        # sequence_ids = [
        #     None,              # [CLS] -> None
        #     0, 0, 0, 0, 0, 0,  # 问题token -> 0
        #     None,              # [SEP] -> None
        #     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  # 上下文token -> 1
        #     None               # [SEP] -> None
        # ]
        # 找到token的起始和结束位置
        sequence_ids = tokenized_examples.sequence_ids(i)
        # 找到context的开始和结束。上下文的开始和结束。
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while idx < len(sequence_ids) and sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # 如果答案完全在context之外，标记为不可回答。
        # offset_mapping[i]：这是找到第一个文本的所有数据的偏移量。
        # offset_mapping[i][context_start]：这是找我们tokenize后的找到的上下文起始位置和终止位置。
        # offset_mapping[i][context_start][0]：这是找我们tokenize后的找到的上下文起始位置。
        # 如果编码后的上下文起始位置 比 问题的终止位置大 并且 编码后的上下文终止位置 比 问题的起始位置小。则认为没有答案。
        if offset_mapping[i][context_start][0] > end_char or offset_mapping[i][context_end][1] < start_char:
            tokenized_examples["start_positions"].append(0)
            tokenized_examples["end_positions"].append(0)
        else:
            # 否则找到答案的token位置
            # 如果是答案的尾部在上下文中，这里取的是上下文的起始位置。作为答案的开始位置。
            idx = context_start
            while idx <= context_end and offset_mapping[i][idx][0] <= start_char:
                idx += 1
            start_position = idx - 1

            # 如果是答案的起始位置在上下文中，这里取的是上下文的终止位置。作为答案的结束位置。
            idx = context_end
            while idx >= context_start and offset_mapping[i][idx][1] >= end_char:
                idx -= 1
            end_position = idx + 1

            tokenized_examples["start_positions"].append(start_position)
            tokenized_examples["end_positions"].append(end_position)

    return tokenized_examples


# 应用预处理
tokenized_train_dataset = train_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=train_dataset.column_names,
)

tokenized_val_dataset = val_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=val_dataset.column_names,
)

# 设置训练参数 - 优化LoRA训练
training_args = TrainingArguments(
    output_dir="./qa-bert-lora-model",
    learning_rate=1e-4,  # LoRA通常使用更高的学习率
    per_device_train_batch_size=16,  # 可以使用更大的batch size
    per_device_eval_batch_size=16,
    num_train_epochs=10,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=50,  # 更频繁的日志记录
    save_strategy="epoch",
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    report_to="none",
    warmup_ratio=0.1,  # 添加warmup
    save_total_limit=2,  # 限制保存模型数量
)

# 数据收集器
data_collator = DefaultDataCollator()

# 创建Trainer
trainer = Trainer(
    model=model,  # 使用LoRA微调的模型
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# 开始训练
print("开始训练QA模型(LoRA微调)...")
trainer.train()

# 保存模型
trainer.save_model()
tokenizer.save_pretrained('./qa-bert-lora-model')

# 评估模型
print("评估模型...")
eval_results = trainer.evaluate()
print(f"评估结果: {eval_results}")


# 预测函数
def predict(context, question):
    model.to('cpu')

    # Tokenize输入
    inputs = tokenizer(
        question,
        context,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    )

    # 预测
    with torch.no_grad():
        outputs = model(**inputs)

    # 获取预测的起始和结束位置
    start_logits = outputs.start_logits
    end_logits = outputs.end_logits

    # 找到最可能的答案跨度
    start_idx = torch.argmax(start_logits, dim=1).item()
    end_idx = torch.argmax(end_logits, dim=1).item()

    # 将token位置转换为字符位置
    all_tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
    answer_tokens = all_tokens[start_idx:end_idx + 1]

    # 将token转换回文本
    answer = tokenizer.convert_tokens_to_string(answer_tokens)

    # 清理答案
    answer = answer.replace(" ", "").replace("##", "")

    return answer


# 在验证集上测试几个样本
print("\n在验证集上测试:")
for i in range(min(3, len(val_paragraphs))):
    context = val_paragraphs[i]
    question = val_questions[i]
    expected_answer = val_answers[i]['text'][0]

    predicted_answer = predict(context, question)

    print(f"问题 {i + 1}: {question}")
    print(f"预期答案: {expected_answer}")
    print(f"预测答案: {predicted_answer}")
    print(f"匹配: {expected_answer == predicted_answer}")
    print()
