import os
import torch
from torch import nn
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast, GradScaler
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
from tqdm import tqdm

# ============================================================
# 1ï¸âƒ£ é…ç½®è·¯å¾„
# ============================================================
DATA_DIR = "../flickr8k/Flicker8k_Dataset"
CAPTION_FILE = "../flickr8k/Flickr8k.token.txt"

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")

# ============================================================
# 2ï¸âƒ£ è§£æ Flickr8k æ•°æ®
# ============================================================
class Flickr8kDataset(Dataset):
    def __init__(self, data_dir, caption_file, processor, max_samples=5000):
        self.data_dir = data_dir
        self.processor = processor
        self.samples = []

        with open(caption_file, "r") as f:
            for line in f:
                img_id, caption = line.strip().split("\t")
                img_name = img_id.split("#")[0]
                img_path = os.path.join(data_dir, img_name)
                if os.path.exists(img_path):
                    self.samples.append((img_path, caption))
                if len(self.samples) >= max_samples:
                    break

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, caption = self.samples[idx]
        image = Image.open(img_path).convert("RGB")
        return {"image": image, "text": caption}

# ============================================================
# 3ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
# ============================================================
model_name = "../../../models/AI-ModelScope/chinese-clip-vit-base-patch16"
model = CLIPModel.from_pretrained(model_name).to(device)
processor = CLIPProcessor.from_pretrained(model_name)

optimizer = AdamW(model.parameters(), lr=1e-5)
scaler = GradScaler(device="cuda")

# ============================================================
# 4ï¸âƒ£ åˆ›å»º DataLoader
# ============================================================
dataset = Flickr8kDataset(DATA_DIR, CAPTION_FILE, processor)
print(f"âœ… æ ·æœ¬æ•°é‡: {len(dataset)}")

def collate_fn(batch):
    texts = [item["text"] for item in batch]
    images = [item["image"] for item in batch]
    inputs = processor(text=texts, images=images, return_tensors="pt", padding=True)
    inputs.pop("token_type_ids", None)    #åˆ é™¤CLIP Modelä¸éœ€è¦çš„å­—æ®µ
    return inputs

dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)

# ============================================================
# 5ï¸âƒ£ è®­ç»ƒï¼ˆä»…ç¤ºä¾‹1è½®ï¼‰
# ============================================================
model.train()
for epoch in range(1):
    for batch in tqdm(dataloader):
        inputs = {k: v.to(device) for k, v in batch.items()}
        optimizer.zero_grad()

        with autocast(device_type="cuda", dtype=torch.float16):
            outputs = model(**inputs,return_loss=True)
            loss = outputs.loss

        #print(type(outputs))
        #print(outputs.keys())

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

    print(f"Epoch done. Loss={loss.item():.4f}")

# ============================================================
# 6ï¸âƒ£ ä¿å­˜æ¨¡å‹
# ============================================================
model.save_pretrained("./clip_finetuned_flickr8k")
processor.save_pretrained("./clip_finetuned_flickr8k")
print("âœ… æ¨¡å‹ä¿å­˜åˆ° ./clip_finetuned_flickr8k")

# ============================================================
# 7ï¸âƒ£ æ¨ç†ç¤ºä¾‹
# ============================================================
model.eval()
texts = ["A man playing guitar", "A dog running in the park", "A group of people sitting on grass"]

# é€‰å–ä¸€éƒ¨åˆ†å›¾ç‰‡ï¼ˆä¾‹å¦‚å‰ 100 å¼ ï¼‰
num_images = 100
images = [Image.open(dataset.samples[i][0]).convert("RGB") for i in range(num_images)]
image_paths = [dataset.samples[i][0] for i in range(num_images)]

# è·å–æ–‡æœ¬ä¸å›¾ç‰‡ç‰¹å¾
with torch.no_grad(), autocast(device_type="cuda", dtype=torch.float16):
    # 1ï¸âƒ£ æ–‡æœ¬ç‰¹å¾
    text_inputs = processor(text=texts, return_tensors="pt", padding=True)
    text_inputs.pop("token_type_ids", None)
    text_inputs = {k: v.to(device) for k, v in text_inputs.items()}
    text_features = model.get_text_features(**text_inputs)

    # 2ï¸âƒ£ å›¾åƒç‰¹å¾
    image_inputs = processor(images=images, return_tensors="pt", padding=True)
    image_inputs.pop("token_type_ids", None)
    image_inputs = {k: v.to(device) for k, v in image_inputs.items()}
    image_features = model.get_image_features(**image_inputs)

# ç‰¹å¾å½’ä¸€åŒ–ï¼ˆCLIP é»˜è®¤å¯¹æ¯”ç›¸ä¼¼åº¦è®¡ç®—å‰è¦ normalizeï¼‰
text_features = nn.functional.normalize(text_features, dim=-1)
image_features = nn.functional.normalize(image_features, dim=-1)

# ç›¸ä¼¼åº¦çŸ©é˜µï¼š [num_texts, num_images]
similarity = text_features @ image_features.T

# è¾“å‡ºæ¯ä¸ªæ–‡æœ¬æœ€åŒ¹é…çš„å›¾ç‰‡
for i, text in enumerate(texts):
    best_img_idx = similarity[i].argmax().item()
    best_img_path = image_paths[best_img_idx]
    print(f"ğŸ“ æ–‡æœ¬: '{text}'")
    print(f"ğŸï¸ æœ€ç›¸ä¼¼å›¾ç‰‡: {best_img_path}")
    print(f"ğŸ”¢ ç›¸ä¼¼åº¦: {similarity[i, best_img_idx]:.4f}")
    print("-" * 60)
