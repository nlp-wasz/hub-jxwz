import os
from pathlib import Path
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
import torch
from PIL import Image
import time

def analyze_images_in_folder(folder_path="./pics"):
    """
    æ‰¹é‡åˆ†ææŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾ç‰‡
    """
    # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    folder = Path(folder_path)
    if not folder.exists():
        print(f"é”™è¯¯ï¼šæ–‡ä»¶å¤¹ {folder_path} ä¸å­˜åœ¨")
        return
    
    # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}
    
    # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    image_files = []
    for ext in image_extensions:
        image_files.extend(folder.glob(f"1_*{ext}"))
        image_files.extend(folder.glob(f"1_*{ext.upper()}"))
    
    if not image_files:
        print(f"åœ¨ {folder_path} ä¸­æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
    try:
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            "../../../models/Qwen/Qwen2-VL-7B-Instruct",
            dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        processor = AutoProcessor.from_pretrained(
            "../../../models/Qwen/Qwen2-VL-7B-Instruct",
            trust_remote_code=True
        )
        print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # åˆ†ææ¯å¼ å›¾ç‰‡
    for i, image_path in enumerate(sorted(image_files), 1):
        print(f"\n{'='*50}")
        print(f"åˆ†æç¬¬ {i} å¼ å›¾ç‰‡: {image_path.name}")
        print(f"{'='*50}")
        
        try:
            # æ‰“å¼€å›¾ç‰‡
            image = Image.open(image_path)
            
            # æ„å»ºåˆ†æè¯·æ±‚
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": "è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„ä¸»è¦ç‰©ä½“ã€åœºæ™¯å’Œå†…å®¹ã€‚åŒ…æ‹¬ï¼š1.ä¸»è¦ç‰©ä½“æ˜¯ä»€ä¹ˆ 2.åœºæ™¯ç±»å‹ 3.é¢œè‰²å’Œæ°›å›´ 4.å…¶ä»–æ˜¾è‘—ç‰¹å¾"}
                    ]
                }
            ]
            
            # å¤„ç†è¾“å…¥
            text = processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            inputs = processor(
                text=[text],
                images=[image],
                padding=True,
                return_tensors="pt"
            ).to(model.device)
            
            # ç”Ÿæˆåˆ†æç»“æœ
            print("æ­£åœ¨åˆ†æä¸­...")
            start_time = time.time()
            
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=512,
                do_sample=False,
                temperature=0.1
            )
            
            # è§£ç ç»“æœ
            generated_ids_trimmed = generated_ids[0][len(inputs.input_ids[0]):]
            analysis_result = processor.decode(
                generated_ids_trimmed, 
                skip_special_tokens=True
            )
            
            end_time = time.time()
            print(f"åˆ†æå®Œæˆ (è€—æ—¶: {end_time - start_time:.2f}ç§’)")
            print(f"\nåˆ†æç»“æœ:\n{analysis_result}")
            
        except Exception as e:
            print(f"åˆ†æå›¾ç‰‡ {image_path.name} æ—¶å‡ºé”™: {e}")
            continue
        
        print(f"\n{'-'*50}")

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸš€ Qwen2-VL å›¾ç‰‡åˆ†æå·¥å…·")
    print("å¼€å§‹åˆ†æ ./pics ç›®å½•ä¸­çš„å›¾ç‰‡...")
    
    # åˆ†æå›¾ç‰‡
    analyze_images_in_folder("./pics")
    
    print("\nğŸ‰ æ‰€æœ‰å›¾ç‰‡åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
