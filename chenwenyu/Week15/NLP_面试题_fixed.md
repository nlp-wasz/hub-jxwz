# NLP& ⼤模型 - 知识点和⾯试题

# 推荐资料

456. LLMs 相关知识及⾯试题：https://wdndev.github.io/llm_interview_note/#/百⾯⼤模型 ⼤模型⾯试题：https://item.jd.com/15027html

# Python ⾯试题

机器学习⾯试题

# 深度学习⾯试题

# ⼤模型 /NLP ⾯试题

1. 请解释什么是词向量，以及它在⾃然语⾔处理中的主要作⽤。
2. 简述“稀疏词向量”（如one-hot编码）的原理，并分析其优缺点。
3. 什么是“分布式语义假设”？请举例说明它在⾃然语⾔处理中的体现。
4. 与稀疏词向量相⽐，稠密词向量（如Word2Vec,GloVe）有哪些优势？这些优势是如何获得的？
5. 请简述Word2Vec模型中的Skip-gram和CBOW两种架构的核⼼思想与区别。
6. GloVe模型是如何将全局统计信息（如共现矩阵）与局部上下⽂窗⼝⽅法结合起来的？
7. 除了Word2Vec和GloVe，请列举并简要说明两种其他类型的词嵌⼊⽅法。
8. 什么是“溢出词表词”？在⾃然语⾔处理任务中，处理 OOV 词为什么重要？
9. 请列举并⽐较⾄少三种处理OOV词的常⻅技术。如何使⽤⼦词（subword）信息来处理 OOV 问题？
10. 请以 BPE（Byte Pair Encoding）算法为例进⾏说明。
11. 除了引⼊⼦词信息，还有哪些模型结构或⽅法可以缓解 OOV 问题？
12. 请对⽐分析基于词（word）、⼦词（subword）和字符（char）的分词⽅法的核⼼理念。

13. 以英⽂和中⽂为例，说明为什么“词”级别的分词在处理这两种语⾔时可能⾯临不同的挑战。

14. ⼦词分词⽅法（如 BPE, WordPiece, Unigram LM）的主要设计⽬标是什么？它们通常是如何⼯作的？

15. 字符级分词有哪些优点和缺点？它特别适合处理哪些类型的语⾔或任务？

16. 不同的分词⽅法会对下游NLP任务（如机器翻译、⽂本分类）的性能产⽣何种影响？请从模型输⼊、泛化能⼒、计算效率等⻆度分析。

17. ⼤语⾔模型的分词器和传统的中⽂分词有什么区别？对于⼀个指定的词表，⼀句话是不是只有⼀种唯⼀的分词⽅式？

18. 为什么传统BM25检索对中⽂分词的质量很敏感，⽽⼤语⾔模型对分词器的选取不敏感？

19. GPT-4、LLaMA等现代⼤语⾔模型采⽤的字节级BPE分词器相⽐传统的BPE分词器有什么优点？

20. 国内预训练的⼤语⾔模型与海外模型相⽐，是如何做到⽤相对更少的词元表达中⽂语料的？

21. ⼤模型是如何区分聊天历史中⽤⼾说的话和AI说的话的？

22. ⼤模型做⼯具调⽤的时候，输出的⼯具调⽤参数是如何与⽂本回复区分开来的？

23. 参考章节中⽤播放列表数据训练歌曲嵌⼊的案例，设计⼀个使⽤嵌⼊技术解决电⼦商务产品推荐的系统。使⽤什么数据作为“句⼦”的等价物？如何将⽤⼾⾏为融⼊嵌⼊模型？

24. Word2vec 的训练过程中，负样本的作⽤是什么？

25. 传统的静态词嵌⼊（如word2vec）与⼤语⾔模型产⽣的上下⽂化嵌⼊相⽐，有什么区别？ 有了上下⽂化嵌⼊，静态词嵌⼊还有什么价值？

26. 上下⽂化嵌⼊是如何解决⼀词多义问题的，如技术语境下，英⽂ token 可能表⽰词元、代币、令牌，⽽中⽂ “推理” 可能表⽰ reasoning 或 inference？

27. 在 word2vec 等词嵌⼊空间中，存在 king - man +woman- queen 的现象，这是为什么？⼤语⾔模型的词元嵌⼊空间是否也有类似的属性？

28. 什么是⼤型语⾔模型？请从其设计⽬标、核⼼能⼒和典型代表模型三个⽅⾯进⾏阐述。

29. ⼤型语⾔模型与传统统计语⾔模型（如 N-gram、HMM）的根本区别是什么？请从架构、训练⽅式、上下⽂理解能⼒和应⽤范围进⾏对⽐。

30. 请解释“迁移学习”在⼤型语⾔模型开发和应⽤中的核⼼地位。它具体带来了哪些优势？

31. Transformer中的编码器和解码器有什么区别，只有编码器或者只有解码器的模型是否有⽤？

32. GPT跟原始Transformer论⽂的模型架构有什么区别？

33. 仅编码器（BERT类）、仅解码器（GPT类）和完整编码器-解码器架构各有什么优缺点？

34. 为什么说Transformer的⾃注意⼒机制相对于早期RNN中的注意⼒机制是⼀个显著的进步？

35. ⼤语⾔模型为什么有最⻓上下⽂⻓度的概念？为什么它是指输⼊和输出的总⻓度？

36. ⼤语⾔模型的⾸字延迟、输⼊吞吐量、输出吞吐量分别是如何计算的？不同应⽤场景对⾸字延迟、输⼊和输出吞吐量的需求分别是什么？

37. 预训练和微调的两步范式为什么如此重要？基础模型通过预训练获得了哪些核⼼能⼒？微调在引导模型遵循指令、回答问题和对⻬⼈类价值观⽅⾯起到什么作⽤？

38. 在设计⼀个⾯向多语⾔或混合⽂本的NLP系统时，你会如何选择和设计分词⽅案？需要考虑哪些因素？

39. 随着BERT等预训练语⾔模型的出现，传统的静态词向量（如Word2Vec）在应⽤中的地位发⽣了哪些变化？它们各⾃更适⽤于哪些场景？

40. 如何利⽤词向量（如 Word2Vec 或 GloVe 的嵌⼊）计算两个词或句⼦之间的语义相似度？请列出具体的计算步骤。

41. 除了余弦相似度，还有哪些⽅法或指标可以衡量词向量空间中的语义关系？

42. 词向量可以捕捉哪些类型的语义关系？请举例说明。它通常在哪些关系上表现不佳？

43. 在处理同义词和多义词时，静态词向量（如Word2Vec）存在什么局限性？后续的模型（如ELMo,BERT）是如何试图解决这些问题的？

44. 请列举并⽐较⾄少三种基于词向量构建句⼦向量的经典⽅法（例如，简单平均、加权平均、 使⽤RNN/LSTM编码等）。

45. 什么是“句嵌⼊”？与词嵌⼊相⽐，学习句嵌⼊的主要挑战是什么？

46. 简述 InferSent 和 Universal Sentence Encoder 这类句⼦编码模型的基本思想。它们是如何获得任务⽆关的句⼦表⽰的？

47. 在什么情况下，对词向量取平均作为句⼦向量是⼀个有效或⽆效的策略？请从任务类型和⽂ 本特点⻆度分析。

48. 请详细描述Transformer模型的整体架构，包括编码器和解码器的组成、数据流，并解释其为何能并⾏处理序列。

49. 什么是“⾃注意⼒机制”？请写出缩放点积注意⼒的计算公式，并解释公式中每⼀部分的作⽤。

50. “多头注意⼒”的设计动机是什么？它如何帮助模型捕获输⼊序列中更丰富的关系？

51. 为什么Transformer需要“位置编码”？请解释原始Transformer中使⽤正弦/余弦函数进⾏位置编码的原理。

52. “⾃回归语⾔建模”和“掩蔽语⾔建模”是两种主要的预训练⽬标。请解释它们的⼯作原理，并各举⼀个代表性模型（如GPT和BERT）说明。

53. 什么是“提⽰⼯程”？它如何⽤于引导模型⾏为、控制“幻觉”和提升特定任务的性能？请举例说明。

54. 解释“零样本学习”、“少样本学习”和“微调”在LLM应⽤中的区别与联系。

55. 在 LLM 处理中，“词元化”的作⽤是什么？常⻅的词元化⽅法（如 BPE、WordPiece）是如何⼯作的？

56. ⼤型语⾔模型如何克服传统 RNN/LSTM 在处理“⻓期依赖关系”上的局限性？

57. 什么是“嵌⼊层”？为什么说它是连接离散符号（词元）与连续向量空间的关键桥梁？

58. ⼤型语⾔模型在“情感分析”任务上有何独特优势？请与传统⽅法进⾏对⽐。

59. LLM在“机器翻译”领域有哪些创新应⽤⽅式？（例如，零样本翻译、上下⽂感知翻译等）

60. “会话AI和聊天机器⼈”是如何受益于LLM的？请描述⼀个现代LLM驱动聊天机器⼈的核⼼组件和⼯作流程。

61. 什么是LLM的“幻觉”问题？除了提⽰⼯程，还有哪些⽅法可以缓解或控制幻觉？

62. 讨论⼤模型输出中可能存在的“偏⻅”问题。在模型开发和应⽤中，可以采取哪些技术或策略来减轻偏⻅？

63. ⼤模型怎么知道它的输出该结束了？

64. 训练时如何防⽌模型看到未来的词元？

65. 注意⼒机制是如何计算上下⽂各个词元之间的相关性的？每个注意⼒头是只关注⼀个词元吗？softmax之前为什么要除以根号d_k？

66. Q 和 K 在注意⼒的表达式⾥看起来是对称的，但 KV 缓存⾥为什么只有 KV，没有 Q？

67. 如果没有KV缓存，推理性能会降低多少？

68. 为什么 Transformer 中需要残差连接？

69. Transformer 中的 LayerNorm 跟 ResNet 中的 BatchNorm 有什么区别，为什么 LLaMA-3 换⽤了 RMSNorm？

70. Transformer中前馈⽹络的作⽤是什么？注意⼒层中已经有softmax⾮线性层，那么前馈⽹络是否必要？

71. 如果需要通过修改尽可能少的参数值，让模型忘记某⼀特定知识，应该修改注意⼒层还是前馈⽹络层的参数？

72. ⼤语⾔模型在数学计算时，为什么经常不准确？

73. 模型深度（层数）与宽度（隐藏维度⼤⼩）、注意⼒头数量、上下⽂⻓度等参数之间是如何相互影响的？如果要训练⼀个⽐当前模型参数规模⼤10倍的模型，你会如何调整这些参数？

74. 以⼀个你熟悉的开源模型为例，介绍模型中每个矩阵的⼤⼩和形状。

75. ⼤语⾔模型推理过程中，内存带宽和算⼒哪个是瓶颈？以⼀个你熟悉的开源模型为例，计算输⼊批次⼤⼩达到多少时，能够平衡利⽤内存带宽和算⼒？

76. 从统计学⻆度看，Transformer 输出层假设词元符合什么分布？

77. 给定⼀个⽀持8K上下⽂的开源模型，如何把它扩展成⽀持32K上下⽂的模型？上下⽂⻓度增加后对KV缓存会带来什么挑战？

78. 为什么注意⼒机制需要多个头？GQA、MQA优化跟简单减少注意⼒头的数量相⽐，有什么不同？GQA、MQA 优化的是训练阶段还是推理阶段？

79. FlashAttention 并不能减少计算量，为什么能实现加速？FlashAttention 是如何实现增量计算 softmax 的？

80. RoPE（旋转位置嵌⼊）相⽐Transformer论⽂中的绝对位置编码有什么优点？RoPE在⻓ 上下⽂外推时会⾯临什么挑战？

81. 由于训练样本⻓度往往⼩于最⼤上下⽂⻓度，把多个训练样本放到同⼀个上下⽂中训练时， 如何避免它们互相⼲扰？

82. 如何利⽤⼀个⼩规模的⼤语⾔模型提升⼤规模模型的推理性能，并尽量不影响⼤模型的推理结果？推测解码并没有减少计算量，为什么能提升推理性能？

83. 请对⽐分析仅编码器（BERT类）、仅解码器（GPT类）和编码器-解码器（T5类）架构的优缺点及适⽤场景。

84. ⾃注意⼒机制如何使Transformer能够捕捉⻓距离依赖关系？与RNN/LSTM相⽐，它在计算效率和建模能⼒上有何本质区别？

85. 为什么⼤模型有“上下⽂⻓度”的限制？这个限制具体指输⼊和输出的总⻓度，这是为什么？

86. 解释多头注意⼒机制的设计动机。与简单地使⽤⼀个⼤的注意⼒头相⽐，多头机制有何优势？

87. 请对⽐多查询注意⼒（MQA）和分组查询注意⼒（GQA）与标准多头注意⼒的区别。它们主要优化的是训练阶段还是推理阶段的什么性能指标？

88. 请⽐较Transformer原始论⽂中的正弦位置编码与RoPE（旋转位置嵌⼊）的原理和优势。

89. RoPE在应⽤于远超过训练⻓度的⽂本（⻓度外推）时会⾯临什么挑战？有哪些改进⽅法？

90. 除了RoPE，还有哪些先进的位置编码或注意⼒偏置⽅法（如ALiBi）旨在提升模型的外推能⼒？请简述其原理。

91. 为什么Transformer模型需要引⼊位置编码？如果不加⼊位置信息，模型会丢失什么能⼒？

92. 请详细说明Transformer原始论⽂中使⽤的正弦和余弦位置编码公式。这种编码⽅式具有什么特性（如相对位置感知、外推性）？

93. 除了原始的正余弦编码，还有哪些常⻅的位置编码⽅案（例如，可学习的位置嵌⼊、相对位置编码）？请简述它们的原理。

94. 相对位置编码（如 T5, Transformer-XL, RoPE 中使⽤的⽅法）与绝对位置编码相⽐，主要优势是什么？

95. BERT的输⼊表⽰由哪三种嵌⼊求和构成？请分别说明它们各⾃的作⽤。

96. BERT 的 Token Embeddings、Segment Embeddings 和 Position Embeddings 分别是如何⽣成的？在预训练和微调阶段，它们有何异同？

97. 在处理⼀个句⼦对任务（如问答、⾃然语⾔推理）时，BERT的输⼊是如何构造的？

98. 在 BERT 之后的预训练模型（如 RoBERTa, ALBERT, DeBERTa）中，这些嵌⼊类型或输⼊表⽰发⽣了哪些变化或优化？

99. 请简述Transformer编码器（如BERT所⽤）和解码器（如GPT所⽤）在结构上的核⼼区别。这些区别如何影响它们各⾃适合的任务类型？

100. 什么是“Encoder-Decoder”架构（或称“Seq2Seq with Transformer”）？请列举⼀个典型模型，并说明它在什么任务上表现出⾊。

101. 从架构演进的⻆度，对⽐分析 BERT（仅编码器）、GPT（仅解码器）和 T5/BART（编码器 - 解码器）这三类预训练模型在语义建模和能⼒上的主要差异。

102. 如何基于表⽰型模型⽣成的嵌⼊向量实现⽂本分类？

103. 使⽤嵌⼊向量实现分类和使⽤⽣成式模型直接分类的⽅法相⽐，有什么优缺点？

104. 如果没有标注数据，如何基于嵌⼊模型实现⽂本分类？如何优化标签描述来提⾼零样本分类的准确率？

105. Transformer为什么⽐朴素⻉叶斯分类器效果好很多？朴素⻉叶斯分类器的条件独⽴性假设有什么问题？

106. 有了强⼤的⽣成式⼤语⾔模型，嵌⼊模型还有什么⽤？请举⼀个适合嵌⼊模型但不适合⽣ 成模型的例⼦。（提⽰：推荐系统）

107. 给定⼤量的⽂档，如何把它们聚类成⼏簇，并总结出每⼀簇的主题？

108. 词袋法和⽂档嵌⼊在实现原理上有什么区别？词袋法是不是⼀⽆是处了？

109. 为什么⼀些模型把温度设置成0，输出的内容仍然有⼀定的不确定性？（提⽰：推测解码）

110. 对于指定的⼤模型，如何通过提⽰词减少其幻觉？

111. ⼀个专业的提⽰词模板应该由哪⼏部分构成？为什么提⽰词中需要描述⻆⾊定义？

112. 对于⼀个复杂的提⽰词，如何测试其中哪些部分是有⽤的，哪些部分是⽆⽤的？

113. 如何设计提⽰词模板，尽量防⽌提⽰词注⼊？如何在系统层⾯检测提⽰词注⼊攻击？

114. 如果把⽤⼾信息放在系统提⽰词中，但在对话轮数较多后，⼤模型经常忘记⽤⼾信息，如何解决？

115. 在没有推理型模型之前，如何让模型先思考后回答？思维链、⾃洽性、思维树等⼏种技术有什么优缺点？

116. 在创意写作任务中，如何让模型⽣成多个可能输出，再从中选取⼀个最好的？

117. 如果需要模型遵循指定的格式输出，提⽰词应该怎么写？

118. 如何保证模型的输出⼀定是合法的JSON格式？（提⽰：限制采样）

119. 将⼤模型⽤于分类任务时，如何保证其输出⼀定是⼏个类别之⼀，不会输出⽆关内容？ （提⽰：限制采样）

120. LDA、BTM、NMF、BERTopic、Top2Vec 等主题模型有什么优缺点？对⻓⽂档、短⽂档、 ⾼质量需求的垂直领域分别应使⽤何种模型？

121. 掩码语⾔建模与BERT的掩蔽策略相⽐有何不同？这种预训练⽅式如何帮助模型在下游的⽂本分类任务中获得更好的性能？

122. 在RAG中，为什么要把⽂档划分成多个块进⾏索引？如何解决⽂档分块后，内容上下⽂缺失的问题？如何处理跨⽚段的依赖关系？

123. 如果发现向量相似度检索的匹配效果不佳，除了更换嵌⼊模型，还有哪些办法？

124. 向量相似度检索不能实现关键词的精确匹配，传统关键词检索不能匹配语义相近的词，如何解决这对⽭盾？

125. 向量相似度检索已经是根据语义相似度匹配，为什么还需要重排序模型？

126. 为什么要在向量相似度检索前，对⽤⼾输⼊的话进⾏改写？

127. RAG系统检索的⽂档可能包含冲突信息或过时数据，如何在⽣成回答时防⽌被这些信息误导？

128. 如何使检索模块能够从⽣成模块获得反馈并动态调整检索策略，例如给不同的⽂档标注可信度？

129. 如何提升RAG系统的可解释性，包括清晰标注⽣成内容的来源，以及量化展⽰系统对回答的确信度？

130. 智能体如何把处理企业任务的经验总结到知识库中，并在后续任务中引⼊知识库中的经验？如何保证经验不断积累，⽽不是简单⽤新的经验覆盖已有的经验？

131. 如果需要根据⼀本⻓篇⼩说的内容回答问题，⼩说⻓度远远超出上下⽂限制，应该如何综合利⽤摘要总结和RAG技术，使其能同时回答故事梗概和故事细节？

132. 如何将RAG系统从纯⽂本扩展到多模态，⽀持检索图像、视频、图⽂并茂的⽂档等多模态信息，并在⽣成回答时以多模态形式呈现，例如包含原始⽂档中的图表和视频？

133. 为什么ViT不能简单地像处理⽂本词元那样，为每个图像块分配⼀个唯⼀的、离散的ID， ⽽是必须采⽤线性投影⽣成连续的嵌⼊向量？

134. 在CLIP训练过程中，为什么需要同时最⼤化匹配图⽂对的相似度和最⼩化⾮匹配对的相似度？

135. 在微调任务中，应该冻结哪些层的权重？微调编码器前⼏层、编码器后⼏层、前馈神经⽹ 络层有什么区别？

136. 如果有标注的训练数据很少，如何扩增训练数据的数量？

137. 推理型模型的RL与⾮推理型模型的RLHF有什么区别？

138. 推理型模型在后训练过程中，思维链会越来越⻓，这样结果的准确率提升了，但响应延迟也增加了。如何处理推理深度与响应延迟的权衡？

139. 随着模型规模变得极⼤（如百亿、千亿参数），在架构上通常会出现哪些设计变化或考量（例如，稀疏混合专家、并⾏策略）？这些变化主要为了解决什么问题？

140. 除了通⽤⽂本语料，在训练⼀个⾯向特定领域（如代码、科学⽂献、多语⾔）的⼤模型时，你会考虑加⼊哪些类型的数据集？为什么？

141. The Pile、Common Crawl 和 Wikipedia 这三个数据集在构成和质量上有何主要区别？在构建预训练语料时，应如何权衡和搭配使⽤它们？

142. 请简述⼤语⾔模型典型的训练流程（例如，预训练、监督微调、指令微调、对⻬等），并说明每个阶段主要使⽤什么类型的数据以及⼤致的数据量级关系。

143. 为什么预训练阶段需要的数据量远⼤于有监督微调阶段？从模型学习的⻆度解释这种数据需求差异。

144. 在⼤模型训练中，“tokens”是常⽤的数据量度量单位。请解释其含义，并估算⼀个包含1万亿tokens的英⽂语料库⼤约对应多少单词或多少GB的存储空间。

145. 在构建⼤模型预训练语料时，评估和保证“数据质量”通常涉及哪些具体的处理步骤或过滤规则？

146. 什么是“数据去重”？为什么在⼤模型训练前进⾏⽂档级、段落级或句⼦级的数据去重⾮ 常重要？

147. “数据多样性”对于训练⼀个健壮的⼤模型有何重要性？在预处理阶段，可以从哪些维度来评估和提升数据集的多样性？

148. 请描述⼤模型⽂本预处理中的⼀个典型流程（从原始数据到模型可接受的输⼊），并说明每个步骤的⽬的。

149. 什么是⼤模型的“缩放法则”？它主要研究哪三个核⼼变量之间的关系？

150. 根据Chinchilla缩放定律的研究结论，在给定计算预算的情况下，应该如何最优地分配模型参数规模和训练数据量？这与之前的研究（如 OpenAI 的 Scaling Laws）有何不同？

151. 在实践约束（如有限的训练数据、有限的GPU内存）下，如果⽆法严格遵循理想的缩放法则，可以采取哪些策略来逼近最优性能？

152. 请解释“灾难性遗忘”现象。在对⼀个预训练模型进⾏持续预训练或领域适应时，可以采取哪些策略来缓解灾难性遗忘？

153. “指令微调”阶段的⽬标是什么？它与“有监督微调”有何区别和联系？

154. 为什么说预训练使模型获得了“通⽤知识”和“语⾔理解能⼒”，⽽监督微调侧重于“任务格式”和“指令遵循”？请结合具体⽰例说明。

155. 什么是⼤语⾔模型的“涌现能⼒”？请列举三个典型的涌现能⼒⽰例。

156. “涌现能⼒”通常在模型规模超过某个阈值时出现。请解释这种现象背后的可能原因（可从缩放定律、模型容量等⻆度分析）。

157. 请详细阐述⼤语⾔模型标准开发流程的三个核⼼阶段（预训练、监督微调、基于⼈类反馈的强化学习）各⾃的⽬标、输⼊输出数据形式及关键技术。

158. 监督微调阶段的数据集通常如何构造？它与传统的有监督学习数据集有何不同？

159. 在RLHF阶段，奖励模型的作⽤是什么？它是如何被训练和使⽤的？

160. 请详细分析在⼤模型训练过程中，GPU显存主要被哪些部分占⽤？给出⼀个包含模型参数、梯度、优化器状态和激活的显存占⽤计算公式。

161. 对于⼀个拥有N亿参数的模型，在混合精度训练下，请估算其参数、梯度和Adam优化器状态所需的最⼩显存。

162. 请解释“梯度检查点”技术的⼯作原理。它如何帮助在训练⼤模型时节省显存？其主要代价是什么？

163. 除了梯度检查点，还有哪些常⽤的显存优化技术？

164. 请解释在分布式训练中，All-Reduce、All-Gather、Reduce-Scatter 这些集合通信原语各⾃完成什么操作？它们通常⽤于哪些并⾏策略中？

165. 在数据并⾏训练中，通信开销主要发⽣在哪个环节？请推导⼀个批次训练中，All-Reduce通信数据量的计算公式。

166. 请简述张量并⾏的核⼼思想（以 Megatron-LM 的列并⾏与⾏并⾏为例）。分析在正向和反向传播过程中，张量并⾏需要进⾏哪些通信操作？

167. 流⽔线并⾏是如何将模型层划分到不同设备上的？它引⼊的“流⽔线⽓泡”是什么？如何通过调整微批次⼤⼩来减少⽓泡⽐例？

168. ZeRO 优化技术有多个阶段（ZeRO-1, ZeRO-2, ZeRO-3）。请阐述 ZeRO-3 的核⼼思想，并说明它相⽐ZeRO-1,ZeRO-2在通信开销上的变化。

169. 当同时使⽤数据并⾏、张量并⾏和流⽔线并⾏（3D并⾏）时，通信开销主要发⽣在哪些层级之间？在设计混合并⾏策略时，应遵循哪些基本原则来最⼩化总体通信开销？

170. 请阐述 PPO 算法在⼤模型对⻬（RLHF）框架中的⻆⾊。具体说明“演员模型”、“评论家

模型”和“奖励模型”分别指什么，以及它们是如何协同⼯作的。

171. PPO 算法中“重要性采样”和“clip 机制”的⽬的是什么？请结合 PPO ⽬标函数中的 clip项进⾏解释。

172. 在PPO训练过程中，为什么需要引⼊“参考模型”？它如何帮助稳定训练并防⽌模型过度偏离原始分布？

173. 奖励模型训练的⽬标是什么？请描述其训练数据的格式和损失函数（如 pairwise rankingloss）。

174. 奖励模型容易在分布外数据上给出不可靠的分数。在训练和使⽤奖励模型时，可以采取哪些措施来缓解这个问题？

175. 请列举并解释⾄少三种⽤于稳定PPO训练的技术（如监控指标、梯度处理、损失函数改进、模型初始化等）。

176. 在 PPO 训练中，为什么需要对奖励值进⾏归⼀化或⽩化？常⻅的做法是什么？

177. “KL散度惩罚”在PPO⽬标函数中的作⽤是什么？如何调整KL惩罚系数以平衡奖励最⼤ 化和模型⾏为偏离？

178. 什么是“垂类微调”？它相较于通⽤领域的指令微调，在数据准备、训练策略和评估指标上有哪些特殊考虑？

179. 在进⾏垂类监督微调时，如何设计数据混合策略（例如，平衡垂类数据、通⽤指令数据和领域基础数据）以避免灾难性遗忘和过拟合？

180. 假设你需要将⼀个通⽤⼤模型微调成法律领域的专⽤助⼿。请描述你构造训练数据集的整体思路，包括数据来源、样本格式和质量控制⽅法。

181. 在什么场景下需要对⼀个已预训练好的⼤模型进⾏“词表扩充”？请以特定领域（如医学、代码）或特定语⾔为例说明。

182. 请简述为⼤模型扩充词表（添加新token）的基本步骤。在初始化新添加token的嵌⼊向量时，有哪些常⻅且有效的策略？

183. 词表扩充后，为什么通常需要对模型进⾏进⼀步的“继续预训练”或“轻量微调”？直接使⽤可能会导致什么问题？

184. 什么是⼤模型的“⻓度外推”能⼒？为什么原始的Transformer位置编码在推理时处理⽐ 训练序列更⻓的⽂本时会⾯临挑战？

185. 请解释“线性插值”和“NTK-aware缩放”这两种相对位置编码的扩展⽅法的原理。它们是如何缓解⻓度外推问题的？

186. 除了调整位置编码，还有哪些模型结构或训练策略层⾯的⽅法可以提升模型的外推能⼒ （例如，使⽤ALiBi、训练时使⽤随机滑动窗⼝等）？

187. 如何系统地评估⼀个模型在“⻓度外推”⽅⾯的性能？需要设计什么样的测试集？

188. 什么是“检索增强⽣成”（RAG）？请描述其核⼼⼯作流程，并解释它是如何实现知识注⼊ 的。

189. 对⽐“继续预训练”和“RAG”这两种知识注⼊⽅法。从知识更新时效性、事实准确性、 幻觉⻛险、部署成本等⻆度分析各⾃的优缺点。

190. Transformer解码器层与编码器层在结构上有何关键区别？这些区别如何使其适⽤于⾃回归⽣成任务？

191. 为什么Transformer架构中⼴泛使⽤残差连接和层归⼀化？它们在训练深度⽹络时分别解决了什么问题？

192. 在计算注意⼒权重时，为什么通常使⽤ Softmax 函数？能否使⽤其他函数替代？Softmax的温度系数如何影响注意⼒分布？

193. 请解释“注意⼒掩码”的作⽤。在Transformer中，有哪⼏种常⻅的注意⼒掩码（如因果掩码、填充掩码）？它们是如何被应⽤到注意⼒分数计算中的？

194. 请⽐较基于单词、字符和⼦词的三种词元化⽅法的优缺点，并说明为什么⼦词词元化（如BPE、WordPiece、Unigram）成为⼤模型的主流选择。

195. RoPE的核⼼思想是什么？它是如何将绝对位置信息通过旋转矩阵融⼊词向量的？

196. 相⽐于原始的Transformer正弦位置编码和可学习的位置嵌⼊，RoPE在理论上有何优势（例如，相对位置感知、⻓度外推性）？

197. 请写出RoPE在⼆维情况下的旋转矩阵形式，并解释其如何实现相对位置依赖。

198. ALiBi的⼯作原理是什么？它是如何在不添加位置嵌⼊的情况下，为注意⼒分数注⼊位置信息的？

199. 为什么ALiBi被证明具有优异的⻓度外推能⼒？其偏置项的斜率设计（如$- 1 / 2 \Lambda _ { \dot { \mathsf { I } } } ^ { \dagger }$ ）背后有何考量？

200. 从计算效率和实现简洁性⻆度，对⽐分析 ALiBi 与 RoPE。

201. 标准点积注意⼒的时间和空间复杂度是多少？它成为处理⻓序列瓶颈的原因是什么？

202. 线性注意⼒的核⼼创新是什么？它是如何通过分解将复杂度从${ \mathsf { O } } ( { \mathsf { n } } ^ { 2 } )$ 降低到O(n)的？请简要描述其数学推导。

203. 线性注意⼒通常需要对注意⼒函数进⾏怎样的近似或核函数变换？这可能会牺牲模型哪些⽅⾯的能⼒？

204. 请解释标准多头注意⼒机制的设计动机。多个“头”是如何并⾏⼯作并捕获不同⼦空间信息的？

205. 在⾃回归解码（推理）阶段，Transformer解码器的注意⼒机制存在什么性能瓶颈？为什么KV缓存是关键的优化技术？

206. 什么是多查询注意⼒和分组查询注意⼒？它们是如何通过共享 Key 和 Value 投影来减少内

存占⽤和带宽需求，从⽽加速推理的？

207. ⽐较MHA、MQA和GQA在模型容量、推理速度、内存占⽤和⽣成质量之间的权衡。在什么场景下更倾向于使⽤GQA？

208. 在深度神经⽹络中，归⼀化层的主要作⽤是什么？请从优化过程（梯度）和模型泛化的⻆ 度分别阐述。

209. 请详细解释 BatchNorm 的⼯作原理，包括其在训练和推理阶段的不同⾏为。为什么BatchNorm在CV任务中⼴泛应⽤，但在NLP⼤模型中较少使⽤？

210. 请详细解释 LayerNorm 的⼯作原理。与 BatchNorm 相⽐，LayerNorm 在处理序列数据（如Transformer）时有哪些优势？

211. RMSNorm 与 LayerNorm 有何异同？请从计算公式和设计动机⻆度进⾏对⽐。为什么⼀些现代⼤模型（如LLaMA）选择使⽤RMSNorm？

212. 请描述 Dropout 在训练阶段的前向传播和反向传播流程。其核⼼思想是什么？

213. 为什么在训练时应⽤了Dropout的层，在推理时必须进⾏缩放（如乘以dropout保留概率p）？如果不这样做，会导致什么问题（即“期望偏移”）？

214. 除了期望偏移，Dropout还可能引起激活值的⽅差偏移。请解释这⼀现象，并说明现代深度学习框架（如 PyTorch）是如何通过实现“倒置 Dropout”来⾃动解决这两个偏移问题的。

215. 在⼤语⾔模型的预训练和微调中，Dropout 的使⽤策略（如使⽤位置、丢弃率）通常有何不同？为什么？

216. 神经⽹络参数初始化为什么⾄关重要？⼀个糟糕的初始化可能导致什么问题？

217. 请解释“Xavier初始化”和“Kaiming初始化”的数学原理。它们分别针对哪种激活函数（Sigmoid/Tanh vs. ReLU）进⾏了优化？

218. 对于Transformer模型中的⾃注意⼒层和前馈神经⽹络层，通常会采⽤何种初始化策略？ 请说明理由。

219. 为什么在评测时，需要区分“知识密集型任务”和“推理密集型任务”？这对评估模型的真实能⼒有何意义？

220. 对于开放域⽂本⽣成任务，请⽐较基于N-gram的评测指标（如BLEU,ROUGE）和基于嵌⼊相似度的指标（如 BERTScore）的优缺点。

221. 为什么传统指标（如BLEU）在评估⼤语⾔模型的⽣成质量时经常失效？请从⽣成多样性、忠实度等⻆度分析。

222. 请解释“困惑度”作为语⾔模型内在评估指标的含义。它能否直接反映下游任务的表现？ 为什么？

223. 请阐述MoE架构的核⼼思想。它是如何通过“稀疏激活”来实现模型容量巨⼤提升，同时保持计算成本基本不变的？

224. MoE模型训练⾯临的主要挑战是什么？（例如，专家负载均衡、训练不稳定等）有哪些常⻅的解决技术？

225. 对⽐稠密模型和MoE模型，它们在推理延迟、显存占⽤、通信开销和最终性能上有何典型差异？

226. 请画出 RAG 系统的核⼼⼯作流程图，并说明“检索器”、“知识库”和“⽣成器”三⼤组件的职责与交互过程。

227. 如何构建⼀个全⾯的RAG评估体系？请从“检索质量”、“⽣成质量”和“端到端效果”三个层⾯，分别列举⾄少两个具体的评估指标。

228. 评估RAG系统时，“忠实度”和“答案相关性”是两个关键维度。请解释它们的含义，并说明如何度量。

229. 简述基于稀疏向量（如BM25）和基于稠密向量（如Embedding模型）的检索⽅法各⾃的⼯作原理与优缺点。

230. 什么是“混合检索”？为什么在实践中，结合稀疏检索和稠密检索往往能取得更好的效果？

231. 除了向量检索，还有哪些可以⽤于RAG召回的技术或策略？（例如，知识图谱查询、关键词扩展等）

232. 在RAG流程中，“重排”模块的作⽤是什么？为什么在初步检索后还需要进⾏重排？

233. 请列举并⽐较两种常⻅的重排⽅法：基于交叉编码器的判别式重排和基于LLM的⽣成式重排。

234. 如何训练⼀个有效的重排模型？其训练数据通常如何构造？

235. 当知识库⽂档⾮常⼤时，RAG系统在索引构建和检索效率⽅⾯会⾯临哪些挑战？有哪些优化思路？（例如，分⽚、分层索引、量化等）

236. 如何处理RAG中的“⻓上下⽂”问题？例如，当检索到的⽂档⽚段过⻓，超过了⽣成模型的上下⽂窗⼝限制时，应该怎么办？

237. 在真实业务场景中部署RAG系统时，如何设计⼀个有效的数据更新与版本管理机制，以确保知识库的时效性和⼀致性？

238. ⼀个⼤模型智能体通常由哪⼏个核⼼模块组成？请简述每个模块的功能。

239. “规划”、“⼯具调⽤”和“记忆”是智能体的关键能⼒。请解释它们在⼤模型智能体框架中分别扮演什么⻆⾊。

240. 请阐述LoRA（Low-RankAdaptation）的核⼼设计思想。它基于什么假设，通过何种⽅式实现对⼤模型的⾼效微调？

241. 详细描述LoRA在Transformer层（例如，⾃注意⼒层的Q、V投影矩阵）上的具体实现流程和参数更新⽅式。

242. LoRA中的“秩”（rank）是⼀个关键超参数。它如何影响微调的性能和效率？在实际应⽤ 中应如何选择？

243. 除了 LoRA，请再列举三种不同的参数⾼效微调⽅法（例如，Prefix-Tuning, Adapter, IA3），并简述其原理。

244. 请对⽐分析“添加式 PEFT”（如 Adapter, Prefix-Tuning）和“重参数化 PEFT”（如 LoRA） 在模型结构、推理开销和效果上的异同。

245. 在选择PEFT⽅法时，需要从哪⼏个维度进⾏权衡和考量？（例如，任务性能、内存开销、推理延迟、实现复杂度等）

246. 在什么情况下，全参数微调可能⽐PEFT⽅法更合适？请从任务特性、数据量和模型规模等⻆度分析。

247. 有⼀种策略是先⽤ LoRA 微调，再将 LoRA 权重合并回原模型进⾏推理。这种做法的优点和潜在缺点是什么？

248. 请解释“贪婪解码”和“集束搜索”的⼯作原理，并分析它们各⾃在⽣成质量和多样性上的特点。

249. 什么是“随机采样”？请说明“温度”参数如何影响随机采样下的概率分布和⽣成结果。

250. 请对⽐“Top-k 采样”和“Top-p（核）采样”策略。它们在控制⽣成多样性和连贯性⽅⾯ 有何异同？

251. 请列出⼤语⾔模型⽂本⽣成中常⻅的 5 个关键参数（例如， max_length, , 等），并解释其作⽤。temperature top_p repetition_penalty“重复惩罚”参数是为了解决什么问题⽽引⼊的？它是如何在⽣成过程中调整 token 概率252.的？

253. 标准注意⼒计算在训练和推理预填充阶段的主要性能瓶颈是什么？（从计算复杂度和内存访问开销⻆度分析）

254. 请简述FlashAttention的核⼼优化思想。它是如何通过“平铺”、“重计算”等技术来优化GPU显存的⾼带宽内存与⽚上SRAM之间的IO，从⽽提升计算速度并降低内存占⽤的？

255. FlashAttention 为后续的⼤模型注意⼒优化（如 FlashAttention-2, FlashDecoding）奠定了哪些基础？

256. 在⾃回归解码过程中，KV缓存的内存管理⾯临哪些挑战？为什么说低效的KV缓存管理是限制推理系统吞吐量的关键瓶颈之⼀？

257. 请阐述PagedAttention的⼯作原理。它是如何借鉴操作系统虚拟内存中的“分⻚”思想， 来解决KV缓存内存碎⽚化问题，从⽽显著提⾼吞吐量的？

258. 什么是模型量化？请简述将 FP16 模型量化为 INT8 模型的“权重量化”基本流程。

259. “GPTQ”和“AWQ”是两种主流的后训练量化⽅法。请对⽐它们在量化粒度、校准数据和保真度上的主要区别。

260. 量化感知训练与后训练量化相⽐，通常能获得更好的效果。请解释其原因。

261. 像R1-Zero这类主要依赖强化学习在可验证任务（如数学、编程）上训练的⽅法，如何扩展到更主观、缺乏明确验证机制的领域（如创意写作、战略分析）？

262. 如果要在⼀个⾮推理型基座模型上，通过强化学习训练出⼀个在1000以内整数四则运算上错误率低于 $1 \%$ 的模型，请估算所需的基座模型规模、GPU资源需求和⼤致训练时⻓。

263. 如果要在 QwQ-32B 这类推理模型基础上，通过 RL 强化其在垂直领域（如学术研究）的能⼒，应如何构建训练数据集？并预估所需的计算资源。

264. FlashAttention并没有减少注意⼒机制的计算量（FLOPs），为什么它能显著提升训练和推理速度？请解释其核⼼优化思想。

265. ⼤模型推理中的KV缓存为什么⾄关重要？像PagedAttention这样的技术是如何优化KV缓存的内存管理，从⽽提升系统吞吐量的？

# 模型相关⾯试题

DeepSeek V3 Base

DeeSeek R1

2. Qwen5

Qwen-VL

# Qwen3

# Qwen-Embedding 和 Rerank

gpt-oss

2. # Gemini 5

# 库 / ⼯具 / 中间件

Linux
Docker
Pytorch
transformers
PEFT
trl
openai-agent
langchain
miluvs/faiss
redis
mysql
kafka
elasticseach

# ⽅案设计

1. 假设你正在训练⼀个⾯向社交媒体⽂本的情感分析模型，其中包含⼤量⽹络⽤语、拼写错误和缩写。请结合本章内容，设计⼀个从⽂本预处理、分词到词表⽰的综合⽅案，并阐述你的

设计理由。

2. 假设你有⼀个包含100万条客⼾评论的数据集，但只有1000条带有标签的数据，同时利⽤ 有标签和⽆标签数据，结合表⽰模型和⽣成模型的优势，构建⼀个分类系统？

3. 给定⼀个句⼦，请设计⼀个流程，说明如何使⽤BERT模型来获取⼀个⾼质量的、固定维度的句⼦向量表⽰（即BERT句嵌⼊）。有哪些常⻅的池化策略？

4. 如果你想微调⼀个预训练语⾔模型（如BERT）来完成⼀个⽂本相似度匹配任务，你会如何设计模型的输⼊、输出和损失函数？

5. 如何编写⼀个智能体，帮助⽤⼾规划⼀次包含机票预订、酒店安排和景点游览的旅⾏？需要配置哪些⼯具？如何确保系统在⾯对不完整或⽭盾信息时仍能提供合理建议？

6. 假设要开发⼀个“智能数据分析助⼿”智能体，能够根据⽤⼾⾃然语⾔查询，⾃动执⾏ SQL查询、数据可视化和报告⽣成。请基于智能体框架，设计其核⼼模块和⼯作流程。

7. 假设你需要为⼀个电商客服场景构建⼀个智能问答系统。请设计⼀个技术⽅案，说明如何利⽤⼤型语⾔模型（考虑预训练、微调、提⽰⼯程等环节）来实现这⼀⽬标，并讨论可能⾯临的挑战（如幻觉、事实准确性）及应对思路。

8. 如果需要设计⼀个 AI 智能伴侣，每天记录⽤⼾说过的所有话、做过的所有事，持续多个⽉，如何在需要的时候快速检索出相关的记忆，让AI能够根据记忆回答问题？综合对话历史窗⼝化、摘要总结、RAG等技术。