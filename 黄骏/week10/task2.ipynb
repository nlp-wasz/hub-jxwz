{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-06T09:44:11.870231Z",
     "start_time": "2025-11-06T09:44:02.541029Z"
    }
   },
   "source": [
    "import base64\n",
    "from zai import ZhipuAiClient\n",
    "\n",
    "client = ZhipuAiClient(api_key='e67088571ddf4f2789b59730c1586fd8.sM6fYMwvexGk1bAC')\n",
    "\n",
    "img_path = \"./screenshot.png\"\n",
    "with open(img_path, \"rb\") as img_file:\n",
    "    img_base = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4.5v\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": img_base\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"图中文字的内容是什么？如果是英文，再告诉我翻译后的中文是什么\"\n",
    "                }\n",
    "            ],\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "    ],\n",
    "    thinking={\n",
    "        \"type\": \"disabled\"\n",
    "    },\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "    if chunk.choices[0].delta.reasoning_content:\n",
    "        print(chunk.choices[0].delta.reasoning_content, end=\"\", flush=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图中文字是英文，内容为：  \n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English - to - German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English - to - French translation task, our model establishes a new single - model state - of - the - art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs well of the best models in the literature. We show the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.  \n",
      "\n",
      "\n",
      "翻译后的中文是：  \n",
      "主流的序列转换模型基于包含编码器和解码器的复杂循环或卷积神经网络。性能最佳的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构——Transformer，它完全基于注意力机制，彻底摒弃了循环和卷积。在两项机器翻译任务上的实验表明，这些模型在质量上更优，同时具有更强的并行性且训练时间显著减少。我们的模型在WMT 2014英语到德语翻译任务上 achieving 28.4 BLEU分数，比包括集成模型在内的现有最佳结果提高了超过2 BLEU。在WMT 2014英语到法语翻译任务上，我们的模型在八块GPU上训练3.5天后，建立了新的单模型最先进BLEU分数41.8，其训练成本仅为文献中最佳模型的很小一部分。我们通过将Transformer成功应用于大规模和有限训练数据的英语成分句法分析，展示了它在其他任务上的良好泛化能力。"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
