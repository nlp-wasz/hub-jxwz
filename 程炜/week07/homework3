import json
from datasets import Dataset
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForSeq2Seq,
    TrainingArguments,
    Trainer,
)
from peft import LoraConfig, get_peft_model
from tqdm import tqdm

def load_and_preprocess_data(train_path, dev_path):
    """加载和预处理CMRC2018数据集"""
    # 加载原始数据
    train_data = json.load(open(train_path))
    dev_data = json.load(open(dev_path))
    
    # 准备训练数据
    def prepare_samples(data):
        samples = []
        for article in data['data']:
            context = article['paragraphs'][0]['context']
            for qa in article['paragraphs'][0]['qas']:
                samples.append({
                    "context": context,
                    "question": qa['question'],
                    "answer": qa['answers'][0]['text'] if qa['answers'] else ""
                })
        return samples
    
    train_samples = prepare_samples(train_data)[:1000]  # 取部分数据用于演示
    dev_samples = prepare_samples(dev_data)[:100]
    
    return Dataset.from_list(train_samples), Dataset.from_list(dev_samples)

def initialize_qwen_model(model_path):
    """初始化Qwen模型和tokenizer"""
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        use_fast=False,
        trust_remote_code=True
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
        torch_dtype=torch.float16
    )
    
    return tokenizer, model

def create_prompt_template(example):
    """创建问答任务的提示模板"""
    return (
        "<|im_start|>system\n"
        "你是一个专业的知识问答系统，请根据提供的上下文回答问题。"
        "如果上下文不包含答案，请回答'不知道'。<|im_end|>\n"
        f"<|im_start|>user\n上下文：{example['context']}\n"
        f"问题：{example['question']}<|im_end|>\n"
        "<|im_start|>assistant\n"
    )

def process_qa_example(example, tokenizer, max_length=512):
    """处理单个问答样本"""
    # 构建提示
    prompt = create_prompt_template(example)
    prompt_tokens = tokenizer(prompt, add_special_tokens=False)
    
    # 处理答案
    answer = example["answer"] if example["answer"] else "不知道"
    answer_tokens = tokenizer(answer, add_special_tokens=False)
    
    # 组合输入和标签
    input_ids = prompt_tokens["input_ids"] + answer_tokens["input_ids"] + [tokenizer.pad_token_id]
    attention_mask = prompt_tokens["attention_mask"] + answer_tokens["attention_mask"] + [1]
    labels = [-100] * len(prompt_tokens["input_ids"]) + answer_tokens["input_ids"] + [tokenizer.pad_token_id]
    
    # 截断超过最大长度的序列
    if len(input_ids) > max_length:
        input_ids = input_ids[:max_length]
        attention_mask = attention_mask[:max_length]
        labels = labels[:max_length]
    
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

def setup_lora(model):
    """配置LoRA"""
    config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = get_peft_model(model, config)
    model.print_trainable_parameters()
    return model

def train_model():
    """训练模型的主函数"""
    # 1. 加载数据
    print("加载数据...")
    train_dataset, eval_dataset = load_and_preprocess_data(
        "cmrc2018_public/train.json",
        "cmrc2018_public/dev.json"
    )
    
    # 2. 初始化模型
    print("初始化Qwen模型...")
    model_path = "Qwen/Qwen1.5-0.5B"  # 可以使用更大的模型如Qwen1.5-7B获得更好效果
    tokenizer, model = initialize_qwen_model(model_path)
    
    # 3. 处理数据
    print("处理训练数据...")
    process_func = lambda x: process_qa_example(x, tokenizer)
    tokenized_train = train_dataset.map(process_func, remove_columns=train_dataset.column_names)
    tokenized_eval = eval_dataset.map(process_func, remove_columns=eval_dataset.column_names)
    
    # 4. 设置LoRA
    print("配置LoRA...")
    model = setup_lora(model)
    
    # 5. 训练配置
    print("设置训练参数...")
    training_args = TrainingArguments(
        output_dir="./qwen_qa_lora",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=8,
        learning_rate=2e-5,
        num_train_epochs=3,
        logging_steps=50,
        evaluation_strategy="steps",
        eval_steps=100,
        save_steps=100,
        fp16=True,
        report_to="none"
    )
    
    # 6. 创建Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_eval,
        data_collator=DataCollatorForSeq2Seq(
            tokenizer=tokenizer,
            padding=True
        ),
    )
    
    # 7. 开始训练
    print("开始训练...")
    trainer.train()
    
    # 8. 保存模型
    print("保存模型...")
    trainer.save_model()
    tokenizer.save_pretrained("./qwen_qa_lora")

def answer_question(model, tokenizer, context, question, device="cuda"):
    """使用微调后的模型回答问题"""
    # 构建提示
    messages = [
        {"role": "system", "content": "你是一个专业的知识问答系统，请根据提供的上下文回答问题。如果上下文不包含答案，请回答'不知道'。"},
        {"role": "user", "content": f"上下文：{context}\n问题：{question}"}
    ]
    
    # 应用聊天模板
    formatted_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # Tokenize输入
    model_inputs = tokenizer([formatted_text], return_tensors="pt").to(device)
    
    # 生成答案
    with torch.no_grad():
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=100,
            do_sample=False,
            temperature=0.1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # 解码答案
    generated_ids = generated_ids[:, model_inputs.input_ids.shape[1]:]
    answer = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    return answer.strip()

def test_model():
    """测试训练好的模型"""
    # 加载基础模型
    model_path = "Qwen/Qwen1.5-0.5B"
    tokenizer, model = initialize_qwen_model(model_path)
    
    # 加载LoRA适配器
    model.load_adapter("./qwen_qa_lora")
    model.to("cuda")
    
    # 测试用例
    test_cases = [
        {
            "context": "北京是中国的首都，位于华北平原北部。",
            "question": "中国的首都是哪里？"
        },
        {
            "context": "Python是一种高级编程语言，由Guido van Rossum于1991年首次发布。",
            "question": "Python是谁发明的？"
        },
        {
            "context": "太阳系有八大行星。",
            "question": "地球的卫星叫什么？"  # 上下文不包含答案
        }
    ]
    
    for case in test_cases:
        answer = answer_question(model, tokenizer, case["context"], case["question"])
        print(f"问题: {case['question']}")
        print(f"上下文: {case['context']}")
        print(f"回答: {answer}")
        print("-" * 50)

if __name__ == "__main__":
    # 训练模型
    train_model()
    
    # 测试模型
    test_model()
