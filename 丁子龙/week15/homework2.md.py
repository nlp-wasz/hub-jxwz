import sys
import os

# è¿è¡Œmineruè§£æpdfä¸ºmarkdown

# os.system("mineru -p 2509-MinerU2.5.pdf -o ./output/")
# ç»“æœè·¯å¾„ 2025-12-12_15-43.jpg
import torch
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI  # æˆ–ç”¨å…¶ä»– LLM
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
#æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download

# ç¡®ä¿å·²è¿è¡Œï¼šollama pull qwen3:8b  # æˆ– qwen2.5:7bã€qwen:14b ç­‰
Settings.llm = Ollama(
    model="qwen3:8b",        # â† ä¸ `ollama list` ä¸­åç§°ä¸¥æ ¼ä¸€è‡´
    request_timeout=300.0,   # å¤§æ¨¡å‹ç”Ÿæˆæ…¢ï¼Œè°ƒé«˜è¶…æ—¶
    # base_url="http://localhost:11434",  # é»˜è®¤ï¼Œå¯çœç•¥
)

print("âœ… Ollama LLM [qwen3:8b] å·²é…ç½®")
# 1ï¸âƒ£ æ˜¾å¼ä» ModelScope ä¸‹è½½æ¨¡å‹ï¼ˆç¡®ä¿æ¥æºå¯æ§ï¼‰
EMBED_MODEL_ID = "iic/gte_Qwen2-1.5B-instruct"
LOCAL_MODEL_DIR = f"./models/{EMBED_MODEL_ID}"  # â†’ ./models/iic_nan-bee-embedding

if not os.path.exists(LOCAL_MODEL_DIR):
    print(f"ğŸ“¥ æ­£åœ¨ä» ModelScope ä¸‹è½½æ¨¡å‹: {EMBED_MODEL_ID} â†’ {LOCAL_MODEL_DIR}")
    snapshot_download(
        model_id=EMBED_MODEL_ID,
        cache_dir="./models",  # ä¸‹è½½åˆ° ./models/
        revision="master",
        local_files_only=False,
    )
    # snapshot_download è¿”å›çš„æ˜¯å®é™…è·¯å¾„ï¼Œä¾‹å¦‚: ./models/iic/nan-bee-embedding/
    # æˆ‘ä»¬ç»Ÿä¸€é‡å‘½å/è·å–
    import glob
    actual_path = glob.glob(f"./models/{EMBED_MODEL_ID.split('/')[0]}/*")[0]
    if actual_path != LOCAL_MODEL_DIR:
        os.rename(actual_path, LOCAL_MODEL_DIR)
else:
    print(f"âœ… æ¨¡å‹å·²å­˜åœ¨æœ¬åœ°: {LOCAL_MODEL_DIR}")

# 2ï¸âƒ£ ä½¿ç”¨æœ¬åœ°è·¯å¾„åˆå§‹åŒ– embeddingï¼ˆ100% ç¡®å®šæ¥æºï¼‰
Settings.embed_model = HuggingFaceEmbedding(
    model_name=LOCAL_MODEL_DIR,  # â† å…³é”®ï¼šç”¨æœ¬åœ°è·¯å¾„
    trust_remote_code=True,
    device="cuda" if torch.cuda.is_available() else "cpu",
)
print(f"ğŸ‰ Embedding æ¨¡å‹å·²åŠ è½½ï¼ˆæ¥è‡ªæœ¬åœ° {LOCAL_MODEL_DIR}ï¼‰")

# 1. åŠ è½½å•ä¸ª markdown æ–‡ä»¶ï¼ˆSimpleDirectoryReader ä¹Ÿæ”¯æŒå•æ–‡ä»¶ listï¼‰
documents = SimpleDirectoryReader(input_files=["/home/dzl/baDouNLP/week/Week15/homework/output/2509-MinerU2.5/auto/2509-MinerU2.5.md"]).load_data()

# 2. æ„å»ºå‘é‡ç´¢å¼•ï¼ˆé»˜è®¤ç”¨ OpenAI embeddingï¼Œå¯æ¢ä¸ºæœ¬åœ°æ¨¡å‹å¦‚ BAAI/bge-smallï¼‰
index = VectorStoreIndex.from_documents(documents)

# 3. æŸ¥è¯¢
query_engine = index.as_query_engine()
response = query_engine.query("ä»‹ç»ä¸€ä¸‹mineru")
print(response)
# ç»“æœå±•ç¤º 2025-12-12_17-02.jpg

































