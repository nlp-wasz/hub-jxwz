import asyncio
import os
os.environ["OPENAI_API_KEY"] = "sk-ea07bf0880504b75a31b1bce38437fcf"
os.environ["OPENAI_BASE_URL"] = "https://dashscope.aliyuncs.com/compatible-mode/v1"

import json
import requests
import urllib.parse
from typing import List, Dict, Any

# å‡è®¾ä»¥ä¸‹å¯¼å…¥èƒ½å¤Ÿæ­£å¸¸å·¥ä½œï¼Œå®ƒä»¬é€šå¸¸æ¥è‡ª agents åº“
from agents import Agent, function_tool, AsyncOpenAI, OpenAIChatCompletionsModel, ModelSettings, Runner, \
    set_default_openai_api, set_tracing_disabled

set_default_openai_api("chat_completions")
set_tracing_disabled(True)

MODEL_NAME = "qwen-max"  # å‡è®¾è¿™æ˜¯ AliCloud å…¼å®¹æ¨¡å¼ä¸‹çš„ä¸€ä¸ªæ¨¡å‹åç§°
API_KEY = os.getenv("OPENAI_API_KEY", "sk-ea07bf0880504b75a31b1bce38437fcf")
BASE_URL = os.getenv("OPENAI_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")

# åˆå§‹åŒ– AsyncOpenAI å®¢æˆ·ç«¯
llm_client = AsyncOpenAI(
    api_key=API_KEY,
    base_url=BASE_URL
)

# å®šä¹‰æ¨¡å‹è®¾ç½®
model_settings = ModelSettings(
    model=MODEL_NAME,
    client=llm_client,
    temperature=0.3
)

# --- 2. å¤–éƒ¨å·¥å…·ï¼ˆJina Search & Crawlï¼‰ ---
JINA_API_KEY = "jina_8918effb420d4bff8530c9d9f3bbe536NWhiCZdKQFNgoFLd4aganV1XnsaA"

def search_jina(query: str) -> str:
    """é€šè¿‡jinaè¿›è¡Œè°·æ­Œæœç´¢ï¼Œè¿”å›JSONæ ¼å¼çš„æœç´¢ç»“æœå­—ç¬¦ä¸²"""
    print(f"-> [Jina Search] æ­£åœ¨æœç´¢: {query[:50]}...")
    try:
        # ç¡®ä¿æŸ¥è¯¢å‚æ•°æ˜¯ URL ç¼–ç çš„
        encoded_query = urllib.parse.quote(query)
        url = f"https://s.jina.ai/?q={encoded_query}&hl=zh-cn"
        headers = {
            "Accept": "application/json",
            "Authorization": f"Bearer {JINA_API_KEY}",
            "X-Respond-With": "no-content"  # Jina Search é»˜è®¤è¿”å›æ‘˜è¦å’Œå¼•ç”¨
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()  # æŠ›å‡º HTTP é”™è¯¯

        # Jina Search è¿”å›çš„æ˜¯ä¸€ä¸ªåŒ…å«ç»“æœçš„ JSON ç»“æ„ï¼Œæå–å…³é”®ä¿¡æ¯
        results = response.json().get('data', [])

        # æå–æ ‡é¢˜ã€é“¾æ¥å’Œæ‘˜è¦
        formatted_results = []
        for res in results:
            formatted_results.append({
                "title": res.get("title", ""),
                "url": res.get("url", ""),
                "snippet": res.get("content", "")
            })

        return json.dumps(formatted_results, ensure_ascii=False)
    except requests.exceptions.RequestException as e:
        print(f"Error during Jina Search: {e}")
        return json.dumps({"error": str(e), "query": query}, ensure_ascii=False)
    except Exception as e:
        print(f"Unexpected error in Jina Search: {e}")
        return json.dumps({"error": str(e), "query": query}, ensure_ascii=False)


async def crawl_jina(url: str) -> str:
    """é€šè¿‡jinaæŠ“å–å®Œæ•´ç½‘é¡µå†…å®¹ï¼Œè¿”å›Markdownæ ¼å¼çš„æ–‡æœ¬"""
    print(f"-> [Jina Crawl] æ­£åœ¨æŠ“å–: {url[:50]}...")
    try:
        # Jina Reader API
        headers = {
            "Accept": "application/json",
            "Authorization": f"Bearer {JINA_API_KEY}",
            "X-Respond-With": "content",  # è¯·æ±‚è¿”å›å®Œæ•´å†…å®¹
            "X-Content-Type": "markdown"  # è¯·æ±‚è¿”å› Markdown æ ¼å¼
        }
        # ä½¿ç”¨ r.jina.ai ä½œä¸ºä»£ç†
        response = requests.get("https://r.jina.ai/" + url, headers=headers, timeout=20)
        response.raise_for_status()

        # è¿”å›å†…å®¹é€šå¸¸åœ¨ 'data' å­—æ®µçš„ 'content' ä¸­
        content = response.json().get("data", {}).get("content", f"æ— æ³•æŠ“å– URL: {url} çš„å†…å®¹ã€‚")

        return content
    except requests.exceptions.RequestException as e:
        print(f"Error during Jina Crawl for {url}: {e}")
        return f"æŠ“å–å¤±è´¥: {e}"
    except Exception as e:
        print(f"Unexpected error in Jina Crawl for {url}: {e}")
        return f"æŠ“å–å¤±è´¥: {e}"


# å°†åŒæ­¥å‡½æ•°åŒ…è£…æˆå¼‚æ­¥ï¼Œä»¥ä¾¿åœ¨ Agents å¼‚æ­¥ç¯å¢ƒä¸­ä½¿ç”¨
async def async_search_jina(query: str) -> str:
    """å¼‚æ­¥è°ƒç”¨ Jina æœç´¢"""
    return await asyncio.to_thread(search_jina, query)


async def async_crawl_jina(url: str) -> str:
    """å¼‚æ­¥è°ƒç”¨ Jina æŠ“å–"""
    return await asyncio.to_thread(crawl_jina, url)

REVIEW_CRITERIA = """
è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ‡å‡†è¯„ä¼°ç« èŠ‚å†…å®¹è´¨é‡ï¼š
1. ã€å®Œæ•´æ€§ã€‘æ˜¯å¦è¦†ç›–ç« èŠ‚ä¸»é¢˜æ ¸å¿ƒè¦ç‚¹ï¼Ÿæœ‰æ— é‡å¤§é—æ¼ï¼Ÿ
2. ã€å‡†ç¡®æ€§ã€‘å…³é”®äº‹å®ã€æ•°æ®ã€æœ¯è¯­æ˜¯å¦ä¸åŸå§‹ææ–™ä¸€è‡´ï¼Ÿæœ‰æ— æé€ ï¼Ÿ
3. ã€é€»è¾‘æ€§ã€‘ç»“æ„æ˜¯å¦æ¸…æ™°ï¼ˆå¼•è¨€-ä¸»ä½“-å°ç»“ï¼‰ï¼Ÿæ®µè½è¡”æ¥æ˜¯å¦è‡ªç„¶ï¼Ÿ
4. ã€è¯­è¨€è§„èŒƒã€‘æ˜¯å¦ç”¨è¯­ä¸“ä¸šã€ç®€æ´ã€æ— å£è¯­åŒ–/é‡å¤ï¼Ÿæœ‰æ— è¯­æ³•é”™è¯¯ï¼Ÿ
5. ã€åŸåˆ›æ€§ã€‘æ˜¯å¦é¿å…ç›´æ¥å¤åˆ¶åŸæ–‡ï¼Ÿæ˜¯å¦åˆç†æ”¹å†™ä¸æ•´åˆï¼Ÿ

è¯·è¾“å‡º JSON æ ¼å¼ç»“æœï¼š
{
  "is_acceptable": true/false,
  "score": 0~10,
  "issues": ["é—®é¢˜1", "é—®é¢˜2", ...],
  "suggestions": ["å»ºè®®1", "å»ºè®®2", ...],
  "missing_keywords": ["åº”æåŠä½†ç¼ºå¤±çš„å…³é”®è¯", ...]
}
"""

external_client = AsyncOpenAI(
    api_key=os.environ["OPENAI_API_KEY"],
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

# --- 3. ä»£ç†å®šä¹‰ (Agents) ---
orchestrator_system_prompt = """
ä½ æ˜¯ä¸€åæ·±åº¦ç ”ç©¶ä¸“å®¶å’Œé¡¹ç›®ç»ç†ã€‚ä½ çš„ä»»åŠ¡æ˜¯åè°ƒæ•´ä¸ªç ”ç©¶é¡¹ç›®ï¼ŒåŒ…æ‹¬ï¼š
1. **ç ”ç©¶è§„åˆ’ (ç”Ÿæˆå¤§çº²):** æ ¹æ®ç”¨æˆ·æä¾›çš„ç ”ç©¶ä¸»é¢˜å’Œåˆæ­¥æœç´¢ç»“æœï¼Œç”Ÿæˆä¸€ä¸ªè¯¦å°½ã€é€»è¾‘ä¸¥å¯†ã€ç»“æ„æ¸…æ™°çš„æŠ¥å‘Šå¤§çº²ã€‚å¤§çº²å¿…é¡»ä»¥ä¸¥æ ¼çš„ JSON æ ¼å¼è¾“å‡ºï¼Œç”¨äºæŒ‡å¯¼åç»­çš„ç« èŠ‚å†…å®¹æ£€ç´¢å’Œèµ·è‰å·¥ä½œã€‚
2. **æŠ¥å‘Šæ•´åˆ (ç»„è£…):** åœ¨æ‰€æœ‰ç« èŠ‚å†…å®¹èµ·è‰å®Œæˆåï¼Œå°†å®ƒä»¬æ•´åˆåœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ç¯‡æµç•…ã€è¿è´¯ã€æ ¼å¼ä¼˜ç¾çš„æœ€ç»ˆç ”ç©¶æŠ¥å‘Šã€‚æŠ¥å‘Šå¿…é¡»åŒ…æ‹¬æ‘˜è¦ã€å®Œæ•´çš„ç« èŠ‚å†…å®¹ã€ç»“è®ºå’Œå¼•ç”¨æ¥æºåˆ—è¡¨ã€‚
"""
DeepResearchAgent = Agent(
    "Deep Research Orchestrator",
    instructions=orchestrator_system_prompt,
    model=OpenAIChatCompletionsModel(
        model="qwen-max",
        openai_client=external_client,
    ),
)

ReviewAgent = Agent(
    "Research Report Reviewer",
    instructions=REVIEW_CRITERIA,
    model=OpenAIChatCompletionsModel(
        model="qwen-max",
        openai_client=external_client,
    ),
)

# 3.2. å†…å®¹èµ·è‰ä»£ç† (Drafting Agent)
drafting_system_prompt = """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„å†…å®¹æ’°ç¨¿äººã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†æä¾›çš„åŸå§‹ç½‘é¡µæŠ“å–å†…å®¹å’Œæœç´¢ç»“æœï¼Œæ ¹æ®æŒ‡å®šçš„ç« èŠ‚ä¸»é¢˜ï¼Œæ’°å†™æˆä¸€ç¯‡ç»“æ„åˆç†ã€é‡ç‚¹çªå‡ºã€ä¿¡æ¯å‡†ç¡®çš„æŠ¥å‘Šç« èŠ‚ã€‚
ä½ å¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š
1. **èšç„¦ä¸»é¢˜:** ä¸¥æ ¼å›´ç»•ç»™å®šçš„ 'ç« èŠ‚ä¸»é¢˜' è¿›è¡Œæ’°å†™ã€‚
2. **ä¿¡æ¯æ¥æº:** åªèƒ½ä½¿ç”¨æä¾›çš„ 'åŸå§‹ç½‘é¡µå†…å®¹' å’Œ 'æœç´¢ç»“æœæ‘˜è¦' ä¸­çš„ä¿¡æ¯ã€‚
3. **æ ¼å¼:** ä½¿ç”¨ Markdown æ ¼å¼ã€‚
4. **å¼•ç”¨:** å¯¹äºæ–‡ä¸­å¼•ç”¨çš„å…³é”®äº‹å®å’Œæ•°æ®ï¼Œå¿…é¡»åœ¨æ®µè½æœ«å°¾ç”¨è„šæ³¨æˆ–æ‹¬å·æ ‡è®°å¼•ç”¨çš„æ¥æº URLï¼Œä¾‹å¦‚ [æ¥æº: URL]ã€‚
"""
DraftingAgent = Agent(
    "Content Drafting Specialist",
    instructions=drafting_system_prompt,
    model=OpenAIChatCompletionsModel(
        model="qwen-max",
        openai_client=external_client,
    ),
)

async def review_and_revise(
    section_title: str,
    draft: str,
    raw_materials: str,
    search_summary: str,
    max_retries: int = 2
) -> str:
    current_draft = draft
    for attempt in range(max_retries + 1):
        # â€”â€” Step: Review â€”â€”
        review_prompt = f"""
ä½ æ˜¯ä¸€åä¸“ä¸šçš„å†…å®¹è´¨é‡å®¡æ ¸ä¸“å®¶ã€‚

**ç« èŠ‚ä¸»é¢˜ï¼š** {section_title}

**å½“å‰è‰ç¨¿ï¼š**
{current_draft}

**åŸå§‹å‚è€ƒææ–™ï¼ˆç”¨äºæ ¸å¯¹äº‹å®ï¼‰ï¼š**
{raw_materials[:4000]}...

**åˆæ£€æœç´¢æ‘˜è¦ï¼ˆè¾…åŠ©åˆ¤æ–­è¦†ç›–åº¦ï¼‰ï¼š**
{search_summary[:2000]}...

{REVIEW_CRITERIA}
è¯·ä¸¥æ ¼æŒ‰ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦ä»»ä½•é¢å¤–æ–‡æœ¬ã€‚
"""

        try:
            review_result = await Runner.run(
                ReviewAgent,  # â† å‡è®¾ä½ å·²å®šä¹‰ ReviewAgent
                review_prompt,
            )
            feedback = json.loads(review_result.final_output.strip("```json\n").strip("\n```"))
        except Exception as e:
            print(f"âš ï¸ [{section_title}] Review è§£æå¤±è´¥ï¼Œè·³è¿‡å®¡æ ¸: {e}")
            return current_draft  # å®¹é”™ï¼šå®¡æ ¸å¤±è´¥åˆ™æ¥å—å½“å‰ç¨¿

        is_ok = feedback.get("is_acceptable", False)
        score = feedback.get("score", 0)
        issues = feedback.get("issues", [])
        suggestions = feedback.get("suggestions", [])

        print(f"ğŸ” [{section_title}] å®¡æ ¸ #{attempt + 1} | åˆ†æ•°: {score}/10 | é€šè¿‡: {is_ok}")
        if issues:
            print(f"   â— é—®é¢˜: {'; '.join(issues[:3])}")

        if is_ok or attempt == max_retries:
            # æ¥å—ç»ˆç¨¿ï¼ˆå³ä½¿æœ€åä¸€æ¬¡ä¸åˆæ ¼ï¼Œä¹Ÿç»ˆæ­¢å¾ªç¯ï¼‰
            return current_draft

        # â€”â€” Step: Revise â€”â€”
        revise_prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±å†…å®¹ç¼–è¾‘ã€‚è¯·æ ¹æ®å®¡æ ¸åé¦ˆä¿®æ”¹ç« èŠ‚è‰ç¨¿ã€‚

**ç« èŠ‚ä¸»é¢˜ï¼š** {section_title}

**å½“å‰è‰ç¨¿ï¼ˆéœ€ä¿®æ”¹ï¼‰ï¼š**
{current_draft}

**å®¡æ ¸åé¦ˆï¼š**
- é—®é¢˜ï¼š{issues}
- å»ºè®®ï¼š{suggestions}
- ç¼ºå¤±å…³é”®è¯ï¼š{feedback.get('missing_keywords', [])}

**ä¿®æ”¹è¦æ±‚ï¼š**
1. ä¿ç•™åŸæ„ï¼Œä»…æå‡è´¨é‡ï¼›
2. è¡¥å……ç¼ºå¤±è¦ç‚¹ï¼Œä¿®æ­£äº‹å®é”™è¯¯ï¼›
3. ä¼˜åŒ–é€»è¾‘æµä¸è¯­è¨€è¡¨è¾¾ï¼›
4. è¾“å‡ºå®Œæ•´ä¿®è®¢ç‰ˆï¼Œä¸è¦è§£é‡Šã€‚

è¯·ç›´æ¥è¾“å‡ºä¿®è®¢åçš„ç« èŠ‚æ­£æ–‡ï¼ˆMarkdown æ ¼å¼ï¼‰ã€‚
"""

        try:
            revised = await Runner.run(
                DraftingAgent,  # å¯å¤ç”¨èµ·è‰ Agentï¼Œæˆ–å®šä¹‰ ReviseAgent
                revise_prompt,
            )
            current_draft = revised.final_output
            print(f"âœï¸ [{section_title}] å·²ä¿®è®¢ï¼ˆç¬¬ {attempt + 1} æ¬¡ï¼‰")
        except Exception as e:
            print(f"âš ï¸ [{section_title}] ä¿®è®¢å¤±è´¥ï¼Œä¿ç•™ä¸Šä¸€ç‰ˆ: {e}")
            break  # ä¿®è®¢å¤±è´¥åˆ™ç»ˆæ­¢å¾ªç¯

    return current_draft

async def process_section(section: dict) -> str:
    section_title = section.get("section_title", "Untitled")
    search_keywords = section.get("search_keywords", "")
    print(f"\nğŸš€ [{section_title}] å¯åŠ¨å¤„ç†...")

    # 1. æ£€ç´¢ + æŠ“å–ï¼ˆåŒå‰ï¼‰
    section_query = f"{section_title} æœç´¢å…³é”®è¯: {search_keywords}"
    section_search_results_str = await async_search_jina(section_query)

    urls_to_crawl = []
    try:
        search_results = json.loads(section_search_results_str)
        urls_to_crawl = [res['url'] for res in search_results if res.get('url')][:2]
    except Exception as e:
        print(f"âš ï¸ [{section_title}] è§£ææœç´¢ç»“æœå¤±è´¥: {e}")

    crawl_tasks = [async_crawl_jina(url) for url in urls_to_crawl]
    crawl_results = await asyncio.gather(*crawl_tasks, return_exceptions=True)

    crawled_content = []
    for url, content in zip(urls_to_crawl, crawl_results):
        if isinstance(content, Exception):
            crawled_content.append(f"--- URL: {url} ---\n[æŠ“å–å¤±è´¥]\n")
        else:
            crawled_content.append(f"--- URL: {url} ---\n{str(content)[:3000]}...\n")
    raw_materials = "\n\n".join(crawled_content)

    # 2. åˆç¨¿ç”Ÿæˆ
    draft_prompt = f"""
    **ç« èŠ‚ä¸»é¢˜:** {section_title}
    **æœç´¢æ‘˜è¦:** {section_search_results_str[:3000]}...
    **åŸå§‹ææ–™:** {raw_materials}

    è¯·æ’°å†™æœ¬ç« å†…å®¹ï¼ˆ500â€“800å­—ï¼‰ï¼Œç»“æ„æ¸…æ™°ã€ä¸“ä¸šä¸¥è°¨ã€‚
    """
    try:
        draft_result = await Runner.run(DraftingAgent, draft_prompt)
        first_draft = draft_result.final_output
    except Exception as e:
        return f"## {section_title}\n\n[èµ·è‰å¤±è´¥: {e}]"

    # 3. âœ… ReAct å¾ªç¯ï¼šReview â†’ Revise â†’ Accept
    final_draft = await review_and_revise(
        section_title=section_title,
        draft=first_draft,
        raw_materials=raw_materials,
        search_summary=section_search_results_str,
        max_retries=2
    )

    print(f"âœ… [{section_title}] ç»ˆç¨¿ç¡®è®¤")
    return f"## {section_title}\n\n{final_draft}"

# --- 4. æ·±åº¦ç ”ç©¶æ ¸å¿ƒæµç¨‹ ---

async def deep_research(query: str, max_sections: int = 5) -> str:
    """
    æ‰§è¡Œæ·±åº¦ç ”ç©¶æµç¨‹ï¼šè§„åˆ’ -> æ£€ç´¢ -> æŠ“å– -> èµ·è‰ -> æ•´åˆã€‚
    """
    print(f"\n--- Deep Research for: {query} ---\n")

    # 1. åˆæ­¥æ£€ç´¢
    print("Step 1: è¿›è¡Œåˆæ­¥æ£€ç´¢...")
    initial_search_results_str = await async_search_jina(query)
    print(initial_search_results_str)

    # 2. ç”Ÿæˆç ”ç©¶å¤§çº² (ä½¿ç”¨ JSON æ¨¡å¼ç¡®ä¿ç»“æ„åŒ–è¾“å‡º)
    print("\nStep 2: åŸºäºåˆæ­¥ç»“æœç”Ÿæˆç ”ç©¶å¤§çº²...")

    init_prompt = f"""ç ”ç©¶ä¸»é¢˜: {query}
åˆæ­¥æœç´¢ç»“æœæ‘˜è¦: {initial_search_results_str}
"""

    outline_prompt = init_prompt + """è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æŠ¥å‘Šå¤§çº²ã€‚å¤§çº²å¿…é¡»åŒ…å«ä¸€ä¸ª 'title' å’Œä¸€ä¸ª 'sections' æ•°ç»„ã€‚
æ¯ä¸ªç« èŠ‚å¯¹è±¡å¿…é¡»åŒ…å« 'section_title' å’Œ 'search_keywords' (ç”¨äºç²¾ç¡®æ£€ç´¢çš„å…³é”®è¯)ã€‚

ç¤ºä¾‹è¾“å‡º JSON æ ¼å¼å¦‚ä¸‹ï¼Œåªè¦jsonï¼Œä¸è¦æœ‰å…¶ä»–è¾“å‡º
{
    "title": "å…³äº XX çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Š",
    "sections": [
        {"section_title": "å¼•è¨€ä¸èƒŒæ™¯", "search_keywords": "å†å², ç°çŠ¶"},
        {"section_title": "æ ¸å¿ƒè¦ç´ ä¸æœºåˆ¶", "search_keywords": "å…³é”®æ¦‚å¿µ, å·¥ä½œåŸç†"},
        {"section_title": "åº”ç”¨ä¸å½±å“", "search_keywords": "è¡Œä¸šåº”ç”¨, ç¤¾ä¼šå½±å“"}
    ]
}
"""
    try:
        # è°ƒç”¨ Orchestrator Agent ç”Ÿæˆ JSON æ ¼å¼çš„å¤§çº²
        outline_response = await Runner.run(
            DeepResearchAgent,
            outline_prompt,
        )
        print(outline_response)
        outline_json = json.loads(outline_response.final_output.strip("```json").strip("```"))

    except Exception as e:
        print(f"Error generating outline: {e}. Falling back to a simple structure.")
        # å¤±è´¥æ—¶æä¾›é»˜è®¤å¤§çº²
        outline_json = {
            "title": f"å…³äº {query} çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Š",
            "sections": [
                {"section_title": "å¼•è¨€ä¸èƒŒæ™¯", "search_keywords": f"{query}, å†å², ç°çŠ¶"},
                {"section_title": "æ ¸å¿ƒè¦ç´ ä¸æœºåˆ¶", "search_keywords": f"{query}, å·¥ä½œåŸç†, å…³é”®æŠ€æœ¯"},
                {"section_title": "åº”ç”¨ä¸å½±å“", "search_keywords": f"{query}, è¡Œä¸šåº”ç”¨, ç¤¾ä¼šå½±å“"},
                {"section_title": "ç»“è®ºä¸å±•æœ›", "search_keywords": f"{query}, å‘å±•è¶‹åŠ¿, æŒ‘æˆ˜"}
            ]
        }

    research_title = outline_json.get("title", f"å…³äº {query} çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Š")
    sections = outline_json.get("sections", [])
    if len(sections) > max_sections:
        sections = sections[:max_sections]

    print(f"æŠ¥å‘Šæ ‡é¢˜: {research_title}")
    print(f"è§„åˆ’äº† {len(sections)} ä¸ªç« èŠ‚ã€‚")

    # 3. é€ç« è¿›è¡Œæ£€ç´¢ã€æŠ“å–å’Œèµ·è‰
    drafted_sections = []

    # ä¸»æµç¨‹ï¼šå¹¶å‘å¤„ç†æ‰€æœ‰ç« èŠ‚
    drafted_sections = await asyncio.gather(
        *(process_section(section) for section in sections),
        return_exceptions=True  # é˜²æ­¢ä¸€ä¸ªç« èŠ‚å¤±è´¥å¯¼è‡´æ•´ä½“å´©æºƒ
    )

    # 4. æŠ¥å‘Šæ•´åˆä¸æœ€ç»ˆè¾“å‡º (è°ƒç”¨ Orchestrator Agent)
    print("\nStep 4: æ•´åˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Š...")
    full_report_draft = "\n\n".join(drafted_sections)

    final_prompt = f"""
    è¯·å°†ä»¥ä¸‹æ‰€æœ‰ç« èŠ‚å†…å®¹æ•´åˆä¸ºä¸€ç¯‡å®Œæ•´çš„ã€ä¸“ä¸šçš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šã€‚

    **æŠ¥å‘Šæ ‡é¢˜:** {research_title}

    **å·²èµ·è‰çš„ç« èŠ‚å†…å®¹:**
    {full_report_draft}

    **ä»»åŠ¡è¦æ±‚:**
    1. åœ¨æŠ¥å‘Šå¼€å¤´æ·»åŠ ä¸€ä¸ª**ã€æ‘˜è¦ã€‘**ï¼Œæ€»ç»“æŠ¥å‘Šçš„ä¸»è¦å‘ç°å’Œç»“è®ºã€‚
    2. ä¿æŒå„ç« èŠ‚ä¹‹é—´çš„è¿è´¯æ€§ã€‚
    3. åœ¨æŠ¥å‘Šæœ«å°¾æ·»åŠ ä¸€ä¸ª**ã€ç»“è®ºä¸å±•æœ›ã€‘**éƒ¨åˆ†ï¼ˆå¦‚æœå¤§çº²ä¸­æ²¡æœ‰ï¼‰ã€‚
    4. æ·»åŠ ä¸€ä¸ª**ã€å¼•ç”¨æ¥æºã€‘**åˆ—è¡¨ï¼Œåˆ—å‡ºæ‰€æœ‰ç« èŠ‚ä¸­æåˆ°çš„ URLã€‚
    5. æ•´ä½“æŠ¥å‘Šå¿…é¡»æ ¼å¼ä¼˜ç¾ï¼Œä½¿ç”¨ Markdown æ ¼å¼ã€‚
    """

    try:
        final_report = await Runner.run(
            DeepResearchAgent,
            final_prompt,
        )
        return final_report.final_output
    except Exception as e:
        return f"æœ€ç»ˆæŠ¥å‘Šæ•´åˆå¤±è´¥: {e}\n\nå·²å®Œæˆçš„ç« èŠ‚è‰ç¨¿:\n{full_report_draft}"

async def main():
    research_topic = "Agentic AIåœ¨è½¯ä»¶å¼€å‘ä¸­çš„æœ€æ–°åº”ç”¨å’ŒæŒ‘æˆ˜"
    final_report = await deep_research(research_topic)
    print(final_report)


# ä½¿ç”¨ Runner å¯åŠ¨å¼‚æ­¥ä¸»å‡½æ•°
if __name__ == "__main__":
    try:
        asyncio.run(main())
    except NameError:
        # Fallback to standard asyncio run if Runner is not defined or preferred
        asyncio.run(main())