[
    {
        "type": "text",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "text_level": 1,
        "bbox": [
            163,
            126,
            834,
            172
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Junbo $\\mathbf { N i u } ^ { 1 , 2 * }$ , Zheng $\\mathbf { L i u } ^ { 1 , 2 * }$ , Zhuangcheng $\\mathbf { G u } ^ { 1 * }$ , Bin Wang1∗‡, Linke Ouyang1∗ Zhiyuan ${ \\mathbf { Z } } { \\mathbf { h } } { \\mathbf { a 0 } } ^ { 1 * }$ , Tao $\\mathbf { C h u ^ { \\mathrm { 1 * } } }$ , Tianyao $\\mathbf { H e ^ { 1 * } }$ , Fan ${ \\mathbf { W } } { \\mathbf { u } } ^ { 1 * }$ , Qintong Zhang $^ { 1 , 2 \\ast }$ , Zhenjiang $\\mathbf { J i n ^ { 1 * } }$ Guang Liang1, Rui Zhang1, Wenzheng Zhang1,2, Yuan ${ \\bf Q } { \\bf u } ^ { 1 }$ , Zhifei $\\mathbf { R e n } ^ { 1 }$ , Yuefeng Sun1   \nYuanhong Zheng1, Dongsheng $\\mathbf { M } \\mathbf { a } ^ { 1 }$ , Zirui Tang1,3, Boyu $\\mathbf { N i u } ^ { 1 , 3 }$ , Ziyang Miao1, Hejun Dong1   \nSiyi $\\mathbf { Q i a n } ^ { 1 , 2 }$ , Junyuan Zhang1, Jingzhou Chen1,2, Fangdong Wang1, Xiaomeng Zhao1, Liqun Wei1   \nWei $\\mathbf { L i } ^ { 1 }$ , Shasha Wang1, Ruiliang ${ \\bf { \\chi } } _ { \\bf { u } } ^ { 1 }$ , Yuanyuan $\\mathbf { C a o ^ { 1 } }$ , Lu Chen1, Qianqian $\\mathbf { W _ { u } } ^ { 1 }$ , Huaiyu $\\mathbf { G u } ^ { 1 }$   \nLindong ${ { \\mathbf { L } } { \\mathbf { u } } ^ { 1 } }$ , Keming Wang1, Dechen $\\mathbf { L i n ^ { 1 } }$ , Guanlin Shen1, Xuanhe Zhou1,3, Linfeng Zhang3   \nYuhang Zang1, Xiaoyi Dong1, Jiaqi Wang1, Bo Zhang1, Lei Bai1, Pei $\\mathbf { C h u ^ { 1 } }$ , Weijia $\\mathbf { L i } ^ { 1 }$ , Jiang $\\mathbf { W _ { u } } ^ { 1 }$ Lijun $\\mathbf { W _ { u } } ^ { 1 }$ , Zhenxiang $\\mathbf { L i } ^ { 1 }$ , Guangyu Wang1, Zhongying ${ { \\bf { T } } { \\bf { u } } ^ { 1 } }$ , Chao ${ \\bf { \\chi } } _ { \\bf { u } } ^ { 1 }$ , Kai Chen1 ",
        "bbox": [
            171,
            176,
            823,
            352
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Yu Qiao1, Bowen Zhou1, Dahua Lin1 \u0000, Wentao Zhang1,2 \u0000, Conghui $\\mathbf { H e } ^ { 1 }$ \u0000 ",
        "bbox": [
            218,
            327,
            777,
            361
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1Shanghai Artificial Intelligence Laboratory, 2Peking University, 3Shanghai Jiao Tong University ",
        "bbox": [
            166,
            367,
            790,
            382
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language model that achieves state-of-the-art recognition accuracy while maintaining exceptional computational efficiency. Our approach employs a coarse-to-fine, two-stage parsing strategy that decouples global layout analysis from local content recognition. In the first stage, the model performs efficient layout analysis on downsampled images to identify structural elements, circumventing the computational overhead of processing high-resolution inputs. In the second stage, guided by the global layout, it performs targeted content recognition on native-resolution crops extracted from the original image, preserving fine-grained details in dense text, complex formulas, and tables. To support this strategy, we developed a comprehensive data engine that generates diverse, large-scale training corpora for both pretraining and fine-tuning. Ultimately, MinerU2.5 demonstrates strong document parsing ability, achieving state-of-the-art performance on multiple benchmarks, surpassing both general-purpose and domain-specific models across various recognition tasks, while maintaining significantly lower computational overhead. ",
        "bbox": [
            163,
            401,
            838,
            613
        ],
        "page_idx": 0
    },
    {
        "type": "discarded",
        "text": "OpenDataLab ",
        "bbox": [
            137,
            59,
            336,
            90
        ],
        "page_idx": 0
    },
    {
        "type": "discarded",
        "text": "上海人工智能实验室 Shanghai Artificial Intelligence Laboratory ",
        "bbox": [
            540,
            58,
            859,
            88
        ],
        "page_idx": 0
    },
    {
        "type": "discarded",
        "text": "\\* Equal contribution \u0000 Corresponding author ‡ Project leader Correspondence: Conghui He, heconghui@pjlab.org.cn Code: https://github.com/opendatalab/MinerU Model: https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B Date: September 30, 2025 ",
        "bbox": [
            163,
            631,
            616,
            705
        ],
        "page_idx": 0
    },
    {
        "type": "discarded",
        "text": "1 ",
        "bbox": [
            493,
            919,
            503,
            931
        ],
        "page_idx": 0
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            22,
            275,
            62,
            723
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Contents ",
        "text_level": 1,
        "bbox": [
            137,
            104,
            235,
            122
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1 Introduction 4 ",
        "text_level": 1,
        "bbox": [
            137,
            136,
            864,
            151
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2 Related Work ",
        "text_level": 1,
        "bbox": [
            137,
            165,
            264,
            178
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "5   \n2.1 Traditional Pipelines 5   \n2.2 General-Purpose Vision Language Models 6   \n2.3 Domain-Specific Vision Language Models 6 ",
        "bbox": [
            160,
            174,
            864,
            226
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "3 MinerU2.5 ",
        "text_level": 1,
        "bbox": [
            137,
            238,
            243,
            251
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "6 ",
        "text_level": 1,
        "bbox": [
            148,
            234,
            864,
            248
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "3.1 Model Architecture 6 ",
        "text_level": 1,
        "bbox": [
            145,
            251,
            866,
            266
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "3.2 Two-Stage Parsing Strategy 7   \n3.3 Training Recipe 8   \n3.3.1 Stage 0-Modality Alignment 8   \n3.3.2 Stage 1-Document Parsing Pre-training 9   \n3.3.3 Stage 2-Document Parsing Fine-tuning 9   \n3.3.4 Data Augmentation Strategies 10   \n3.4 Model Deployment 10 ",
        "bbox": [
            160,
            260,
            864,
            372
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "4 Data Engine ",
        "text_level": 1,
        "bbox": [
            137,
            386,
            254,
            400
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "11 ",
        "text_level": 1,
        "bbox": [
            142,
            385,
            857,
            397
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "4.1 Overall Workflow 11 ",
        "text_level": 1,
        "bbox": [
            142,
            400,
            861,
            414
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "4.1.1 Data Curation . 11   \n4.1.2 Pre-training Dataset Preparation 12   \n4.1.3 Fine-tuning Dataset Construction 13   \n4.2 Task Reformulation and Enhancement 13   \n4.2.1 Layout Analysis . 13   \n4.2.2 Formula Recognition 15   \n4.2.3 Table Recognition 16   \n4.3 Iterative Mining via Inference Consistency 17 ",
        "bbox": [
            158,
            415,
            862,
            537
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "5 Evaluation 19 ",
        "text_level": 1,
        "bbox": [
            137,
            549,
            864,
            563
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "5.1 Full-Document Parsing Task . 20 ",
        "text_level": 1,
        "bbox": [
            145,
            563,
            862,
            578
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "5.1.1 Evaluation Details and Metrics 21   \n5.1.2 Evaluation Results 21   \n5.2 Element-Specific Parsing Task 22   \n5.2.1 Layout Analysis . 22   \n5.2.2 Table Recognition . 23   \n5.2.3 Formula Recognition 24 ",
        "bbox": [
            160,
            570,
            864,
            670
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "6 Conclusion 25 ",
        "text_level": 1,
        "bbox": [
            137,
            681,
            862,
            696
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "A Qualitative examples 30 ",
        "text_level": 1,
        "bbox": [
            140,
            710,
            861,
            724
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "A.1 Overview . 31 ",
        "text_level": 1,
        "bbox": [
            143,
            727,
            862,
            738
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "A.1.1 Among PDF types 31   \nA.1.2 Among Table types 34   \nA.1.3 Among Formula types 36   \nA.2 Compare to Previous Versions 38   \nA.2.1 Table 38   \nA.2.2 Formula 40   \nA.2.3 Layout&OCR 42   \nA.3 Compare with Others 44   \nA.3.1 Table . 44   \nA.3.2 Formula 50 ",
        "bbox": [
            160,
            741,
            869,
            891
        ],
        "page_idx": 1
    },
    {
        "type": "discarded",
        "text": "2 ",
        "bbox": [
            493,
            919,
            504,
            931
        ],
        "page_idx": 1
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            140,
            58,
            862,
            74
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "A.3.3 Layout&OCR 53 ",
        "bbox": [
            191,
            106,
            862,
            122
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "B Prompt Details ",
        "text_level": 1,
        "bbox": [
            137,
            136,
            276,
            150
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "B.1 Layout Detection 56   \nB.2 Text Recognition . 56   \nB.3 Formula Recognition 56   \nB.4 Table Recognition 57 ",
        "bbox": [
            160,
            148,
            864,
            212
        ],
        "page_idx": 2
    },
    {
        "type": "discarded",
        "text": "3 ",
        "bbox": [
            493,
            920,
            504,
            931
        ],
        "page_idx": 2
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            137,
            58,
            862,
            74
        ],
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/b8d4ab93da53bbe886ed117763b8e05c7c53d871bf48bd94786a3fa30ba5f47a.jpg",
        "image_caption": [
            "Figure 1: Performance Highlights of MinerU2.5 on OmniDocBench. MinerU2.5 consistently outperforms both general-purpose VLMs (e.g., Gemini-2.5 Pro, Qwen2.5-VL-72B, GPT-4o) and domainspecific models (e.g., MonkeyOCR, dots.ocr, PP-StructureV3), establishing new performance records in text recognition, formula recognition, table recognition, and reading order prediction. Detailed results are presented in Table 5. "
        ],
        "image_footnote": [],
        "bbox": [
            150,
            108,
            849,
            444
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "1 Introduction ",
        "text_level": 1,
        "bbox": [
            137,
            559,
            308,
            579
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Document parsing [57] serves as a fundamental task in multimodal understanding, underpinning a variety of downstream applications such as information extraction [18, 43], Retrieval-Augmented Generation (RAG) [19, 56, 58] and intelligent document analysis [2, 4, 40]. In contrast to natural images, document images are characterized by significantly higher resolutions, denser content, and more complex structural layouts [20, 51, 52]. These inherent properties introduce a unique set of challenges. Firstly, the high resolution and fine-grained layout structures necessitate models capable of processing images at their native resolution. Secondly, the text-dense and often lengthy nature of documents imposes stringent requirements on the parameter efficiency and robustness of the models. Thirdly, the success of OCR is contingent not only on precise text recognition but also heavily on reliable layout analysis and efficient inference. ",
        "bbox": [
            137,
            590,
            862,
            742
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Contemporary approaches to document parsing can be broadly categorized into two paradigms: pipeline-based approaches [8, 24, 32, 46] and end-to-end approaches based on VLMs [1, 3, 7, 37, 52]. The former employs a modular design, decomposing the task into discrete stages such as layout detection, reading order prediction, and recognition of text lines, formulas, and tables. Each stage is handled by a specialized model. While this approach offers interpretability, it suffers from a cumbersome workflow and the potential for error propagation across modules. The latter paradigm exhibits superior semantic modeling capabilities, yet it is still widely constrained by the hallucination problem in long-document processing and suffers from severe efficiency bottlenecks when dealing with high-resolution inputs. A critical factor limiting the performance and efficiency of VLM-based parsing is token redundancy, arising from large blank or low-information regions within the document image. ",
        "bbox": [
            137,
            750,
            862,
            887
        ],
        "page_idx": 3
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            493,
            920,
            503,
            930
        ],
        "page_idx": 3
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            137,
            58,
            862,
            74
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            137,
            107,
            859,
            138
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "In response to the aforementioned challenges, we introduce a new document parsing framework, MinerU2.5. The key innovation is a decoupled architecture that separates global layout analysis from local content recognition via an efficient coarse-to-fine, two-stage inference mechanism. In the first stage, the model conducts fast and holistic layout analysis on downsampled document images, capturing the global structural organization with minimal computational cost. In the second stage, guided by the detected layout, it crops key regions from the original high-resolution input and performs fine-grained recognition within local windows, thereby preserving native resolution and ensuring high accuracy. This decoupled strategy not only reduces computational cost by an order of magnitude, primarily by avoiding the enormous number of visual tokens with $\\mathcal { O } ( N ^ { 2 } )$ complexity inherent in end-to-end native-resolution approaches [3, 6, 37], but also brings multiple advantages: it significantly enhances the interpretability of parsing, effectively mitigates the common hallucination problem in VLMs, and allows the two stages to be independently optimized and iterated, resulting in more robust and efficient parsing capabilities. Ultimately, with its lightweight design of only 1.2B parameters, MinerU2.5 exhibits strong adaptability and efficiency in scenarios with long documents and high-density content while ensuring high parsing accuracy. Furthermore, to overcome the challenges of insufficient data diversity, sample imbalance, and inconsistent annotation quality in document parsing, we have developed a closed-loop data engine for complex documents. This engine systematically collects, processes, and generates large-scale, high-quality document corpora. This ensures that our model exhibits precise parsing capabilities and robustness across a wide spectrum of layouts, document types, and complex elements. ",
        "bbox": [
            135,
            143,
            862,
            446
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "MinerU2.5 not only achieves state-of-the-art (SOTA) performance across a wide range of public benchmarks but also represents a qualitative leap in practical application and user experience over the previous MinerU2 version, as demonstrated by the examples in Appendix A . Its key improvements include: ",
        "bbox": [
            137,
            454,
            862,
            513
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "• Comprehensive and Granular Layout Analysis: It not only preserves non-body elements like headers, footers, and page numbers to ensure full content integrity, but also employs a refined and standardized labeling schema. This enables a clearer, more structured representation of elements such as lists, references, and code blocks.   \nBreakthroughs in Formula Parsing: Delivers high-quality parsing of complex, lengthy mathematical formulae and accurately recognizes mixed-language (Chinese-English) equations.   \n• Enhanced Robustness in Table Parsing: Effortlessly handles challenging cases, including rotated tables, borderless tables, and tables with partial borders. ",
        "bbox": [
            151,
            522,
            864,
            659
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "2 Related Work ",
        "text_level": 1,
        "bbox": [
            137,
            681,
            320,
            702
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "2.1 Traditional Pipelines ",
        "text_level": 1,
        "bbox": [
            135,
            715,
            370,
            734
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Early OCR systems [8, 24, 32, 46] decompose document parsing into modular pipelines, sequentially executing layout detection [44, 59], text recognition [8], and reading order [50]. For instance, Marker [32] implements a sequential pipeline integrating Surya OCR [33] with layout analysis and reading order prediction modules to process diverse document types. MinerU [46] leverages PDF-Extract-Kit [30] to orchestrate multiple specialized models for layout detection, formula recognition and table extraction. This modular architecture enables specialized optimization of individual components and facilitates targeted refinement of specific subtasks through well-defined module boundaries. However, pipeline-based methods are prone to error propagation across stages and exhibit limited robustness when confronted with complex layouts such as multi-column text or cross-page structures. Moreover, modular systems often entail multiple interdependencies in practice, rendering usage, maintenance, and updates cumbersome and less efficient. ",
        "bbox": [
            137,
            741,
            862,
            878
        ],
        "page_idx": 4
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            140,
            58,
            862,
            73
        ],
        "page_idx": 4
    },
    {
        "type": "discarded",
        "text": "5 ",
        "bbox": [
            493,
            920,
            504,
            931
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            137,
            107,
            861,
            137
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "2.2 General-Purpose Vision Language Models ",
        "text_level": 1,
        "bbox": [
            135,
            156,
            562,
            175
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "General-purpose vision language models (VLMs) [1, 3, 7, 63] have emerged as an alternative paradigm for document understanding. Gemini2.5 Pro [7] demonstrates strong OCR capabilities among general VLMs, surpassing traditional pipeline models like MinerU [46] in text parsing and approaching specialized systems like UniMERNet [45] in formula recognition, showcasing the potential of VLMs in OCR applications. Among open-source models, Qwen2.5-VL-72B [3] achieves the best results, using native-resolution vision encoders [10] to adapt to different image sizes, demonstrating the effectiveness of arbitrary-resolution processing in OCR tasks. However, these general models exhibit inherent limitations for document-centric tasks. Proprietary models like Gemini2.5 Pro [7] are expensive and slow in processing, while open-source models require massive parameter scales for optimal performance, limiting practical deployment. Additionally, both types remain susceptible to hallucinations in densely populated text regions, affecting reliability in complex document layouts. ",
        "bbox": [
            135,
            183,
            862,
            349
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "2.3 Domain-Specific Vision Language Models ",
        "text_level": 1,
        "bbox": [
            135,
            368,
            558,
            387
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "End-to-End Approaches. Recent domain-specific models [4, 6, 15, 23, 35, 37, 52] adopt end-to-end architectures that unify document parsing within a single model, eliminating the need for cascaded processing stages. GOT [52], as an early representative of end-to-end approaches, pioneered the OCR 2.0 paradigm by establishing both model architecture and data methodology that unified recognition across diverse modalities—text, formulas, tables, and charts—within a single framework. Subsequent models like Ocean-OCR [6], olmOCR [35], and dots.ocr [37] leverage native resolution vision encoders to process documents and construct massive document corpora, further advancing the performance of end-to-end architectures. However, end-to-end designs face scalability challenges: joint optimization of layout and content often reduces accuracy on complex documents, while native-resolution processing introduces prohibitive $\\mathcal { O } ( N ^ { 2 } )$ complexity. Despite strengths in semantic modeling, these models suffer from hallucinations on long documents and severe inefficiency with high-resolution inputs, where token redundancy from blank or low-information regions becomes a major bottleneck. ",
        "bbox": [
            137,
            395,
            862,
            575
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Multi-Stage Approaches. Recently, multi-stage methods [11, 17] leveraging VLMs decouple layout analysis from content recognition, combining the efficiency of pipeline approaches with the accuracy of unified models. Dolphin [11] employs a Swin-Transformer VLM that first performs page-level layout, then conducts efficient parallel parsing of identified regions. However, Swin-Transformer’s fixed resolution severely limits crop parsing—sub-regions with extreme aspect ratios suffer from distortion when resized to predetermined dimensions, degrading recognition quality while increasing computational overhead. MonkeyOCR [17] adopts a similar multi-stage strategy but employs a native resolution vision encoder in its second stage, improving both performance and efficiency. However, MonkeyOCR requires multiple specialized models across different stages, increasing system complexity and deployment overhead. A single unified model with native resolution parsing presents a promising direction to address these limitations, which is precisely the goal that MinerU2.5 pursues. ",
        "bbox": [
            137,
            594,
            862,
            761
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "3 MinerU2.5 ",
        "text_level": 1,
        "bbox": [
            137,
            785,
            289,
            805
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "3.1 Model Architecture ",
        "text_level": 1,
        "bbox": [
            137,
            818,
            357,
            835
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Figure 2 illustrates the overall architecture of MinerU2.5, which is inspired by the classical Qwen2-VL framework [48]. The overall model architecture consists of three major components: ",
        "bbox": [
            132,
            844,
            861,
            875
        ],
        "page_idx": 5
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            140,
            58,
            862,
            73
        ],
        "page_idx": 5
    },
    {
        "type": "discarded",
        "text": "6 ",
        "bbox": [
            493,
            920,
            503,
            931
        ],
        "page_idx": 5
    },
    {
        "type": "image",
        "img_path": "images/d39071540b4f20b937d41387aed303fa05b32be2570445d0531434307ec620a5.jpg",
        "image_caption": [
            "Figure 2: The framework of MinerU2.5. In stage I, MinerU2.5 performs rapid, global layout analysis on a downsampled page. In stage II, MinerU2.5 leverages the layout results to crop key regions from the original high-resolution document, performing fine-grained content recognition (e.g., text, table, and formula recognition) within these native-resolution local regions. The detailed prompts used in the inference are illustrated in Appendix B. "
        ],
        "image_footnote": [],
        "bbox": [
            142,
            107,
            848,
            420
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Language Model. For the decoder, we employ a 0.5B-parameter Qwen2-Instruct model [42], as document parsing tasks typically exhibit relatively low dependency on large-scale language models. To better accommodate diverse resolutions and aspect ratios in cropped image parsing, we replace the original 1D-RoPE [39] with M-RoPE [48], thus enhancing the model’s generalization ability across varying resolutions. ",
        "bbox": [
            137,
            536,
            862,
            613
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Vision Encoder. Inspired by Qwen2-VL, MinerU2.5 incorporates a native-resolution encoding mechanism. Although the Qwen2.5-VL series [3] adopts window attention to improve efficiency, this design causes performance degradation in document parsing tasks. Therefore, we employ a 675M-parameter NaViT [10] initialized from Qwen2-VL. This vision encoder supports dynamic image resolutions and employs 2D-RoPE for positional encoding, enabling it to flexibly handle inputs of various resolutions and aspect ratios. ",
        "bbox": [
            137,
            631,
            862,
            723
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Patch Merger. To balance efficiency and performance, the architecture uses pixel-unshuffle [38] on adjacent $2 \\times 2$ vision tokens, preprocessing the aggregated vision tokens before passing them into the large language model. This design effectively achieves a trade-off between computational efficiency and task performance. ",
        "bbox": [
            137,
            741,
            862,
            803
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "3.2 Two-Stage Parsing Strategy ",
        "text_level": 1,
        "bbox": [
            138,
            821,
            428,
            840
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "In high-resolution document parsing with VLMs, a large proportion of low-information blank regions introduces severe token redundancy, which substantially reduces overall efficiency. Existing end-to-end visual encoding strategies for VLMs face inherent limitations: ",
        "bbox": [
            137,
            847,
            862,
            892
        ],
        "page_idx": 6
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            493,
            920,
            503,
            930
        ],
        "page_idx": 6
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            135,
            58,
            861,
            73
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "• Crop-based approaches [52, 63] can partially reduce computational overhead but inevitably sacrifice semantic consistency and layout information.   \n• Native-resolution approaches [3, 13, 37, 29] preserve fine-grained details in high-resolution inputs, yet produce an enormous number of visual tokens with $\\mathcal { O } ( N ^ { 2 } )$ complexity, rendering them computationally impractical. ",
        "bbox": [
            151,
            106,
            861,
            190
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "To address this dilemma, we propose a two-stage parsing strategy. This design decouples layout analysis from local content recognition, thereby improving interpretability, enhancing optimization potential for downstream tasks such as OCR, and effectively reducing the risk of hallucinations. Below, we provide more details of each stage. ",
        "bbox": [
            137,
            198,
            862,
            258
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Stage I: Layout Analysis. In the first stage, the input image is uniformly resized to a thumbnail of $1 0 3 6 \\times 1 0 3 6$ pixels, enabling global layout analysis while controlling computational cost. The parameter choice is determined through systematic analysis: the thumbnail size must balance global visibility and efficiency—too small leads to detail loss, while too large triggers the quadratic complexity of NaViT. In contrast to native-aspect-ratio thumbnails, adopting a fixed thumbnail size results in more stable bounding-box localization and facilitates more efficient training. ",
        "bbox": [
            137,
            276,
            862,
            368
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Stage II: Content Recognition. In the second stage, the model leverages the detected layout to crop the native high-resolution image into local regions, which are then parsed at fine granularity. Cropped regions are fed at native resolution with an upper bound of $2 0 4 8 \\times 2 8 \\times 2 8$ pixels, avoiding detail loss from overly small crops while preventing redundant computation from excessively large ones. This design ensures a robust trade-off between accuracy and efficiency across diverse document parsing scenarios. ",
        "bbox": [
            137,
            387,
            862,
            478
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "3.3 Training Recipe ",
        "text_level": 1,
        "bbox": [
            137,
            497,
            325,
            516
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "As described in Section 3.1, MinerU2.5 consists of three core components: vision encoder, patch merger, and language model. Prior to the pre-training phase of MinerU2.5, the vision encoder is initialized from Qwen2-VL-2B-Instruct, while the language model is initialized from Qwen2-Instruct-0.5B. The overall training procedure of MinerU2.5 is divided into three stages, as summarized in Table 1. ",
        "bbox": [
            137,
            523,
            864,
            584
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "3.3.1 Stage 0-Modality Alignment ",
        "text_level": 1,
        "bbox": [
            137,
            602,
            398,
            618
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "To ensure that MinerU2.5 acquires the fundamental vision–language alignment ability as well as the OCR recognition capability, we first conduct two-stage modality alignment training on Visual Question Answering (VQA) datasets. ",
        "bbox": [
            137,
            627,
            862,
            671
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Language-Image Alignment. Only the two-layer MLP within the patch merger is trained, while both the vision encoder and the language model are frozen. We use image-caption pairs1 for training to effectively project visual features into the LLM embedding space, thus achieving alignment of the modal representation. ",
        "bbox": [
            137,
            690,
            862,
            752
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Visual Instruction Tuning. All model parameters are unfrozen. The focus is on knowledge accumulation and ability expansion, particularly strengthening visual alignment and OCR capability. The training data2 mainly covers image captioning, interleaved text-image pairs, visual alignment, and OCR data. The goal is to enable MinerU2.5 to follow instructions across diverse visual tasks and generate reasonable responses. ",
        "bbox": [
            137,
            770,
            862,
            847
        ],
        "page_idx": 7
    },
    {
        "type": "discarded",
        "text": "1This dataset is sourced from LLaVA-Pretrain. 2This dataset is sourced from LLaVA-Instruct. ",
        "bbox": [
            155,
            857,
            415,
            882
        ],
        "page_idx": 7
    },
    {
        "type": "discarded",
        "text": "8 ",
        "bbox": [
            493,
            920,
            504,
            931
        ],
        "page_idx": 7
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            142,
            58,
            864,
            73
        ],
        "page_idx": 7
    },
    {
        "type": "table",
        "img_path": "images/2307dfcdf2cac88e02aca759ffbf6ad5633b1c6846729bce77db876fdcc1f89e.jpg",
        "table_caption": [
            "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
            "Table 1: Training setup and hyperparameters in three training stages. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=2 colspan=1></td><td rowspan=1 colspan=4>Stage-0</td><td rowspan=1 colspan=1>Stage-1</td><td rowspan=1 colspan=1>Stage-2</td></tr><tr><td rowspan=1 colspan=2>a</td><td rowspan=1 colspan=2>b</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td></tr><tr><td rowspan=1 colspan=1>G     Max Resolution#Tokens per Image</td><td rowspan=1 colspan=2>2048 × 28 × 284~2048</td><td rowspan=1 colspan=2>4096 × 28 × 284~ 4096</td><td rowspan=1 colspan=1>2048× 28 × 284~2048</td><td rowspan=1 colspan=1>2048 × 28 × 284~2048</td></tr><tr><td rowspan=1 colspan=1>     Dataset#Samples</td><td rowspan=1 colspan=2>Image Caption558K</td><td rowspan=1 colspan=2>VQA665K</td><td rowspan=1 colspan=1>Layout&amp;OCR6.9M</td><td rowspan=1 colspan=1>Layout&amp;OCR630K</td></tr><tr><td rowspan=1 colspan=1>TrainableG      Sequence LengthData Augmentation</td><td rowspan=1 colspan=2>MLP Adaptor4096No</td><td rowspan=1 colspan=2>All4096No</td><td rowspan=1 colspan=1>All8192Yes</td><td rowspan=1 colspan=1>All16384Yes</td></tr><tr><td rowspan=4 colspan=1>Batch SizeYriiirng     LR: φviTLR: {0MLP, ΦLm}Epoch</td><td rowspan=4 colspan=2>1281 ×10-31 ×10-31</td><td rowspan=1 colspan=2>641 ×10-5</td><td rowspan=2 colspan=1>2564 ×10-64 ×10-5</td><td rowspan=4 colspan=1>2564 ×10-64 ×10-53</td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=2>1 ×10-5</td><td rowspan=1 colspan=1></td></tr><tr><td rowspan=2 colspan=2>1</td><td></td></tr><tr><td rowspan=1 colspan=1>4 ×10-52</td></tr></table>",
        "bbox": [
            137,
            104,
            859,
            359
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Empirical results demonstrate that MinerU2.5, after VQA-based modality alignment training, exhibits significant improvements in tasks such as layout analysis and content recognition. Conversely, skipping this stage leads to higher losses and a clear drop in overall performance. ",
        "bbox": [
            137,
            398,
            862,
            444
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "3.3.2 Stage 1-Document Parsing Pre-training ",
        "text_level": 1,
        "bbox": [
            137,
            463,
            478,
            478
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "The objective of the document parsing pre-training stage is to enable MinerU2.5 to acquire two fundamental capabilities: layout analysis and content recognition. At this stage, all parameters of the model remain fully trainable. ",
        "bbox": [
            137,
            488,
            862,
            532
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Training Data. We leveraged a large-scale mixture of model-labeled data and public datasets to ensure both sufficient scale and document diversity. For layout analysis, in consideration of training efficiency, full document images were resized to a fixed resolution with corresponding relative coordinates, and the prompt “Layout Detection:” was used. For content recognition, we employed single-element image samples of text blocks, formula blocks, and table blocks as inputs, with prompts “Text Recognition:”, “Formula Recognition:”, and “Table Recognition:” respectively. More details are shown in the Appendix B. ",
        "bbox": [
            137,
            550,
            862,
            657
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Training Configuration. The model, initialized from Stage 0, was trained for 2 epochs. Each epoch consisted of a total of 6.9M samples, including 2.3M for layout analysis, 2.4M for text blocks, 1.1M for formula blocks, and 1.1M for table blocks. ",
        "bbox": [
            137,
            676,
            862,
            722
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Through this document parsing pre-training, the model has acquired strong layout analysis and content recognition capabilities, demonstrating excellent performance across most simple and medium-level scenarios. The resulting model not only serves as a strong baseline for downstream fine-tuning, but also functions as an efficient hard-sample miner within our data engineering pipeline, facilitating the identification of challenging cases for human annotation and further improving document parsing performance. ",
        "bbox": [
            137,
            728,
            862,
            820
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "3.3.3 Stage 2-Document Parsing Fine-tuning ",
        "text_level": 1,
        "bbox": [
            137,
            838,
            477,
            854
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "The objective of the document parsing fine-tuning stage is to further enhance parsing performance in challenging scenarios, while maintaining the detection and parsing capabilities already acquired by ",
        "bbox": [
            133,
            863,
            861,
            893
        ],
        "page_idx": 8
    },
    {
        "type": "discarded",
        "text": "9 ",
        "bbox": [
            493,
            920,
            504,
            930
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "MinerU2.5. ",
        "text_level": 1,
        "bbox": [
            137,
            107,
            218,
            122
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Training Data. To achieve this goal, it is crucial to construct a compact yet high-quality dataset: ",
        "bbox": [
            140,
            141,
            841,
            157
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "• To preserve the model’s fundamental capabilities, we sampled high-quality and diverse examples from the pre-training dataset via data engineering and incorporated them into Stage 2 training, ensuring broad coverage across different document element types.   \n• From a large-scale, multi-source PDF corpus, we employed data engineering to identify cases where the model still underperformed. We summarized these difficult scenarios and conducted targeted data collection with manual annotation to obtain high-quality samples representing challenging cases. ",
        "bbox": [
            153,
            162,
            864,
            279
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Training Configuration. We fine-tuned the pre-trained model for 3 epochs. Each epoch contained a total of 630K samples, consisting of 43K for layout analysis, 300K for text blocks, 147K for formula blocks, and 140K for table blocks. ",
        "bbox": [
            140,
            296,
            862,
            342
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "With this targeted data iteration strategy, Stage 2 fine-tuning enables the model to not only retain its established document parsing abilities but also achieve significant improvements in previously challenging scenarios. ",
        "bbox": [
            137,
            349,
            862,
            395
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "3.3.4 Data Augmentation Strategies ",
        "text_level": 1,
        "bbox": [
            137,
            414,
            411,
            429
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "To enhance the model’s robustness in handling diverse documents in an open-world setting, we designed a variety of targeted data augmentation strategies during both Stage 1 and Stage 2. These augmentations simulate common types of document interference, and can be categorized as shown in Table 2. ",
        "bbox": [
            137,
            438,
            861,
            497
        ],
        "page_idx": 9
    },
    {
        "type": "table",
        "img_path": "images/84b388fc8ec6ded4ebb71f6d2cfd5ee2845b11e669459d06b7fe2869aff0d17a.jpg",
        "table_caption": [
            "Table 2: Data augmentation strategies for document parsing. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Augmentation Type</td><td>Operations</td></tr><tr><td>Spatial Transformations Background Transformations</td><td>Scaling, Grid Distortion,Rotation Texture, Weather effect, Image background, Watermark, Scanlines, Shadow</td></tr><tr><td>Color Transformations Degradation Transformations</td><td>Brightness Contrast, Illumination,RGB Shift PSF Blur, Vibration Blur, Gaussian Blur, Erosion/Dilation</td></tr></table>",
        "bbox": [
            210,
            508,
            787,
            631
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Note that spatial transformations are not applied to layout analysis samples. For different element types, we carefully design augmentation parameters and probabilities in order to strike a balance between model performance and robustness. ",
        "bbox": [
            137,
            675,
            862,
            722
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "3.4 Model Deployment ",
        "text_level": 1,
        "bbox": [
            137,
            739,
            359,
            758
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "We implement an efficient offline inference pipeline for MinerU2.5 based on vLLM [16]. While vLLM provides high-throughput serving for large language models, we introduce two additional optimizations tailored for our two-stage document parsing pipeline to further minimize end-to-end latency. First, we employ an asynchronous backend to handle batching submission of page-level requests, enabling better overlap between CPU and GPU workloads. Second, we decouple Stage I and Stage II into independent inference tasks, allowing downstream processing to begin as soon as individual results become available, rather than waiting for entire batches. ",
        "bbox": [
            137,
            766,
            862,
            873
        ],
        "page_idx": 9
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 9
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            140,
            58,
            864,
            73
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "A key challenge during deployment was suppressing degenerate token repetition without penalizing legitimate repetitive structures (e.g., tables, equations, or structured content). To address this, we dynamically adjust sampling parameters like frequency penalty and presence penalty in Stage II based on the layout type detected in Stage I. For instance, higher penalties are applied to text paragraphs, while lower values are used for tabular content. ",
        "bbox": [
            137,
            107,
            862,
            183
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Furthermore, we carefully tuned key vLLM scheduling parameters, including max num batched tokens, max num seqs, and cuda graph sizes, to improve batch utilization and kernel launch efficiency. ",
        "bbox": [
            137,
            190,
            864,
            220
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "We evaluate all compared models on OmniDocBench [31], a dataset of 1,355 document pages with an average of over 1,100 tokens per page. All models are tested using their official inference scripts under a consistent batched parallel processing protocol, with vLLM startup overhead excluded for fair comparison. After preliminary optimization, MinerU2.5 achieves an end-to-end throughput of 2.12 pages/s. The end-to-end generation speed, measured only on valid output tokens from Stage II, reaches 2337.25 tokens $/ \\mathsf { \\pmb { s } } ^ { 3 }$ . As shown in Table 3, MinerU2.5 outperforms MonkeyOCR-Pro-3B by $4 \\times$ and dots.ocr by $7 \\times$ in page throughput, demonstrating strong inherent efficiency for large-scale document parsing. Notably, even without any deployment optimizations, MinerU2.5 achieves a baseline throughput of 0.95 pages/s and 1045.14 tokens/s, already surpassing other compared models under default configurations. ",
        "bbox": [
            135,
            227,
            862,
            378
        ],
        "page_idx": 10
    },
    {
        "type": "table",
        "img_path": "images/95e563727f1fe9d55ef64dc7e001d944a72969ca152e953a4e0b141eca5cf399.jpg",
        "table_caption": [],
        "table_footnote": [
            "Table 3: Inference performance comparison of specialized VLMs and MinerU2.5 across different backends and GPUs. "
        ],
        "table_body": "<table><tr><td>Model</td><td>Parameters</td><td>Backend</td><td>Hardware</td><td>Tokens/sec</td><td>Pages/sec</td></tr><tr><td>MinerU2-VLM [46]</td><td>0.9B</td><td>SGLang [60]</td><td rowspan=\"6\"></td><td>3091.23</td><td>2.84</td></tr><tr><td>dots.ocr [37]</td><td>3.0B</td><td rowspan=\"3\">A100 80G vLLM [16]</td><td>311.06</td><td>0.28</td></tr><tr><td>MonkeyOCR-pro-3B [17]</td><td>3.7B</td><td>520.16</td><td>0.47</td></tr><tr><td>MonkeyOCR-pro-1.2B [17]</td><td>1.9B</td><td>589.76</td><td>0.53</td></tr><tr><td>Nanonets-OCR-s [26]</td><td>3.7B</td><td>605.92</td><td>0.55</td></tr><tr><td rowspan=\"3\">MinerU2.5</td><td rowspan=\"3\">1.2B</td><td rowspan=\"3\">vLLM</td><td>RTX 4090 48G</td><td>1875.82</td><td>1.70</td></tr><tr><td>A100 80G</td><td>2337.25</td><td>2.12</td></tr><tr><td>H200 141G</td><td>4938.31</td><td>4.47</td></tr></table>",
        "bbox": [
            138,
            390,
            861,
            554
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "4 Data Engine ",
        "text_level": 1,
        "bbox": [
            137,
            625,
            305,
            645
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "The state-of-the-art performance of MinerU2.5 is underpinned by a systematic Data Engine designed to generate large-scale, high-quality training data with uniform annotation standards. This engine first establishes a vast and diverse foundation through rigorous data curation and refined automated annotation for pre-training. Building upon this foundation, we introduce our novel Iterative Mining via Inference Consistency (IMIC) strategy, which efficiently identifies complex “hard cases” for targeted human annotation. This multi-stage approach creates a virtuous cycle of improvement, progressively enhancing the model’s capabilities. The entire process is illustrated in Figure 3. ",
        "bbox": [
            137,
            656,
            862,
            763
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "4.1 Overall Workflow ",
        "text_level": 1,
        "bbox": [
            137,
            781,
            343,
            799
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "4.1.1 Data Curation ",
        "text_level": 1,
        "bbox": [
            137,
            808,
            294,
            821
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Our process begins with a large-scale internal document pool comprising publicly available web data and commercially procured documents. While diverse, this raw pool suffers from a significant long-tail distribution. To mitigate this imbalance and enhance training robustness, we implement a rigorous curation process to build a balanced Chinese-English dataset with high diversity across multiple dimensions: ",
        "bbox": [
            137,
            832,
            859,
            862
        ],
        "page_idx": 10
    },
    {
        "type": "discarded",
        "text": "3The end-to-end generation speed is calculated based on the number of valid tokens produced by Stage II divided by the total processing time for both stages. ",
        "bbox": [
            137,
            869,
            859,
            896
        ],
        "page_idx": 10
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            140,
            58,
            862,
            73
        ],
        "page_idx": 10
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            919,
            506,
            931
        ],
        "page_idx": 10
    },
    {
        "type": "image",
        "img_path": "images/77f3e58ac63b5f6c4cc69010648eb0b4cc9837320e38be3e4ff04d9e23b1ae2a.jpg",
        "image_caption": [
            "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
            "Figure 3: Overview of the Data Engine. Our data pipeline consists of three core stages. (1) Data Curation: We filter a massive, raw document pool to construct a diverse and balanced dataset based on layout, document type, element balance, and language. (2) Pre-training Data Preparation: We generate automated annotations for the curated data and then refine them using specialized, powerful models for text, tables, and formulas to ensure high quality. (3) Fine-tuning Dataset Construction: We employ our Iterative Mining via Inference Consistency (IMIC) strategy to automatically discover hard cases, which then undergo meticulous expert curation to create a high-quality SFT dataset. "
        ],
        "image_footnote": [],
        "bbox": [
            145,
            106,
            848,
            378
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            137,
            526,
            862,
            571
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "• Layout Diversity: We employ page-level image clustering to select exemplars from a wide spectrum of visual layouts and styles.   \n• Document Type Diversity: Using document metadata (e.g., discipline, tags), we perform stratified sampling to ensure a balanced representation of types such as academic papers, textbooks, reports, and presentations.   \n• Element Balance: A preliminary detection model helps ensure a balanced class distribution of key elements like titles, paragraphs, tables, formulas, and figures in the curated set.   \n• Language Balance: We filter the data to maintain a comparable volume of Chinese and English documents. ",
        "bbox": [
            160,
            580,
            864,
            738
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "4.1.2 Pre-training Dataset Preparation ",
        "text_level": 1,
        "bbox": [
            137,
            757,
            428,
            773
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Initial annotations for the curated dataset are generated using our MinerU2-pipeline, establishing a baseline for subsequent refinement. To move beyond this baseline quality, we perform a multi-step refinement process using specialized, expert models for different content types: ",
        "bbox": [
            138,
            781,
            862,
            828
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "• Textual Content: We leverage the powerful Qwen2.5-VL-72B-Instruct to verify and correct initial text recognition results on cropped text regions. • Formula Content: Recognized formulas are substituted with higher-fidelity outputs from an ",
        "bbox": [
            155,
            834,
            862,
            888
        ],
        "page_idx": 11
    },
    {
        "type": "discarded",
        "text": "12 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "in-house UniMERNet model, which we retrained on our extensive formula dataset to boost its accuracy. ",
        "bbox": [
            165,
            107,
            861,
            137
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "• Table Content: All table structures are re-generated using an in-house, high-performance table parsing model. ",
        "bbox": [
            155,
            145,
            861,
            175
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "This refinement workflow yields a high-quality pre-training dataset of image-annotation pairs, covering our four core tasks: layout analysis, text recognition, formula recognition, and table recognition. ",
        "bbox": [
            137,
            183,
            862,
            213
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "4.1.3 Fine-tuning Dataset Construction ",
        "text_level": 1,
        "bbox": [
            138,
            232,
            436,
            247
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "While pre-training ensures broad capabilities, the noise inherent in automated annotations creates a ceiling for model performance. To break through this ceiling, our fine-tuning strategy pivots to high-value, difficult examples. We designed an Iterative Mining via Inference Consistency (IMIC) strategy to automatically filter these hard cases from the large-scale data pool. To ensure annotation quality, these select samples are processed through an AI-assisted pipeline: they are first pre-annotated by a foundation model, such as Gemini-2.5-Pro for complex tables, and then meticulously reviewed and corrected by human experts4. The final Supervised Fine-Tuning (SFT) dataset combines these highquality hard cases with a smaller, randomly sampled set of regular examples, equipping MinerU2.5 to excel in complex, real-world parsing scenarios. ",
        "bbox": [
            135,
            256,
            862,
            392
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "4.2 Task Reformulation and Enhancement ",
        "text_level": 1,
        "bbox": [
            135,
            411,
            526,
            429
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "To move beyond the limitations of existing document analysis methods, we systematically reformulated the core tasks of layout analysis, formula recognition, and table recognition. This involved defining more robust standards, designing novel task paradigms, and introducing specialized metrics and representations. ",
        "bbox": [
            137,
            438,
            862,
            497
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "4.2.1 Layout Analysis ",
        "text_level": 1,
        "bbox": [
            137,
            517,
            308,
            532
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "A Unified Tagging System. A fundamental challenge in layout analysis is the lack of a standardized tagging system. Existing datasets suffer from widespread inconsistencies in element definitions, granularity, and scope. To address this, we engineered a hierarchical and comprehensive tagging system by analyzing a vast corpus of documents. Our system is defined by three key principles: ",
        "bbox": [
            138,
            540,
            862,
            602
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "• Comprehensive Coverage: It includes non-body content often ignored by others, such as headers, footers, and page numbers, which is critical for downstream applications like RAG. Fine Granularity: It decomposes complex elements. For instance, figures are sub-categorized into image, chart, and chemical structure, with distinct tags for their associated captions. • Semantic Distinction: Visually distinct text blocks like code, algorithms, references, and lists are assigned their own categories to preserve crucial semantic information. ",
        "bbox": [
            151,
            608,
            862,
            715
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Table 4 presents a comparison with mainstream tagging systems, highlighting the superior coverage and granularity of our proposed system. ",
        "bbox": [
            135,
            722,
            859,
            753
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "An Enhanced Multi-Task Paradigm. Traditional methods often treat layout analysis as a standard object detection task, which ignores element rotation and defers reading order prediction to downstream modules. This approach not only impairs the recognition of rotated elements but also increases system coupling. We propose an enhanced paradigm that redefines layout analysis as a multi-task problem. This paradigm simultaneously predicts four key attributes for each document element in a single inference pass: its Position, Class, Rotation Angle, and Reading Order. This integrated design effectively resolves the challenge of parsing rotated elements and streamlines the entire document analysis pipeline. ",
        "bbox": [
            137,
            760,
            862,
            852
        ],
        "page_idx": 12
    },
    {
        "type": "discarded",
        "text": "4Human review is augmented by our open-source QA tool, Dingo, which applies both rule-based and model-based checks. See https://github.com/MigoXLab/dingo. ",
        "bbox": [
            137,
            861,
            864,
            886
        ],
        "page_idx": 12
    },
    {
        "type": "discarded",
        "text": "13 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 12
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            140,
            58,
            864,
            73
        ],
        "page_idx": 12
    },
    {
        "type": "table",
        "img_path": "images/d471898cfc71c3517b743d99db3fcc098e5ab532c1c30dbd6891c3779aabba86.jpg",
        "table_caption": [
            "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
            "Table 4: Comparison of category support across different OCR systems. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Category</td><td>MinerU2-pipeline</td><td>PaddleOCR</td><td>MinerU2.5</td></tr><tr><td rowspan=\"15\">Textual</td><td>text</td><td>text, toc,abstract</td><td>text</td></tr><tr><td>title</td><td>title,page_title</td><td>title</td></tr><tr><td>×</td><td>×</td><td>phonetic</td></tr><tr><td>image_caption</td><td>common_caption</td><td>image_caption</td></tr><tr><td>image_footnote</td><td>common_footnote</td><td>image_footnote</td></tr><tr><td>table_caption</td><td>common_caption</td><td>table_caption</td></tr><tr><td>table_footnote</td><td>common_footnote</td><td>table_footnote</td></tr><tr><td>×</td><td>code</td><td>code</td></tr><tr><td>×</td><td>×</td><td>code_caption</td></tr><tr><td>×</td><td>×</td><td>algorithm</td></tr><tr><td>×</td><td>ref_text, ref-block</td><td>reference</td></tr><tr><td>×</td><td>×</td><td>list</td></tr><tr><td>Image image</td><td></td><td>image,seal,chart, molecular</td><td>image</td></tr><tr><td>Table table</td><td></td><td>table</td><td>table</td></tr><tr><td rowspan=\"2\">Equation</td><td>equation</td><td>equation</td><td>equation</td></tr><tr><td>×</td><td>×</td><td>equation_block</td></tr><tr><td rowspan=\"5\">Page Margins</td><td>×</td><td>header</td><td>header</td></tr><tr><td>×</td><td>footer</td><td>footer</td></tr><tr><td>×</td><td>aside_text</td><td>aside_text</td></tr><tr><td>×</td><td>page_number</td><td>page_number</td></tr><tr><td>×</td><td>page_footnote</td><td>page_footnote</td></tr></table>",
        "bbox": [
            254,
            104,
            743,
            473
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            135,
            523,
            859,
            554
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "PageIoU: A New Metric for Layout Quality. Layout analysis is typically evaluated with object detection metrics like mAP, which rely on a fixed Intersection over Union (IoU) threshold. While effective for well-defined objects, this approach is ill-suited for document layouts where text block boundaries are often ambiguous. This can lead to a discrepancy where quantitative IoU-based scores do not align with qualitative visual assessment. ",
        "bbox": [
            137,
            560,
            862,
            637
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "As illustrated in Figure 4, a prediction that coarsely covers a paragraph (Case 1) can achieve a perfect recall score $( { \\mathrm { R e c a l l } } @ { \\mathrm { I o U } } 0 . 5 = 1 . 0 )$ ), while a more accurate line-by-line prediction (Case 2) is penalized for not matching the paragraph-level ground truth, yielding a lower score $( \\mathrm { R e c a l l } @ \\mathrm { I o U } 0 . 5 = 0 . 6 )$ . Visually, however, Case 2 is clearly a better fit. ",
        "bbox": [
            137,
            643,
            862,
            705
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "To better evaluate document layout analysis, we introduce PageIoU, a page-level coverage metric that measures the spatial consistency between predicted layouts and ground-truth annotations. Let the predicted layout be ",
        "bbox": [
            137,
            712,
            862,
            757
        ],
        "page_idx": 13
    },
    {
        "type": "equation",
        "img_path": "images/0b27058cbc676a0fd557786006d9cb0e00e64275cbcea9ce1b5e1170b07df9da.jpg",
        "text": "$$\nP = \\{ b b o x _ { i } \\mid i = 1 , 2 , \\ldots , n \\} ,\n$$",
        "text_format": "latex",
        "bbox": [
            395,
            756,
            602,
            773
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "and the ground truth be ",
        "bbox": [
            137,
            779,
            312,
            794
        ],
        "page_idx": 13
    },
    {
        "type": "equation",
        "img_path": "images/7c65ea8a1025864706e3c5b6aa3ad970addc28352b882ff47e42f652f746e666.jpg",
        "text": "$$\nG = \\{ b b o x _ { j } \\mid j = 1 , 2 , \\ldots , m \\} ,\n$$",
        "text_format": "latex",
        "bbox": [
            392,
            792,
            606,
            813
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "where each bbox denotes a bounding box on the page. We first compute coverage maps for both prediction and ground truth. For example, the ground-truth coverage map is defined as: ",
        "bbox": [
            135,
            815,
            864,
            847
        ],
        "page_idx": 13
    },
    {
        "type": "equation",
        "img_path": "images/e8685b2c37ab45393206a29432ff6384ee47cfd66b3f35cd3d6804cef4692609.jpg",
        "text": "$$\nG _ { c o v e r } = \\left\\{ \\sum _ { j = 1 } ^ { m } 1 _ { p \\in b b o x _ { j } } \\bigg | p \\in M \\right\\} ,\n$$",
        "text_format": "latex",
        "bbox": [
            380,
            854,
            616,
            898
        ],
        "page_idx": 13
    },
    {
        "type": "discarded",
        "text": "14 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 13
    },
    {
        "type": "image",
        "img_path": "images/327cf515e1b5f4b613c5b91d8e40101fcca3948b1d3b912833e582143b924ffb.jpg",
        "image_caption": [
            "Figure 4: Illustration of the proposed PageIoU metric. Case 1 and Case 2 show that IoU-based recall may produce contradictory results compared with visual inspection, whereas PageIoU provides a page-level coverage score that aligns more closely with qualitative observations. "
        ],
        "image_footnote": [],
        "bbox": [
            137,
            97,
            854,
            521
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "where $p$ is a page pixel and $M$ denotes the non-background region of the page. Similarly, $P _ { c o v e r }$ can be obtained. Based on these, PageIoU is defined as: ",
        "bbox": [
            132,
            601,
            861,
            631
        ],
        "page_idx": 14
    },
    {
        "type": "equation",
        "img_path": "images/b75ead529d168681f8181d459117b2cc53875cfdae46ad9d2fec1085bf365b00.jpg",
        "text": "$$\n\\mathrm { P a g e I o U } ( P , G ) = \\frac { \\left| { \\cal P } _ { c o v e r } \\cap { \\cal G } _ { c o v e r } \\right| } { \\left| { \\cal P } _ { c o v e r } \\cup { \\cal G } _ { c o v e r } \\right| } = \\frac { \\sum _ { p \\in M } \\operatorname* { m i n } \\left\\{ { \\cal P } _ { c o v e r } ( p ) , { \\cal G } _ { c o v e r } ( p ) \\right\\} } { \\sum _ { p \\in M } \\operatorname* { m a x } \\left\\{ { \\cal P } _ { c o v e r } ( p ) , { \\cal G } _ { c o v e r } ( p ) \\right\\} } .\n$$",
        "text_format": "latex",
        "bbox": [
            246,
            638,
            750,
            679
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Here, $| \\cdot |$ denotes the summation over all pixel values, while $\\cap$ and $\\cup$ correspond to the pixel-wise minimum and maximum of coverage counts, respectively. As shown in Figure 4, PageIoU aligns with human perception, scoring the qualitatively poor prediction 0.78 and the superior one 0.97. ",
        "bbox": [
            135,
            694,
            862,
            739
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "4.2.2 Formula Recognition ",
        "text_level": 1,
        "bbox": [
            137,
            757,
            346,
            773
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Decoupling Atomic and Compound Formulas. Existing models struggle with long or multi-line formulas, and VLMs are prone to severe structural hallucinations. We identify the root cause as the tendency to treat all formulas as monolithic entities, failing to account for internal complexity. To this end, MinerU2.5 introduces a ”whole-part” decoupling philosophy, classifying formulas into two types based on their structural and semantic integrity: ",
        "bbox": [
            137,
            781,
            862,
            858
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "• Atomic Formulas: The smallest, indivisible semantic units with a tight 2D topology (e.g., a single fraction, a matrix). ",
        "bbox": [
            153,
            864,
            862,
            896
        ],
        "page_idx": 14
    },
    {
        "type": "discarded",
        "text": "15 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 14
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            137,
            58,
            862,
            74
        ],
        "page_idx": 14
    },
    {
        "type": "image",
        "img_path": "images/08d0730c13ef08265343bb2610f16a0c325fd1bcba3eaaba7638b3a9e26e8de8.jpg",
        "image_caption": [
            "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
            "Figure 5: The proposed ADR framework. First, a compound formula is decomposed into atomic lines via layout analysis. Next, each line is individually recognized into LaTeX. Finally, the individual results are structurally recombined to produce the complete output. "
        ],
        "image_footnote": [],
        "bbox": [
            138,
            107,
            849,
            459
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "• Compound Formulas: An ordered set of atomic formulas composed vertically with specific alignment relationships (e.g., a multi-line derivation aligned at the equal signs). ",
        "bbox": [
            155,
            545,
            861,
            577
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "The Atomic Decomposition & Recombination (ADR) Framework. To handle the complexity of compound formulas, we propose the ADR framework, which implements a multi-stage ”divide and conquer” strategy. As illustrated in Figure 5, the ADR pipeline is powered by our versatile MinerU2.5 model, which acts as both a layout analyzer and a recognition engine, guided by task-specific prompts. The process begins with an initial layout analysis pass, where MinerU2.5, guided by a layout detection prompt, identifies and classifies all formula regions on the page as either atomic or compound. Next, in the decomposition stage, each identified compound formula is segmented into an ordered sequence of its constituent atomic formula lines, which are then cropped as individual images. In the third stage, these simple, semantically independent atomic formula images are fed back into the MinerU2.5 model. This time, using a formula recognition prompt, the model performs high-precision translation of each image into its corresponding LaTeX string. Finally, a lightweight recombination step uses the positional information from the initial layout pass to structurally reassemble the individual LaTeX strings into a single, coherent block, correctly formatting them within environments like align. This approach transforms a single, difficult recognition task into a series of simpler ones, ensuring both high-fidelity recognition of each component and the logical integrity of the overall structure. ",
        "bbox": [
            135,
            583,
            862,
            810
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "4.2.3 Table Recognition ",
        "text_level": 1,
        "bbox": [
            137,
            829,
            323,
            844
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Overcoming Long-Sequence Dependencies. A primary challenge in table recognition is parsing complex, long tables, especially for VLM-based approaches that target HTML. We attribute this difficulty to two inherent weaknesses of the HTML representation: (1) its complex, non-visual syntax must be learned implicitly by the model; and (2) its high token redundancy results in excessively long sequences, degrading performance on large tables. (The issue of rotated tables is effectively handled by our enhanced layout paradigm.) ",
        "bbox": [
            135,
            853,
            859,
            883
        ],
        "page_idx": 15
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 15
    },
    {
        "type": "image",
        "img_path": "images/abb0b0b473f9a8517a622d3a6cf264f9f69208cabc256747492fe5e059da718c.jpg",
        "image_caption": [
            "Figure 6: The Table Recognition Pipeline. The pipeline first detects a table and its rotation, then corrects its geometry. Next, the rectified image is recognized into the OTSL result, which is finally converted to standard HTML. "
        ],
        "image_footnote": [],
        "bbox": [
            145,
            106,
            848,
            284
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            137,
            371,
            862,
            433
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "OTSL: An Optimized Table Structure Language. To robustly handle complex tables, we propose a four-stage recognition pipeline, as depicted in Figure 6. The first two stages handle geometric normalization: the system detects the table’s bounding box and rotation angle, then corrects the image by cropping and rotating it to a canonical orientation. For the crucial third stage, table recognition, we leverage the Optimized Table-Structure Language (OTSL) [25], an intermediate representation developed by IBM [citation, 2023]. We adopted OTSL for its significant advantages over HTML as a target for VLMs. Its minimalist design features a direct structural correspondence to a table’s visual 2D matrix, reducing the number of structural tokens from over 28 to just 5 and shortening the average sequence length by approximately $5 0 \\%$ . This makes it a far more effective target for model generation. The final stage is a straightforward conversion from the OTSL output into standard HTML. ",
        "bbox": [
            137,
            439,
            862,
            590
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "4.3 Iterative Mining via Inference Consistency ",
        "text_level": 1,
        "bbox": [
            137,
            609,
            565,
            627
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "To enable continuous model improvement and the efficient expansion of our high-quality training dataset, we introduce the IMIC (Iterative Mining via Inference Consistency) strategy. IMIC automatically identifies the most challenging samples—or ”hard cases”—for the current model from a large corpus of unlabeled data. This allows us to direct limited human annotation efforts toward the data that offers the maximum value for model improvement. ",
        "bbox": [
            137,
            636,
            862,
            712
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "The core principle of IMIC leverages the stochasticity inherent in model inference. For a given sample, if the model has learned its features robustly, multiple inference passes with stochastic sampling enabled should yield highly consistent outputs. Conversely, significant divergence across outputs suggests the sample lies near the model’s decision boundary—a ’hard case’ where its predictions are uncertain. Such samples are the most valuable candidates for manual annotation, as they directly target the model’s specific weaknesses. ",
        "bbox": [
            137,
            719,
            862,
            810
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "As illustrated in Figure 7, the implementation is tailored to each recognition task: ",
        "bbox": [
            142,
            818,
            722,
            833
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "• Layout analysis: For full document pages, we perform multiple inference runs and measure consistency by calculating the pairwise PageIoU between the resulting layouts. Samples falling below a predefined similarity threshold are flagged as hard cases for precise manual annotation. ",
        "bbox": [
            158,
            839,
            862,
            886
        ],
        "page_idx": 16
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            506,
            930
        ],
        "page_idx": 16
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            135,
            58,
            862,
            74
        ],
        "page_idx": 16
    },
    {
        "type": "image",
        "img_path": "images/286df5c78476b026e4c2bb25c57870ac19a4b70e4941ce1025d5444330dea9b3.jpg",
        "image_caption": [
            "Figure 7: llustration of the proposed IMIC (Iterative Mining via Inference Consistency) strategy. From top to bottom: (a) Layout analysis, (b) Table recognition, and (c) Formula recognition. For each task, the model performs multiple stochastic inference runs, and the pairwise consistency between outputs is calculated with task-specific metrics (PageIoU, TEDS, CDM). Samples with low consistency are automatically identified as hard cases and prioritized for manual annotation. "
        ],
        "image_footnote": [],
        "bbox": [
            145,
            102,
            857,
            641
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "• Formula Recognition: For cropped formula images, consistency is assessed using the pairwise CDM [47] across multiple outputs. Samples with low consistency are prioritized for manual correction. ",
        "bbox": [
            160,
            760,
            864,
            805
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "• Table Recognition: For cropped table images, we use the TEDS (Tree-Edit-Distance-based Similarity) score to evaluate consistency across multiple recognized structures. Low-consistency samples are routed to the manual annotation workflow. ",
        "bbox": [
            161,
            813,
            862,
            858
        ],
        "page_idx": 17
    },
    {
        "type": "discarded",
        "text": "18 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 17
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            137,
            58,
            862,
            73
        ],
        "page_idx": 17
    },
    {
        "type": "table",
        "img_path": "images/c05582e7a5c0d182f32638acca62e0d1c768851e1fc4b4103ee0246775bea962.jpg",
        "table_caption": [
            "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model Type</td><td>Methods</td><td>Parameters</td><td>Overall个</td><td>TextEdit</td><td>FormulaCDM个</td><td>TableTEDS个</td><td>TableTEDS-S个</td><td></td></tr><tr><td></td><td>Marker-1.8.2 [32]</td><td></td><td>71.30</td><td>0.206</td><td>76.66</td><td>57.88</td><td>71.17</td><td>0.250</td></tr><tr><td>Pipeline Tools</td><td>MinerU2-pipeline [46]</td><td></td><td>75.51</td><td>0.209</td><td>76.55</td><td>70.90</td><td>79.11</td><td>0.225</td></tr><tr><td></td><td>PP-StructureV3 [8]</td><td></td><td>86.73</td><td>0.073</td><td>85.79</td><td>81.68</td><td>89.48</td><td>0.073</td></tr><tr><td rowspan=\"5\">General VLMs</td><td>GPT-4o [1]</td><td>-</td><td>75.02</td><td>0.217</td><td>79.70</td><td>67.07</td><td>76.09</td><td>0.148</td></tr><tr><td>InternVL3-76B [63]</td><td>76B</td><td>80.33</td><td>0.131</td><td>83.42</td><td>70.64</td><td>77.74</td><td>0.113</td></tr><tr><td>InternVL3.5-241B [49]</td><td>241B</td><td>82.67</td><td>0.142</td><td>87.23</td><td>75.00</td><td>81.28</td><td>0.125</td></tr><tr><td>Qwen2.5-VL-72B [3]</td><td>72B</td><td>87.02</td><td>0.094</td><td>88.27</td><td>82.15</td><td>86.22</td><td>0.102</td></tr><tr><td>Gemini-2.5 Pro [7]</td><td>-</td><td>88.03</td><td>0.075</td><td>85.82</td><td>85.71</td><td>90.29</td><td>0.097</td></tr><tr><td rowspan=\"11\">Specialized VLMs</td><td>Dolphin [11]</td><td>322M</td><td>74.67</td><td>0.125</td><td>67.85</td><td>68.70</td><td>77.77</td><td>0.124</td></tr><tr><td>OCRFlux [5]</td><td>3B</td><td>74.82</td><td>0.193</td><td>68.03</td><td>75.75</td><td>80.23</td><td>0.202</td></tr><tr><td>Mistral-OCR [41]</td><td></td><td>78.83</td><td>0.164</td><td>82.84</td><td>70.03</td><td>78.04</td><td></td></tr><tr><td>POINTS-Reader [22]</td><td>3B</td><td>80.98</td><td>0.134</td><td>79.20</td><td>77.13</td><td>81.66</td><td>0.144</td></tr><tr><td>olmOCR-7B [35]</td><td>7B</td><td>81.79</td><td>0.096</td><td>86.04</td><td>68.92</td><td>74.77</td><td>0.145 0.121</td></tr><tr><td>MinerU2-VLM[46]</td><td>0.9B</td><td>85.56</td><td>0.078</td><td>80.95</td><td>83.54</td><td>87.66</td><td>0.086</td></tr><tr><td>Nanonets-OCR-s [26]</td><td>3.7B</td><td>85.59</td><td>0.093</td><td>85.90</td><td>80.14</td><td>85.57</td><td>0.108</td></tr><tr><td>MonkeyOCR-pro-1.2B [17]</td><td>1.9B</td><td>86.96</td><td>0.084</td><td>85.02</td><td>84.24</td><td>89.02</td><td>0.130</td></tr><tr><td>MonkeyOCR-3B [17]</td><td>3.7B</td><td>87.13</td><td>0.075</td><td>87.45</td><td>81.39</td><td>85.92</td><td>0.129</td></tr><tr><td>dots.ocr [37]</td><td>3B</td><td>88.41</td><td>0.048</td><td>83.22</td><td>86.78</td><td>90.62</td><td>0.053</td></tr><tr><td>MonkeyOCR-pro-3B [17]</td><td>3.7B</td><td>88.85</td><td>0.075</td><td>87.25</td><td>86.78</td><td>90.63</td><td>0.128</td></tr><tr><td></td><td>MinerU2.5</td><td>1.2B</td><td>90.67</td><td>0.047</td><td>88.46</td><td>88.22</td><td>92.38</td><td>0.044</td></tr></table>",
        "bbox": [
            138,
            104,
            862,
            337
        ],
        "page_idx": 18
    },
    {
        "type": "table",
        "img_path": "images/5af5fdb53360b0988c03c347a655a5bb63a6e03c4185aaa13df46952dd1d1f47.jpg",
        "table_caption": [
            "Table 5: Performance comparison of document parsing methods on OmniDocBench across text, formula, table, and reading order extraction tasks. "
        ],
        "table_footnote": [
            "Table 6: Document Parsing Performance in Text Edit Distance on OmniDocBench: evaluation using edit distance across 9 PDF page types. "
        ],
        "table_body": "<table><tr><td rowspan=\"2\">Model Type</td><td rowspan=\"2\">Models</td><td rowspan=\"2\">Slides</td><td rowspan=\"2\">Academic Papers</td><td rowspan=\"2\">Book</td><td rowspan=\"2\">Textbook</td><td rowspan=\"2\">Exam Papers</td><td rowspan=\"2\">Magazine</td><td rowspan=\"2\">Newspaper</td><td rowspan=\"2\">Notes</td><td rowspan=\"2\">Financial Report</td></tr><tr><td></td></tr><tr><td></td><td>Marker-1.8.2 [32]</td><td>0.1796</td><td>0.0412</td><td>0.1010</td><td>0.2908</td><td>0.2958</td><td>0.1111</td><td>0.2717</td><td>0.4656</td><td>0.0341</td></tr><tr><td>Pipeline</td><td>MinerU2-pipeline [46]</td><td>0.4244</td><td>0.0230</td><td>0.2628</td><td>0.1224</td><td>0.0822</td><td>0.3950</td><td>0.0736</td><td>0.2603</td><td>0.0411</td></tr><tr><td>Tools</td><td>PP-StructureV3 [8]</td><td>0.0794</td><td>0.0236</td><td>0.0415</td><td>0.1107</td><td>0.0945</td><td>0.0722</td><td>0.0617</td><td>0.1236</td><td>0.0181</td></tr><tr><td rowspan=\"6\">General VLMs</td><td>GPT-4o [1]</td><td>0.1019</td><td>0.1203</td><td>0.1288</td><td>0.1599</td><td>0.1939</td><td>0.1420</td><td>0.6254</td><td>0.2611</td><td>0.3343</td></tr><tr><td>InternVL3-76B [63]</td><td>0.0349</td><td>0.1052</td><td>0.0629</td><td>0.0827</td><td>0.1007</td><td>0.0406</td><td>0.5826</td><td>0.0924</td><td>0.0665</td></tr><tr><td>InternVL3.5-241B [49]</td><td>0.0475</td><td>0.0857</td><td>0.0237</td><td>0.1061</td><td>0.0933</td><td>0.0577</td><td>0.6403</td><td>0.1357</td><td>0.1117</td></tr><tr><td>Qwen2.5-VL-72B [3]</td><td>0.0422</td><td>0.0801</td><td>0.0586</td><td>0.1146</td><td>0.0681</td><td>0.0964</td><td>0.2380</td><td>0.1232</td><td>0.0264</td></tr><tr><td>Gemini-2.5 Pro [7]</td><td>0.0326</td><td>0.0182</td><td>0.0694</td><td>0.1618</td><td>0.0937</td><td>0.0161</td><td>0.1347</td><td>0.1169</td><td>0.0169</td></tr><tr><td>Dolphin [11]</td><td>0.0957</td><td>0.0453</td><td>0.0616</td><td>0.1333</td><td>0.1684</td><td>0.0702</td><td>0.2388</td><td>0.2561</td><td>0.0186</td></tr><tr><td rowspan=\"10\">Specialized VLMs</td><td>OCRFlux [5]</td><td>0.0870</td><td>0.0867</td><td>0.0818</td><td>0.1843</td><td>0.2072</td><td>0.1048</td><td>0.7304</td><td>0.1567</td><td>0.0193</td></tr><tr><td>Mistral-OCR [41]</td><td>0.0917</td><td>0.0531</td><td>0.0610</td><td>0.1349</td><td>0.1341</td><td>0.0581</td><td>0.5643</td><td>0.3097</td><td>0.0523</td></tr><tr><td>POINTS-Reader [22]</td><td>0.0334</td><td>0.0779</td><td>0.0671</td><td>0.1372</td><td>0.1901</td><td>0.1343</td><td>0.3789</td><td>0.0937</td><td>0.0951</td></tr><tr><td>olmOCR-7B [35]</td><td>0.0497</td><td>0.0365</td><td>0.0539</td><td>0.1204</td><td>0.0728</td><td>0.0697</td><td>0.2916</td><td>0.1220</td><td>0.0459</td></tr><tr><td>MinerU2-VLM[46]</td><td>0.0745</td><td>0.0104</td><td>0.0357</td><td>0.1276</td><td>0.0698</td><td>0.0652</td><td>0.1831</td><td>0.0803</td><td>0.0236</td></tr><tr><td>Nanonets-OCR-s [26]</td><td>0.0551</td><td>0.0578</td><td>0.0606</td><td>0.0931</td><td>0.0834</td><td>0.0917</td><td>0.1965</td><td>0.1606</td><td>0.0395</td></tr><tr><td>MonkeyOCR-pro-1.2B[17]</td><td>0.0961</td><td>0.0354</td><td>0.0530</td><td>0.1110</td><td>0.0887</td><td>0.0494</td><td>0.0995</td><td>0.1686</td><td>0.0198</td></tr><tr><td>MonkeyOCR-3B [17]</td><td>0.0904</td><td>0.0362</td><td>0.0489</td><td>0.1072</td><td>0.0745</td><td>0.0475</td><td>0.0962</td><td>0.1165</td><td>0.0196</td></tr><tr><td>dots.ocr [37]</td><td>0.0290</td><td>0.0231</td><td>0.0433</td><td>0.0788</td><td>0.0467</td><td>0.0221</td><td>0.0667</td><td>0.1116</td><td>0.0076</td></tr><tr><td>MonkeyOCR-pro-3B [17]</td><td>0.0879</td><td>0.0459</td><td>0.0517</td><td>0.1067</td><td>0.0726</td><td>0.0482</td><td>0.0937</td><td>0.1141</td><td>0.0211</td></tr><tr><td></td><td>MinerU2.5</td><td>0.0294</td><td>0.0235</td><td>0.0332</td><td>0.0499</td><td>0.0681</td><td>0.0316</td><td>0.0540</td><td>0.1161</td><td>0.0104</td></tr></table>",
        "bbox": [
            138,
            386,
            862,
            632
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "5 Evaluation ",
        "text_level": 1,
        "bbox": [
            137,
            683,
            290,
            704
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "In this section, we present a comprehensive quantitative evaluation of MinerU2.5 to demonstrate its effectiveness in document parsing tasks. Specifically, we compare MinerU2.5 against leading general-purpose VLMs including GPT-4o [1], Gemini-2.5 Pro [7], and Qwen2.5-VL [3], as well as state-of-the-art domain-specific VLMs such as dots.ocr [37], MonkeyOCR [17], and olmOCR [35]. The evaluation is organized into two parts: Section 5.1 presents full-document parsing results across multiple benchmarks, while Section 5.2 focuses on element-specific capabilities including layout analysis, formula recognition, and table recognition. ",
        "bbox": [
            135,
            715,
            862,
            823
        ],
        "page_idx": 18
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            930
        ],
        "page_idx": 18
    },
    {
        "type": "table",
        "img_path": "images/4b5d7af2569dad41691464800605369c1ead283b5dfbbb68d2062e5598a8d70c.jpg",
        "table_caption": [
            "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">Edit Distance</td><td colspan=\"2\">F1-score ↑</td><td colspan=\"2\">Precision↑</td><td colspan=\"2\">Recall↑</td><td colspan=\"2\">BLEU个</td><td colspan=\"2\">METEOR↑</td></tr><tr><td>en</td><td>zh</td><td>en</td><td>zh</td><td>en</td><td>zh</td><td>en</td><td>zh</td><td>en</td><td>zh</td><td>en</td><td>zh</td></tr><tr><td>Mathpix [27]</td><td>0.064</td><td>0.223</td><td>0.930</td><td>0.919</td><td>0.950</td><td>0.952</td><td>0.911</td><td>0.889</td><td>0.901</td><td>0.593</td><td>0.924</td><td>0.768</td></tr><tr><td>PP-StructureV3 [8]</td><td>0.068</td><td>0.210</td><td>0.871</td><td>0.929</td><td>0.856</td><td>0.924</td><td>0.892</td><td>0.935</td><td>0.796</td><td>0.570</td><td>0.902</td><td>0.802</td></tr><tr><td>MinerU2-pipeline [46]</td><td>0.099</td><td>0.225</td><td>0.663</td><td>0.919</td><td>0.635</td><td>0.908</td><td>0.703</td><td>0.934</td><td>0.504</td><td>0.571</td><td>0.670</td><td>0.810</td></tr><tr><td>PaddleOCR [8]</td><td>0.323</td><td>0.649</td><td>0.707</td><td>0.864</td><td>0.690</td><td>0.912</td><td>0.730</td><td>0.821</td><td>0.517</td><td>0.537</td><td>0.674</td><td>0.699</td></tr><tr><td>Gemini-2.5 Pro [7]</td><td>0.080</td><td>0.204</td><td>0.922</td><td>0.927</td><td>0.940</td><td>0.959</td><td>0.906</td><td>0.898</td><td>0.877</td><td>0.690</td><td>0.921</td><td>0.862</td></tr><tr><td>GPT-4o [1]</td><td>0.085</td><td>0.450</td><td>0.919</td><td>0.686</td><td>0.929</td><td>0.694</td><td>0.910</td><td>0.703</td><td>0.870</td><td>0.354</td><td>0.922</td><td>0.495</td></tr><tr><td>Qwen2.5-VL-72B [3]</td><td>0.093</td><td>0.140</td><td>0.923</td><td>0.940</td><td>0.936</td><td>0.956</td><td>0.912</td><td>0.926</td><td>0.879</td><td>0.798</td><td>0.924</td><td>0.876</td></tr><tr><td>InternVL3-76B [63]</td><td>0.125</td><td>0.282</td><td>0.828</td><td>0.871</td><td>0.842</td><td>0.889</td><td>0.817</td><td>0.856</td><td>0.728</td><td>0.527</td><td>0.829</td><td>0.759</td></tr><tr><td>Qwen2-VL-7B [48]</td><td>0.165</td><td>0.270</td><td>0.849</td><td>0.883</td><td>0.834</td><td>0.847</td><td>0.873</td><td>0.942</td><td>0.795</td><td>0.578</td><td>0.859</td><td>0.763</td></tr><tr><td>MiniCPM-V2.6-8B [54]</td><td>0.244</td><td>0.437</td><td>0.804</td><td>0.778</td><td>0.793</td><td>0.721</td><td>0.837</td><td>0.875</td><td>0.695</td><td>0.431</td><td>0.640</td><td>0.642</td></tr><tr><td>MinerU2-VLM[46]</td><td>0.048</td><td>0.182</td><td>0.936</td><td>0.941</td><td>0.926</td><td>0.927</td><td>0.947</td><td>0.958</td><td>0.893</td><td>0.611</td><td>0.950</td><td>0.837</td></tr><tr><td>Ocean-OCR [6]</td><td>0.057</td><td>0.062</td><td>0.937</td><td>0.962</td><td>0.932</td><td>0.956</td><td>0.956</td><td>0.974</td><td>0.906</td><td>0.912</td><td>0.945</td><td>0.916</td></tr><tr><td>MonkeyOCR-pro-1.2B [17]</td><td>0.064</td><td>0.190</td><td>0.929</td><td>0.934</td><td>0.918</td><td>0.925</td><td>0.944</td><td>0.948</td><td>0.884</td><td>0.699</td><td>0.941</td><td>0.850</td></tr><tr><td>SmolDocling [28]</td><td>0.080</td><td>0.878</td><td>0.899</td><td>0.157</td><td>0.895</td><td>0.140</td><td>0.912</td><td>0.268</td><td>0.839</td><td>0.048</td><td>0.907</td><td>0.151</td></tr><tr><td>dots.ocr [37]</td><td>0.083</td><td>0.179</td><td>0.904</td><td>0.931</td><td>0.920</td><td>0.951</td><td>0.890</td><td>0.913</td><td>0.849</td><td>0.639</td><td>0.911</td><td>0.842</td></tr><tr><td>GOT[52]</td><td>0.084</td><td>0.117</td><td>0.895</td><td>0.928</td><td>0.891</td><td>0.934</td><td>0.906</td><td>0.929</td><td>0.835</td><td>0.805</td><td>0.874</td><td>0.848</td></tr><tr><td>MinerU2.5</td><td>0.033</td><td>0.082</td><td>0.945</td><td>0.965</td><td>0.948</td><td>0.966</td><td>0.942</td><td>0.964</td><td>0.909</td><td>0.817</td><td>0.950</td><td>0.887</td></tr></table>",
        "bbox": [
            161,
            102,
            836,
            366
        ],
        "page_idx": 19
    },
    {
        "type": "table",
        "img_path": "images/fe8b2f37bddcc927fb6f6eef8271c2ae4798e6575daeea9458f4256c36e94596.jpg",
        "table_caption": [
            "Table 7: Evaluation results on Ocean-OCR bench on dense English (en) and Chinese (zh) OCR for document-level pages. Some model results are sourced from the OceanOCR official reports. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>Overall</td><td>AR</td><td>OSM</td><td>TA</td><td>OS</td><td>HF</td><td>MC</td><td>LTT</td><td>Base</td></tr><tr><td>MinerU2-pipeline[46]</td><td>55.6</td><td>61.8</td><td>13.5</td><td>60.9</td><td>17.3</td><td>96.6</td><td>59.0</td><td>39.1</td><td>96.6</td></tr><tr><td>Nanonets-OCR-s[26]</td><td>60.7</td><td>63.9</td><td>41.0</td><td>77.7</td><td>39.5</td><td>40.7</td><td>69.9</td><td>53.4</td><td>99.3</td></tr><tr><td>GPT-4o[1]</td><td>63.2</td><td>44.1</td><td>37.6</td><td>69.1</td><td>40.9</td><td>94.2</td><td>68.9</td><td>54.1</td><td>96.7</td></tr><tr><td>MonkeyOCR-pro-1.2B[17]</td><td>64.3</td><td>65.4</td><td>26.9</td><td>60.3</td><td>31.2</td><td>93.3</td><td>66.2</td><td>81.7</td><td>89.5</td></tr><tr><td>Qwen2.5-VL-72B[3]</td><td>64.8</td><td>72.2</td><td>51.1</td><td>67.3</td><td>38.6</td><td>73.6</td><td>68.3</td><td>49.1</td><td>98.3</td></tr><tr><td>MonkeyOCR-pro-3B[17]</td><td>68.8</td><td>67.7</td><td>28.4</td><td>74.6</td><td>36.1</td><td>91.2</td><td>76.6</td><td>80.1</td><td>95.3</td></tr><tr><td>olmOCR[35]</td><td>71.8</td><td>63.9</td><td>41.0</td><td>72.9</td><td>43.9</td><td>95.1</td><td>77.3</td><td>81.2</td><td>98.9</td></tr><tr><td>dots.ocr[37]</td><td>73.6</td><td>66.3</td><td>35.8</td><td>88.3</td><td>40.9</td><td>94.1</td><td>82.4</td><td>81.2</td><td>99.5</td></tr><tr><td>MinerU2.5</td><td>75.2</td><td>76.6</td><td>54.6</td><td>84.9</td><td>33.7</td><td>96.6</td><td>78.2</td><td>83.5</td><td>93.7</td></tr></table>",
        "bbox": [
            181,
            421,
            818,
            577
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Table 8: Evaluation results on olmOCR-bench grouped by document types, including arXiv Math(AR), Old Scans Math (OSM), Tables (TA), Old Scans (OS), Headers Footers (HF), Multi Column (MC) and Long Tiny Text (LTT). Results on AR and OSM are replaced with ExpRate, and other results are sourced from the official reports of olmOCR-bench and dots.ocr. The Overall Score (Overall) represents the average across all document types. ",
        "bbox": [
            135,
            587,
            864,
            665
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "5.1 Full-Document Parsing Task ",
        "text_level": 1,
        "bbox": [
            138,
            688,
            437,
            707
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "We evaluate MinerU2.5’s full document parsing performance on three prominent benchmarks: OmniDocBench [31], Ocean-OCR [6] benchmarks, and olmOCR-bench [35]. These benchmarks provide comprehensive evaluation from different dimensions, covering diverse document types, various quality conditions, and different parsing challenges to thoroughly assess the model’s robustness and generalization capabilities. ",
        "bbox": [
            137,
            714,
            862,
            790
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "• OmniDocBench [31]: This evaluation dataset is designed for diverse document parsing in realworld scenarios, encompassing nine document types, four layout types, and three language types. It offers a comprehensive assessment of parsing scores for text, formulas, tables, and reading order in full-document parsing, as well as for element-specific parsing tasks. ",
        "bbox": [
            160,
            796,
            862,
            859
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "• olmOCR-bench [35]: This evaluation dataset comprises 1,402 PDF documents sourced from various repositories, organized into seven subsets. Certain test patterns are applicable across all document types (e.g., presence, absence, reading order), while others are specifically targeted at challenging yet crucial content extraction objectives (e.g., tables, mathematical formulas). ",
        "bbox": [
            158,
            866,
            862,
            881
        ],
        "page_idx": 19
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            174,
            107,
            862,
            154
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "• Ocean-OCR benchmark [6]: This evaluation dataset consists of 100 images from English papers and 100 images from Chinese papers. It primarily evaluates the ability of text parsing and employs several text OCR-related evaluation metrics, such as Normalized Edit Distance, F1 Score, Precision, Recall, BLEU, and METEOR. ",
        "bbox": [
            163,
            160,
            861,
            220
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "5.1.1 Evaluation Details and Metrics ",
        "text_level": 1,
        "bbox": [
            137,
            238,
            418,
            255
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "For OmniDocBench [31], we evaluate on the latest version with three key improvements: ",
        "bbox": [
            143,
            262,
            774,
            279
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "• Enhanced resolution for Notes and Newspapers from 72 to $2 0 0 \\mathrm { D P I } ,$ , enabling more accurate evaluation of fine-grained text and handwritten content.   \n• An addition of 374 pages to balance Chinese-English content distribution and enrich mathematical formula coverage. Currently, it contains a total of 1,355 pages.   \n• Evaluation methodology updated to hybrid matching algorithm. ",
        "bbox": [
            160,
            285,
            864,
            377
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "The Overall score combines three core metrics: ",
        "bbox": [
            135,
            383,
            473,
            398
        ],
        "page_idx": 20
    },
    {
        "type": "equation",
        "img_path": "images/abe9fc24f14f025536c555f060a896694c4f2fec49073b2904366457312cbdc1.jpg",
        "text": "$$\n\\mathbf { O v e r a l l } = { \\frac { ( 1 - \\mathbf { T e x t ^ { E d i t } } ) \\times 1 0 0 + \\mathbf { T a b l e ^ { T E D S } } + \\mathbf { F o r m u l a ^ { C D M } } } { 3 } }\n$$",
        "text_format": "latex",
        "bbox": [
            277,
            415,
            718,
            450
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "For olmOCR-bench [35], we replace the formula scores of Arxiv Math (AR) and Old Scans Math (OSM) with the more reliable ExpRate of CDM [47]. The original evaluation compares LaTeX formulas by parsing them into abstract syntax trees and matching Unicode tokens, which is overly sensitive to syntax variations (e.g., \\cdots vs. \\dotsb) that render identically but are scored as different. To avoid this bias, we adopt ExpRate, which directly compares rendered outputs, assigning 1 for exact matches and 0 otherwise. ",
        "bbox": [
            135,
            459,
            862,
            551
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "5.1.2 Evaluation Results ",
        "text_level": 1,
        "bbox": [
            137,
            569,
            328,
            584
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "MinerU2.5 demonstrates exceptional performance across all benchmarks, achieving state-of-the-art results in most metrics (Tables 5 to 8). ",
        "bbox": [
            137,
            593,
            861,
            623
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "As shown in Table 5, MinerU2.5 achieves an overall score of 90.67 on OmniDocBench, outperforming the second-best model MonkeyOCR-pro-3B [17] by 1.82 and dots.ocr [37] by 2.26 points. In text recognition tasks, MinerU2.5 achieves the lowest edit distance of 0.047, marginally better than dots.ocr at 0.048 and significantly outperforming PP-StructureV3 [8], which scores 0.073. For formula recognition, MinerU2.5 leads with a CDM score of 88.46, exceeding both Qwen2.5-VL-72B at 88.27 and MonkeyOCR-3B at 87.45. In table recognition tasks, MinerU2.5 achieves the highest TEDS score of 88.22 and TEDS-S score of 92.38. For reading order evaluation, it maintains the best edit distance of 0.044. The document-type specific results presented in Table 6 demonstrate that MinerU2.5 achieves best or second-best performance in 6 out of 9 categories. For textbooks, it delivers the best performance with an edit distance of 0.0499, substantially outperforming dots.ocr’s 0.0788. For newspapers, MinerU2.5 leads with a score of 0.0540, surpassing all competing models. In both financial reports and slides categories, MinerU2.5 achieves second-best performance with scores of 0.0104 and 0.0294 respectively. ",
        "bbox": [
            137,
            630,
            862,
            813
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "For the results of the Ocean-OCR benchmark presented in Table 7, MinerU2.5 demonstrates exceptional performance in dense OCR tasks. On English documents, it achieves the lowest edit distance of 0.033 and the highest F1-score of 0.945, accompanied by best-in-class BLEU and METEOR scores of 0.909 and 0.950 respectively. For Chinese documents, MinerU2.5 achieves the highest F1-score of 0.965 and Precision of 0.966, while maintaining strong BLEU and METEOR scores of 0.817 and 0.887 respectively. ",
        "bbox": [
            137,
            819,
            862,
            896
        ],
        "page_idx": 20
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            506,
            931
        ],
        "page_idx": 20
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            142,
            58,
            864,
            73
        ],
        "page_idx": 20
    },
    {
        "type": "table",
        "img_path": "images/7f89f37943e0c08d3e63bae910ad57a8884533523e3a6ca4293d87a602bcc8a1.jpg",
        "table_caption": [
            "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">Textual</td><td colspan=\"2\">Image</td><td colspan=\"3\">Table</td><td colspan=\"3\">Equation</td><td colspan=\"3\">Page Margins</td><td colspan=\"3\">Full Page</td></tr><tr><td>P↑</td><td></td><td>R↑F1↑</td><td>P↑</td><td>R↑</td><td>F1↑ P↑</td><td>R↑</td><td>F1个</td><td>P↑</td><td>R个</td><td></td><td></td><td></td><td></td><td></td><td>F1↑P↑R↑F1↑P↑R↑F1↑</td><td></td></tr><tr><td></td><td colspan=\"10\"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LayoutLMv3 [14]</td><td>90.4</td><td>48.2</td><td>58.1</td><td>72.1</td><td>51.2 57.2</td><td>72.6</td><td>55.1</td><td>61.0</td><td>=</td><td>36.9</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MinerU2-VLM[46]</td><td>90.3</td><td>95.6 91.9</td><td></td><td>87.2</td><td>91.0 90.9</td><td>96.0</td><td>97.1</td><td>97.8</td><td>87.4</td><td>95.8</td><td>90.5</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DocLayout-YOLO [59]</td><td></td><td>95.4 98.3 96.5</td><td></td><td>87.6</td><td>96.7</td><td>94.7 94.9</td><td>98.1</td><td>98.4</td><td>95.3</td><td>90.6</td><td>93.8</td><td></td><td>98.7</td><td></td><td></td><td></td><td>92.3 97.7 94.1</td></tr><tr><td>PP-StructureV3 [8]</td><td>96.8</td><td>96.7</td><td>96.6</td><td>86.4</td><td>92.1</td><td>92.9 96.6</td><td>97.4</td><td>98.2</td><td>96.5</td><td>97.6</td><td>96.7</td><td>92.9</td><td>86.2</td><td>88.1</td><td>94.8</td><td></td><td>96.294.6</td></tr><tr><td>MinerU2.5</td><td>97.2</td><td>98.0</td><td>97.5</td><td>89.6</td><td>94.3</td><td>95.0 96.0</td><td>98.1</td><td>98.4</td><td>92.4</td><td>99.6</td><td>94.7</td><td>89.9</td><td>95.4</td><td></td><td>91.4 95.8</td><td></td><td>97.0 95.9</td></tr><tr><td colspan=\"10\">D4LA [9]</td><td colspan=\"10\"></td></tr><tr><td>LayoutLMv3 [14] 86.941.2 52.4</td><td></td><td></td><td></td><td></td><td>59.332.0 31.4</td><td></td><td></td><td>59.3 41.8 43.3</td><td></td><td>50.5</td><td>-</td><td></td><td>-</td><td></td><td>=</td><td></td><td>=</td></tr><tr><td>MinerU2-VLM[46]</td><td></td><td>88.388.9</td><td>87.9</td><td>56.7</td><td>35.0 38.1</td><td>89.1</td><td>84.1</td><td>90.6</td><td>38.3</td><td>99.4</td><td>79.1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DocLayout-YOLO [59]</td><td>86.3</td><td>97.8</td><td>90.8</td><td>41.5</td><td>92.9 62.6</td><td>87.6</td><td>89.0</td><td>89.8</td><td>31.9</td><td>80.2</td><td>91.1</td><td>-</td><td>95.0</td><td>-</td><td></td><td></td><td>82.6 95.4 87.3</td></tr><tr><td>PP-StructureV3 [8]</td><td>88.5</td><td>93.5</td><td>90.0</td><td>50.1</td><td>82.3 67.9</td><td>87.1</td><td>81.1</td><td>89.7</td><td>24.6</td><td>85.9</td><td>92.1</td><td>76.8</td><td>84.2</td><td>79.1</td><td>85.7</td><td></td><td>91.0 86.0</td></tr><tr><td>MinerU2.5</td><td>91.8</td><td>98.3</td><td>94.6</td><td>53.8</td><td>94.3 72.8</td><td>91.9</td><td>78.9</td><td>91.4</td><td>46.0</td><td>100.0</td><td>91.0</td><td>75.9</td><td>97.6</td><td>84.2</td><td>90.4</td><td></td><td>：92.5 90.2</td></tr><tr><td colspan=\"10\">DocLaynet [34]</td><td colspan=\"10\"></td></tr><tr><td>LayoutLMv3 [14]</td><td>88.8</td><td>59.3</td><td>67.9</td><td>79.0</td><td>50.3</td><td>61.9 75.2</td><td>54.9</td><td>61.8</td><td>=</td><td>31.9</td><td>-</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MinerU2-VLM[46]</td><td>88.1</td><td></td><td>96.1 91.7</td><td>85.5</td><td>78.1</td><td>91.3</td><td>94.9</td><td>94.4 95.6</td><td>83.9</td><td>97.0</td><td>90.0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DocLayout-YOLO [59]</td><td>86.9</td><td></td><td>96.8 91.2</td><td>85.8</td><td>96.2</td><td>91.3</td><td>92.0</td><td>95.7 94.8</td><td>80.5</td><td>86.9</td><td></td><td>82.8</td><td></td><td>97.7</td><td></td><td></td><td>88.096.3 90.9</td><td></td></tr><tr><td>PP-StructureV3 [8]</td><td>90.9</td><td>97.3</td><td>93.8</td><td>91.7</td><td>90.4</td><td>94.2</td><td>96.4</td><td>93.7 96.7</td><td></td><td>88.8</td><td>96.0</td><td>92.1</td><td>76.8</td><td>79.3</td><td>77.4</td><td>92.4</td><td>95.7</td><td>93.0</td></tr><tr><td>MinerU2.5</td><td>90.2</td><td>99.6</td><td>94.8</td><td>92.5</td><td>96.3</td><td>95.9</td><td>96.3</td><td>93.5 97.1</td><td>88.9</td><td></td><td>98.6</td><td>93.5</td><td>76.3</td><td>98.9</td><td>86.3</td><td>92.8</td><td>97.7</td><td>94.6</td></tr></table>",
        "bbox": [
            138,
            99,
            859,
            380
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Table 9: Comparison of layout analysis performance (Precision@PageIoU, Recall@PageIoU, F1- score@PageIoU) across different methods and content types on multiple layout analysis benchmarks. ",
        "bbox": [
            137,
            402,
            862,
            433
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "The results of olmOCR-bench are shown in Table 8, where MinerU2.5 achieves an overall score of 75.2, surpassing dots.ocr’s 73.6 by 1.6 points. In the arXiv Math category, it leads with a score of 76.6, outperforming Qwen2.5-VL-72B [3]’s 72.2 by 4.4 points. For Old Scans Math, MinerU2.5 dominates with a score of 54.6, exceeding all other evaluated models. In the Long Tiny Text category, it achieves 83.5, surpassing MonkeyOCR-pro-1.2B [17] which scores 81.7. ",
        "bbox": [
            137,
            458,
            862,
            536
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "5.2 Element-Specific Parsing Task ",
        "text_level": 1,
        "bbox": [
            135,
            554,
            450,
            573
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "5.2.1 Layout Analysis ",
        "text_level": 1,
        "bbox": [
            135,
            580,
            310,
            597
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "We validate the effectiveness of our layout analysis by performing a fair, zero-shot comparison with leading methods on three publicly available datasets: ",
        "bbox": [
            137,
            604,
            862,
            636
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "• OmniDocBench [31]: A recent benchmark for document parsing that includes detailed layout annotations. • $\\mathbf { D ^ { 4 } L A }$ [9]: Contains 11,092 noisy document images annotated with 27 categories, split into 8,868 training and 2,224 test images. We use its test set with annotations for evaluation. DocLayNet [34]: A large-scale dataset of 80,863 pages from 7 document types, manually annotated with 11 categories. We use its validation set with annotations for evaluation. ",
        "bbox": [
            151,
            642,
            862,
            748
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "We compare our MinerU2.5 with several recent methods, including LayoutLMv3 [14], MinerU2- VLM [46], DocLayout-YOLO [59] and PP-StructureV3 [8]. For a equitable assessment, we evaluate all models without dataset-specific training. To account for differences in detection granularity and category definitions, we unified the evaluation by mapping all labels to five broad categories and using the PageIoU metric, which assesses the spatial overlap without considering category labels for the “Full Page” score. ",
        "bbox": [
            135,
            755,
            862,
            847
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "The results in Table 9 show that MinerU2.5 significantly outperforms other models, achieving the top Full Page F1-score@PageIoU across all benchmarks. It also secures leading F1-scores@PageIoU for the ",
        "bbox": [
            135,
            853,
            861,
            885
        ],
        "page_idx": 21
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            930
        ],
        "page_idx": 21
    },
    {
        "type": "table",
        "img_path": "images/2dd714dc913f778bd89f32504be7810fcfd31c9bf88125860ea0d6f48486452e.jpg",
        "table_caption": [
            "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">PubTabNet</td><td colspan=\"2\">FinTabNet</td><td colspan=\"2\">CC-OCR</td><td colspan=\"2\">OCRBench v2</td><td colspan=\"2\">In-house TR Benchmark</td></tr><tr><td>TEDS↑</td><td>TEDS-S↑</td><td>TEDS↑</td><td>TEDS-S↑</td><td>TEDS↑</td><td>TEDS-S↑</td><td>TEDS↑</td><td>TEDS-S↑</td><td>TEDS↑</td><td>TEDS-S↑</td></tr><tr><td>RapidTable [36]</td><td>86.57</td><td>96.43</td><td>73.77</td><td>84.84</td><td>50.93</td><td>65.84</td><td>65.55</td><td>77.73</td><td>51.96</td><td>71.94</td></tr><tr><td>MiniCPM-V 4.5 [55]</td><td>80.30</td><td>87.67</td><td>85.41</td><td>89.18</td><td>68.49</td><td>77.55</td><td>80.28</td><td>85.65</td><td>55.47</td><td>69.61</td></tr><tr><td>InternVL3.5-241B [49]</td><td>83.75</td><td>88.76</td><td>84.74</td><td>87.92</td><td>62.87</td><td>69.52</td><td>79.5</td><td>85.81</td><td>56.32</td><td>69.3</td></tr><tr><td>Qwen2.5-VL-7B[3]</td><td>81.60</td><td>86.78</td><td>82.58</td><td>87.46</td><td>78.29</td><td>84.26</td><td>77.44</td><td>84.71</td><td>57.34</td><td>73.17</td></tr><tr><td>Qwen2.5-VL-72B [3]</td><td>84.39</td><td>87.91</td><td>82.90</td><td>87.13</td><td>81.22</td><td>86.48</td><td>81.33</td><td>86.58</td><td>62.79</td><td>76.91</td></tr><tr><td>GPT-4o [1]</td><td>76.53</td><td>86.16</td><td>83.94</td><td>87.00</td><td>66.98</td><td>79.04</td><td>70.51</td><td>79.55</td><td>46.99</td><td>70.29</td></tr><tr><td>Gemini-2.5 Pro [7]</td><td>1</td><td>-</td><td>-</td><td>1</td><td>85.56</td><td>90.07</td><td>88.94</td><td>89.47</td><td>69.72</td><td>81.29</td></tr><tr><td>dots.ocr [37]</td><td>90.65</td><td>93.76</td><td>84.12</td><td>87.86</td><td>75.42</td><td>81.65</td><td>82.04</td><td>86.27</td><td>66.91</td><td>79.27</td></tr><tr><td>Nanonets-OCR-s [26]</td><td>63.58</td><td>75.68</td><td>68.06</td><td>73.6</td><td>66.15</td><td>71.33</td><td>69.66</td><td>76.28</td><td>54.35</td><td>66.12</td></tr><tr><td>MinerU2-VLM[46]</td><td>88.11</td><td>90.85</td><td>78.49</td><td>83.03</td><td>64.61</td><td>71.8</td><td>73.22</td><td>78.24</td><td>63.54</td><td>76.66</td></tr><tr><td>MinerU2.5</td><td>89.07</td><td>93.11</td><td>95.97</td><td>97.61</td><td>79.76</td><td>85.16</td><td>87.13</td><td>90.62</td><td>71.48</td><td>82.83</td></tr></table>",
        "bbox": [
            138,
            98,
            859,
            276
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Table 10: Table Recognition Performance. MinerU2.5 achieves SOTA performance on most benchmarks among TEDS and TEDS-S metrics, and the remaining ones are also generally competitive with the SOTA. (CCOCR and OCRBench v2 are OCR evaluation benchmarks, we only select the subsets that contain tables. PubTabNet and FinTabNet have a large number of images, so we have not evaluate Gemini-2.5 Pro on them.). ",
        "bbox": [
            135,
            297,
            862,
            375
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "majority of individual element types. This consistent superiority confirms that the PageIoU metric provides a robust basis for comparison, effectively capturing model performances independent of annotation inconsistencies. ",
        "bbox": [
            137,
            401,
            862,
            446
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "5.2.2 Table Recognition ",
        "text_level": 1,
        "bbox": [
            137,
            465,
            323,
            481
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "We evaluate representative methods, covering traditional table recognition methods, general multimodal large models and document parsing models, on five table recognition benchmarks as shown in Table 10. Below is an introduction to each benchmark: ",
        "bbox": [
            137,
            489,
            862,
            535
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "• PubTabNet [62] is the first large-scale table recognition dataset that provides annotations (in HTML format) of table images, captured from scientific articles. PubTabNet contains 9k tables in its test set.   \n• FinTabNet [61] is a dataset containing tables from the annual reports of 500 companies. The major challenge of this benchmark is that financial tables largely differ from scientific and government document tables in that the former has fewer graphical lines, larger gaps within each table, and more color variations. FinTabNet contains 10k tables in its test set.   \nCC-OCR [53] and OCRBench v2 [12] are both designed to evaluate the OCR capabilities of multimodal large models and contain several OCR tasks. We only retain the data related to document recognition and those images that include tables. After filtering, CC-OCR remains 300 images and OCRBench v2 remains 700 images.   \nIn-house TR Benchmark. To better evaluate the table recognition accuracy of different methods, we considering various table attributes such as the number of table rows and columns, the number of merged cells, the length of the table, the length of the cell content, the type of cell content, the line style of the table, and construct a very diverse evaluation set, which contains approximately 500 tables. ",
        "bbox": [
            160,
            541,
            864,
            808
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "MinerU2.5 achieves SOTA performance on most benchmarks, and shows competitive results with the SOTA on the remaining ones. Specifically, for PubTabNet, Rapidtable [36] achieves the best performance in the TEDS-S metric, while dots.ocr [37] excel in the TEDS metric. Meanwhile, despite using only $2 0 \\%$ of the PubTabNet training set, MinerU2.5 still demonstrate comparable results, coming second and third in TEDS and TEDS-S, respectively. For FinTabNet, MinerU2.5 achieves the best result ",
        "bbox": [
            137,
            814,
            862,
            890
        ],
        "page_idx": 22
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 22
    },
    {
        "type": "table",
        "img_path": "images/46b555625cd8f03a8b735a87cfce53b9900b32351f2ffb49b20ebc4e40a53152.jpg",
        "table_caption": [
            "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td colspan=\"8\"></td></tr><tr><td rowspan=\"2\">Method</td><td colspan=\"4\">Public Dataset</td><td colspan=\"3\">In-house Dataset</td></tr><tr><td>CPE</td><td>HWE</td><td>SCE</td><td>SPE LaTeX-80MM</td><td>Chinese</td><td>Fuzzy Math</td><td>Complex</td></tr><tr><td>UniMERNet* [45]</td><td>98.2</td><td>96.5</td><td>95.4</td><td>99.2 83.9</td><td>84.0</td><td>84.3</td><td>67.9</td></tr><tr><td>PP-Formula_plus-L [21]</td><td>98.2</td><td>94.7</td><td>95.7 99.2</td><td>85.9</td><td>84.0</td><td>86.5</td><td>76.5</td></tr><tr><td>Gemini-2.5-flash [7]</td><td>89.2</td><td>90.0</td><td>85.1 97.5</td><td>78.7</td><td>88.1</td><td>89.4</td><td>80.1</td></tr><tr><td>Qwen2.5-VL-72B [3]</td><td>88.9</td><td>91.8</td><td>95.5</td><td>96.2 83.4</td><td>90.8</td><td>86.7</td><td>81.4</td></tr><tr><td>GPT-4o [1]</td><td>82.7</td><td>85.9</td><td>87.8</td><td>96.7 73.4</td><td>88.3</td><td>85.0</td><td>78.6</td></tr><tr><td>InternVL3.5-241B [49]</td><td>91.7</td><td>93.2</td><td>95.1</td><td>97.8 86.9</td><td>82.7</td><td>90.3</td><td>82.0</td></tr><tr><td>dots.ocr [37]</td><td>86.8</td><td>90.5</td><td>94.7</td><td>97.5 81.8</td><td>74.4</td><td>86.2</td><td>77.4</td></tr><tr><td>MinerU2.5</td><td>96.6</td><td>94.4</td><td>96.4</td><td>98.4 90.6</td><td>90.7</td><td>92.6</td><td>82.2</td></tr></table>",
        "bbox": [
            138,
            88,
            859,
            276
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Table 11: Formula Recognition Performance (CDM metric used for evaluation). MinerU2.5 achieves 4 SOTA results and one second-best result across 7 benchmarks. Latex- ${ \\bf \\delta } \\cdot 8 0 { \\bf M } ^ { M }$ denotes the matrix benchmark of Latex-80M dataset. ∗ indicates that the UniMERNet results are based on an improved version compared to the publicly available open-source implementation. ",
        "bbox": [
            135,
            299,
            862,
            361
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "and outperform other methods by a significant margin, this could be mainly credited to the large-scale high-quality table data we extracted from financial reports for training. On CC-OCR benchmark, MinerU2.5 came third after Gemini-2.5 Pro and Qwen2.5-VL-72B. On OCRBench v2 benchmark, MinerU2.5’s performance is competitive to that of Gemini-2.5 Pro, and it significantly outperform other methods. On the diverse In-house TR Benchmark, MinerU2.5 and Gemini-2.5 Pro both significantly outperform other methods, with MinerU2.5 achieving a slight advantage over Gemini- $. 2 . 5 \\mathrm { P r o }$ . ",
        "bbox": [
            137,
            387,
            862,
            478
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "5.2.3 Formula Recognition ",
        "text_level": 1,
        "bbox": [
            137,
            497,
            344,
            512
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "For formula recognition, comparison models include various approaches, covering specialized formula recognition models, document parsing models, and general vision-language models. The evaluation datasets consist of the following: ",
        "bbox": [
            138,
            521,
            862,
            566
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "• UniMER-Test [45] is a comprehensive evaluation dataset for general formula recognition. Targeted at real-world formula recognition across various scenarios, UniMER-Test includes four subsets: CPE (complex printed equations), HWE (handwritten equations), SPE (screen printed equations), and SCE (simple printed equations).   \n• LaTeX- ${ \\bf 8 0 M } ^ { M }$ is a matrix subset of $\\mathrm { L a T e } X { - } 8 0 \\mathrm { M } ^ { 5 }$ , featuring intricate mathematical structures encompassing matrices, conditional expressions, and nested combinations.   \n• In-house dataset consists of the following subsets: (1) Chinese, targeted at evaluation on realworld document equations which contain Chinese characters. (2) Fuzzy math, which focuses on authentic mathematics textbooks and exam documents characterized by compromised visual quality due to factors like blur, degeneration, watermarks, and so on. (3) Complex, an extremely difficult dataset aimed at assessing the ability of converting the most complex mathematical formulas to LaTeX codes. ",
        "bbox": [
            160,
            574,
            862,
            770
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Results are shown in Table 11 and the CDM [47] metric is used for evaluation. Across all seven evaluation datasets, MinerU2.5 achieves the best results in four datasets and one second-best result, demonstrating SOTA formula recognition capabilities. Specifically, on public datasets, MinerU2.5 achieves best CDM results of 96.4 on SCE and 90.6 on LaTeX-80MM, showcasing leading performance in scenarios involving blurred screenshots and complex matrices. Besides, on CPE, HWE, and SPE, while being slightly outperformed by specialized formula recognition models, MinerU2.5 still deliver comparable performance. On in-house evaluation datasets, MinerU2.5’s performance in Chinese text recognition is on par with Qwen2.5-VL-72B, leading to a second-place result of 90.6. Meanwhile, MinerU2.5 achieves the best results on both the real-world mathematic documents (Fuzzy Math) and extremely hard formula recognition (Complex). ",
        "bbox": [
            137,
            777,
            862,
            869
        ],
        "page_idx": 23
    },
    {
        "type": "discarded",
        "text": "5https://github.com/OleehyO/TexTeller ",
        "bbox": [
            155,
            880,
            397,
            891
        ],
        "page_idx": 23
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            137,
            107,
            862,
            167
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "6 Conclusion ",
        "text_level": 1,
        "bbox": [
            137,
            190,
            297,
            212
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "In this paper, we present MinerU2.5, a 1.2B-parameter vision-language model that achieves a new state-of-the-art in efficient document parsing through its innovative decoupled, coarse-to-fine strategy. By separating global layout analysis from local recognition, it delivers unprecedented accuracy in a lightweight model, effectively resolving the trade-off between performance and cost. Beyond its standalone capabilities, the primary significance of MinerU2.5 lies in its role as a foundational tool for the LLM era. Its ability to rapidly convert vast, unstructured document collections into clean, structured data is invaluable for curating high-quality pre-training corpora. Furthermore, by preserving the semantic integrity of tables, formulas, and layouts, it is poised to significantly enhance the quality and reliability of Retrieval-Augmented Generation (RAG) systems, unlocking the vast knowledge contained within complex documents for next-generation AI applications. ",
        "bbox": [
            135,
            223,
            862,
            376
        ],
        "page_idx": 24
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            930
        ],
        "page_idx": 24
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            140,
            58,
            864,
            73
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "References ",
        "text_level": 1,
        "bbox": [
            137,
            103,
            254,
            123
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. ",
        "bbox": [
            142,
            130,
            861,
            171
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[2] Haoli Bai, Zhiguang Liu, Xiaojun Meng, Wentao Li, Shuang Liu, Nian Xie, Rongfu Zheng, Liangwei Wang, Lu Hou, Jiansheng Wei, et al. Wukong-reader: Multi-modal pre-training for fine-grained visual document understanding. arXiv preprint arXiv:2212.09621, 2022. ",
        "bbox": [
            142,
            179,
            861,
            222
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. ",
        "bbox": [
            140,
            228,
            862,
            257
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[4] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418, 2023. ",
        "bbox": [
            142,
            262,
            859,
            292
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[5] chatdoc com. Ocrflux. https://github.com/chatdoc-com/OCRFlux, 2025. Accessed:2025-09-25. ",
        "bbox": [
            142,
            299,
            779,
            313
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[6] Song Chen, Xinyu Guo, Yadong Li, Tao Zhang, Mingan Lin, Dongdong Kuang, Youwei Zhang, Lingfeng Ming, Fengyu Zhang, Yuran Wang, et al. Ocean-ocr: Towards general ocr application via a vision-language model. arXiv preprint arXiv:2501.15558, 2025. ",
        "bbox": [
            138,
            320,
            864,
            362
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. ",
        "bbox": [
            137,
            368,
            862,
            411
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[8] Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, et al. Paddleocr 3.0 technical report. arXiv preprint arXiv:2507.05595, 2025. ",
        "bbox": [
            137,
            417,
            864,
            446
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[9] Cheng Da, Chuwei Luo, Qi Zheng, and Cong Yao. Vision grid transformer for document layout analysis. In Proceedings of the IEEE/CVF international conference on computer vision, pages 19462–19472, 2023. ",
        "bbox": [
            137,
            453,
            862,
            482
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[10] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin, et al. Patch n’pack: Navit, a vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36: 2252–2274, 2023. ",
        "bbox": [
            137,
            487,
            862,
            544
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[11] Hao Feng, Shu Wei, Xiang Fei, Wei Shi, Yingdong Han, Lei Liao, Jinghui Lu, Binghong Wu, Qi Liu, Chunhui Lin, et al. Dolphin: Document image parsing via heterogeneous anchor prompting. arXiv preprint arXiv:2505.14059, 2025. ",
        "bbox": [
            135,
            550,
            859,
            593
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[12] Ling Fu, Zhebin Kuang, Jiajun Song, Mingxin Huang, Biao Yang, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, et al. Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning. arXiv preprint arXiv:2501.00321, 2024. ",
        "bbox": [
            133,
            599,
            862,
            642
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[13] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. ",
        "bbox": [
            133,
            648,
            862,
            678
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[14] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document ai with unified text and image masking. In Proceedings of the 30th ACM international conference on multimedia, pages 4083–4091, 2022. ",
        "bbox": [
            133,
            684,
            861,
            727
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[15] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision, pages 498–517. Springer, 2022. ",
        "bbox": [
            133,
            733,
            861,
            776
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[16] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. ",
        "bbox": [
            133,
            782,
            859,
            825
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[17] Zhang Li, Yuliang Liu, Qiang Liu, Zhiyin Ma, Ziyang Zhang, Shuo Zhang, Zidun Guo, Jiarui Zhang, Xinyu Wang, and Xiang Bai. Monkeyocr: Document parsing with a structure-recognition-relation triplet paradigm. arXiv preprint arXiv:2506.05218, 2025. ",
        "bbox": [
            137,
            832,
            862,
            873
        ],
        "page_idx": 25
    },
    {
        "type": "discarded",
        "text": "26 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 25
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            138,
            58,
            864,
            73
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing   \n[18] Haofu Liao, Aruni RoyChowdhury, Weijian Li, Ankan Bansal, Yuting Zhang, Zhuowen Tu, Ravi Kumar Satzoda, R Manmatha, and Vijay Mahadevan. Doctr: Document transformer for structured information extraction in documents. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19584–19594, 2023.   \n[19] Demiao Lin. Revolutionizing retrieval-augmented generation with enhanced pdf structure recognition. arXiv preprint arXiv:2401.12599, 2024.   \n[20] Chaohu Liu, Kun Yin, Haoyu Cao, Xinghua Jiang, Xin Li, Yinsong Liu, Deqiang Jiang, Xing Sun, and Linli Xu. Hrvda: High-resolution visual document assistant. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15534–15545, 2024.   \n[21] Hongen Liu, Cheng Cui, Yuning Du, Yi Liu, and Gang Pan. Pp-formulanet: Bridging accuracy and efficiency in advanced formula recognition. arXiv preprint arXiv:2503.18382, 2025.   \n[22] Yuan Liu, Zhongyin Zhao, Le Tian, Haicheng Wang, Xubing Ye, Yangxiu You, Zilin Yu, Chuhan Wu, Xiao Zhou, Yang Yu, et al. Points-reader: Distillation-free adaptation of vision-language models for document conversion. arXiv preprint arXiv:2509.01215, 2025.   \n[23] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. Textmonkey: An ocr-free large multimodal model for understanding document. arXiv preprint arXiv:2403.04473, 2024.   \n[24] Nikolaos Livathinos, Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Kasper Dinkla, Yusik Kim, et al. Docling: An efficient open-source toolkit for ai-driven document conversion. arXiv preprint arXiv:2501.17887, 2025.   \n[25] Maksym Lysak, Ahmed Nassar, Nikolaos Livathinos, Christoph Auer, and Peter Staar. Optimized table tokenization for table structure recognition. In International Conference on Document Analysis and Recognition, pages 37–50. Springer, 2023.   \n[26] Souvik Mandalm. Nanonets-ocr-s. https://nanonets.com/research/nanonets-ocr-s/, 2025. Accessed:2025- 09-25.   \n[27] Mathpix. Mathpix. https://mathpix.com/, 2025. Accessed:2025-09-25.   \n[28] Ahmed Nassar, Andres Marafioti, Matteo Omenetti, Maksym Lysak, Nikolaos Livathinos, Christoph Auer, Lucas Morin, Rafael Teixeira de Lima, Yusik Kim, A Said Gurbuz, et al. Smoldocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion. arXiv preprint arXiv:2503.11576, 2025.   \n[29] Junbo Niu, Yuanhong Zheng, Ziyang Miao, Hejun Dong, Chunjiang Ge, Hao Liang, Ma Lu, Bohan Zeng, Qiahao Zheng, Conghui He, et al. Native visual understanding: Resolving resolution dilemmas in visionlanguage models. arXiv preprint arXiv:2506.12776, 2025.   \n[30] OpenDataLab. Pdf-extract-kit. https://github.com/opendatalab/PDF-Extract-Kit, 2025. Accessed:2025- 09-25.   \n[31] Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, et al. Omnidocbench: Benchmarking diverse pdf document parsing with comprehensive annotations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 24838–24848, 2025.   \n[32] Vik Paruchuri. Marker. https://github.com/datalab-to/marker, 2025. Accessed:2025-09-25.   \n[33] Vikas Paruchuri and Datalab Team. Surya: A lightweight document ocr and analysis toolkit. https: //github.com/VikParuchuri/surya, 2025. Accessed:2025-09-25.   \n[34] Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S Nassar, and Peter Staar. Doclaynet: A large human-annotated dataset for document-layout segmentation. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pages 3743–3751, 2022.   \n[35] Jake Poznanski, Aman Rangapur, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Christopher Wilhelm, Kyle Lo, and Luca Soldaini. olmocr: Unlocking trillions of tokens in pdfs with vision language models. arXiv preprint arXiv:2502.18443, 2025. ",
        "bbox": [
            132,
            44,
            866,
            896
        ],
        "page_idx": 26
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            930
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "[36] RapidAI. Rapid table. https://github.com/RapidAI/RapidTable, 2024. Accessed: 2025-9-25. ",
        "bbox": [
            135,
            108,
            769,
            122
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[37] rednote. dots.ocr: Multilingual document layout parsing in a single vision-language model. https://github. com/rednote-hilab/dots.ocr, 2025. Accessed:2025-09-25. ",
        "bbox": [
            135,
            130,
            861,
            157
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[38] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert,´ and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1874–1883, 2016. ",
        "bbox": [
            135,
            165,
            861,
            219
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[39] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. ",
        "bbox": [
            133,
            227,
            861,
            256
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[40] Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit Bansal. Unifying vision, text, and layout for universal document processing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19254–19264, 2023. ",
        "bbox": [
            133,
            262,
            862,
            305
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[41] Mistral AI Team. Mistral-ocr. https://mistral.ai/news/mistral-ocr?utm source=ai-bot.cn, 2025. Accessed:2025-09-25. ",
        "bbox": [
            133,
            313,
            864,
            339
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[42] Qwen Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. ",
        "bbox": [
            133,
            347,
            658,
            362
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[43] Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, and Zhibo Yang. Omniparser: A unified framework for text spotting key information extraction and table recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15641–15653, 2024. ",
        "bbox": [
            133,
            368,
            862,
            424
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[44] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, et al. Yolov10: Real-time end-to-end object detection. Advances in Neural Information Processing Systems, 37:107984–108011, 2024. ",
        "bbox": [
            133,
            431,
            859,
            460
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[45] Bin Wang, Zhuangcheng Gu, Guang Liang, Chao Xu, Bo Zhang, Botian Shi, and Conghui He. Unimernet: A universal network for real-world mathematical expression recognition. arXiv preprint arXiv:2404.15254, 2024. ",
        "bbox": [
            135,
            467,
            864,
            496
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[46] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan $\\mathrm { Q u } ,$ Fukai Shang, et al. Mineru: An open-source solution for precise document content extraction. arXiv preprint arXiv:2409.18839, 2024. ",
        "bbox": [
            133,
            501,
            864,
            544
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[47] Bin Wang, Fan Wu, Linke Ouyang, Zhuangcheng Gu, Rui Zhang, Renqiu Xia, Botian Shi, Bo Zhang, and Conghui He. Image over text: Transforming formula recognition evaluation with character detection matching. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 19681–19690, 2025. ",
        "bbox": [
            132,
            550,
            861,
            593
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[48] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. ",
        "bbox": [
            132,
            599,
            861,
            642
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[49] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. ",
        "bbox": [
            132,
            648,
            861,
            691
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[50] Zilong Wang, Yiheng Xu, Lei Cui, Jingbo Shang, and Furu Wei. Layoutreader: Pre-training of text and layout for reading order detection. arXiv preprint arXiv:2108.11591, 2021. ",
        "bbox": [
            133,
            698,
            861,
            727
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[51] Zilong Wang, Yichao Zhou, Wei Wei, Chen-Yu Lee, and Sandeep Tata. Vrdu: A benchmark for visually-rich document understanding. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5184–5193, 2023. ",
        "bbox": [
            133,
            733,
            862,
            775
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[52] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via a unified end-to-end model. arXiv preprint arXiv:2409.01704, 2024. ",
        "bbox": [
            133,
            782,
            861,
            824
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[53] Zhibo Yang, Jun Tang, Zhaohai Li, Pengfei Wang, Jianqiang Wan, Humen Zhong, Xuejing Liu, Mingkun Yang, Peng Wang, Shuai Bai, et al. Cc-ocr: A comprehensive and challenging ocr benchmark for evaluating large multimodal models in literacy. arXiv preprint arXiv:2412.02210, 2024. ",
        "bbox": [
            137,
            832,
            862,
            873
        ],
        "page_idx": 27
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 27
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            138,
            58,
            862,
            74
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "[54] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. ",
        "bbox": [
            135,
            107,
            862,
            136
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "[55] Tianyu Yu, Zefan Wang, Chongyi Wang, Fuwei Huang, Wenshuo Ma, Zhihui He, Tianchi Cai, Weize Chen, Yuxiang Huang, Yuanqian Zhao, Bokai Xu, Junbo Cui, Yingjing Xu, Liqing Ruan, Luoyuan Zhang, Hanyu Liu, Jingkun Tang, Hongyuan Liu, Qining Guo, Wenhao Hu, Bingxiang He, Jie Zhou, Jie Cai, Ji Qi, Zonghao Guo, Chi Chen, Guoyang Zeng, Yuxuan Li, Ganqu Cui, Ning Ding, Xu Han, Yuan Yao, Zhiyuan Liu, and Maosong Sun. Minicpm-v 4.5: Cooking efficient mllms via architecture, data, and training recipe. arXiv preprint arXiv:2509.18154, 2025. ",
        "bbox": [
            137,
            142,
            862,
            227
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "[56] Junyuan Zhang, Qintong Zhang, Bin Wang, Linke Ouyang, Zichen Wen, Ying Li, Ka-Ho Chow, Conghui He, and Wentao Zhang. Ocr hinders rag: Evaluating the cascading impact of ocr on retrieval-augmented generation. arXiv preprint arXiv:2412.02592, 2024. ",
        "bbox": [
            132,
            234,
            861,
            276
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "[57] Qintong Zhang, Bin Wang, Victor Shea-Jay Huang, Junyuan Zhang, Zhengren Wang, Hao Liang, Conghui He, and Wentao Zhang. Document parsing unveiled: Techniques, challenges, and prospects for structured information extraction. arXiv preprint arXiv:2410.21169, 2024. ",
        "bbox": [
            132,
            284,
            862,
            325
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "[58] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A survey. arXiv preprint arXiv:2402.19473, 2024. ",
        "bbox": [
            135,
            332,
            861,
            373
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "[59] Zhiyuan Zhao, Hengrui Kang, Bin Wang, and Conghui He. Doclayout-yolo: Enhancing document layout analysis through diverse synthetic data and global-to-local adaptive perception. arXiv preprint arXiv:2410.12628, 2024. ",
        "bbox": [
            133,
            381,
            862,
            422
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "[60] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao ${ \\mathrm { Y u } } ,$ Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37:62557–62583, 2024. ",
        "bbox": [
            133,
            429,
            862,
            472
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "[61] Xinyi Zheng, Douglas Burdick, Lucian Popa, Xu Zhong, and Nancy Xin Ru Wang. Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 697–706, 2021. ",
        "bbox": [
            132,
            478,
            861,
            522
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "[62] Xu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno Yepes. Image-based table recognition: data, model, and evaluation. In European conference on computer vision, pages 564–580. Springer, 2020. ",
        "bbox": [
            132,
            527,
            862,
            558
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "[63] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. ",
        "bbox": [
            133,
            564,
            861,
            606
        ],
        "page_idx": 28
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            930
        ],
        "page_idx": 28
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            140,
            58,
            864,
            73
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Appendix ",
        "text_level": 1,
        "bbox": [
            137,
            103,
            266,
            128
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "A Qualitative examples ",
        "text_level": 1,
        "bbox": [
            138,
            148,
            408,
            170
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "This section presents qualitative examples illustrating the capabilities of the MinerU2.5 through document parsing outputs generated for various pages. This section is structured as follows: Section A.1 illustrates the MinerU2.5’s performance on Document Parsing, Table Recognition and Formula Recognition among all types of documents. Section A.2 showcases specific attribute pages with improved performance. Section A.3 demonstrates MinerU2.5’s performance on some complex pages compared to other models. ",
        "bbox": [
            135,
            181,
            864,
            272
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Examples demonstrating the Document Parsing performance among PDF types are provided in Figures 8 to 10, including Academic literature, Books, Textbooks, Research Report, Financial Report, Slides, Exam Paper, Note, Newspaper and Magazine. ",
        "bbox": [
            137,
            280,
            862,
            327
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Table Recognition performance among various types of tables is demonstrated in Figures 11 and 12, including the photograph of the table, table with colorful background, table with formula, table with empty cells, handwritten table, large table, rotated table, no-line table, three-line table, and full-line table. ",
        "bbox": [
            137,
            333,
            862,
            393
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "The performance of Formula Recognition among types of formulas is demonstrated in Figures 13 and 14, including formula with background, formula with Chinese, formula with matrix, formula with condition and nested condition, handwritten formula, blurred formula, multi-column formula, degradation formula. ",
        "bbox": [
            137,
            400,
            862,
            462
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Figures 15 to 18 demonstrate that MinerU2.5’s document parsing ability improved when encounter rotated tables, table with merged cells, formula with Chinese and multi-line and complex formula, comparing with previous version (MinerU2-VLM, MinerU2-pipeline). Moreover, MinerU2.5 achieves finer bounding bbox in layout detection and performs better on watermark pages than previous version, as illustrated in Figures 19 and 20. ",
        "bbox": [
            137,
            468,
            862,
            545
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "MinerU2.5 achieves outstanding performance in scenarios involving PDF pages with complex elements, and its performance is relatively better compared to existing state-of-the-art models. ",
        "bbox": [
            135,
            551,
            864,
            583
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Figures 21 to 26 showcase the scenarios with complex tables in the page, including full-page table, content dense table, colorful table with amounts of empty cells, a tightly-arranged multiple table, table with irregular merged cells, a table without lines. MinerU2.5 can achieve better parsing outputs on these pages, while other models encounter errors such as table structure error, table structure lost, table content lost and table split error. ",
        "bbox": [
            137,
            589,
            862,
            665
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Figures 27 to 29 illustrates the performance of MinerU2.5 in the page with nested conditional expressions, complex matrix and nested matrix compared to other SOTA models, MinerU2.5 can correctly parse the complex formula while others might generate wrong outputs. ",
        "bbox": [
            137,
            672,
            864,
            719
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Figures 30 to 32 shows MinerU2.5’s outstanding performance in pages with complex layout, e.g., alternating texts and images, with very-few frame tables, and pages with watermark compared with others. ",
        "bbox": [
            137,
            724,
            862,
            772
        ],
        "page_idx": 29
    },
    {
        "type": "discarded",
        "text": "30 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 29
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            137,
            58,
            864,
            73
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "A.1 Overview ",
        "text_level": 1,
        "bbox": [
            137,
            106,
            276,
            122
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "A.1.1 Among PDF types ",
        "text_level": 1,
        "bbox": [
            137,
            131,
            328,
            147
        ],
        "page_idx": 30
    },
    {
        "type": "image",
        "img_path": "images/059cb8ac954be7e32e9aa2c07e0f39bb3b42596a884df341e4b9a2fe7423de4a.jpg",
        "image_caption": [
            "Figure 8: The Layout and rendered markdown output for Academic literature, Books, Textbooks. "
        ],
        "image_footnote": [],
        "bbox": [
            179,
            209,
            820,
            864
        ],
        "page_idx": 30
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            506,
            931
        ],
        "page_idx": 30
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            140,
            58,
            864,
            73
        ],
        "page_idx": 30
    },
    {
        "type": "image",
        "img_path": "images/62316db2437d4cad2254a155d434c0284d1fe52dc2bda6fcf79da3e9821d622c.jpg",
        "image_caption": [
            "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
            "Figure 9: The Layout and rendered markdown output for Research Report, Financial Report, Slides and Exam Paper. "
        ],
        "image_footnote": [],
        "bbox": [
            176,
            207,
            823,
            808
        ],
        "page_idx": 31
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 31
    },
    {
        "type": "image",
        "img_path": "images/fd69903d5c3c2e6ce8dc191dab74738d977249daf581f29521ec88eaceefcd42.jpg",
        "image_caption": [
            "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
            "Figure 10: The Layout and rendered markdown output for Note, Newspaper and Magazine. "
        ],
        "image_footnote": [],
        "bbox": [
            163,
            94,
            834,
            820
        ],
        "page_idx": 32
    },
    {
        "type": "discarded",
        "text": "33 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "A.1.2 Among Table types ",
        "text_level": 1,
        "bbox": [
            137,
            107,
            336,
            123
        ],
        "page_idx": 33
    },
    {
        "type": "image",
        "img_path": "images/428ce81c3e3b0c119911067a5d9ae8aab97e7f76f6b6705d114bd30ed67b7d7f.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            196,
            140,
            803,
            281
        ],
        "page_idx": 33
    },
    {
        "type": "table",
        "img_path": "images/7de2ed2d4455456b9cbb8e5aacceb1d0f9705eaa07967b7cbdb86d90adaae9dd.jpg",
        "table_caption": [
            "Colorful Table ",
            "Large Table "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>见底/见顶时间T</td><td>市场形态</td><td colspan=\"2\">T±5</td><td>T±10</td><td>T±15</td></tr><tr><td>2015-06-12</td><td>顶部</td><td colspan=\"2\">724.67</td><td>805.37</td><td>1950.11</td></tr><tr><td>2015-09-15</td><td>底部</td><td colspan=\"2\">391.78</td><td>419.74 31.48</td><td>891.21</td></tr><tr><td>2015-12-22</td><td colspan=\"2\">顶部</td><td colspan=\"2\">27.46 265.92</td><td>29.89</td></tr><tr><td>2016-01-28</td><td colspan=\"2\">底部</td><td></td><td>142.04 191.97</td><td>99.04</td></tr><tr><td>2017-11-13</td><td colspan=\"2\">顶部</td><td colspan=\"2\">222.65</td><td>254.29</td></tr><tr><td>2018-10-18</td><td colspan=\"2\">底部</td><td>568.83</td><td>1530.78 1465.46</td><td>2009.05</td></tr><tr><td>2019-04-10 2020-07-13</td><td colspan=\"2\">项部</td><td colspan=\"2\">662.43</td><td>1517.01</td></tr><tr><td>2021-03-18</td><td colspan=\"2\">项部 底部</td><td colspan=\"2\">8064.47 7902.07 2009.72</td><td>6790.54</td></tr><tr><td>2021-12-13</td><td colspan=\"2\">项部</td><td colspan=\"2\">2852.76 5341.16 3579.57</td><td>2849.22</td></tr><tr><td>2022-04-26</td><td colspan=\"2\"></td><td colspan=\"2\">2197.51 2264.42</td><td>3065.12</td></tr><tr><td>2022-07-04</td><td colspan=\"2\">底部</td><td colspan=\"2\">2340.62</td><td>3399.59</td></tr><tr><td>2022-10-31</td><td colspan=\"2\">项部 底部</td><td>616.g9</td><td>3688.16 6618.07</td><td>4627.22 5339.34</td></tr><tr><td>见在/见顶时间T</td><td colspan=\"3\">市场形态 T±5</td><td>T±10</td><td>T±15</td></tr><tr><td>2015-06-12</td><td colspan=\"2\">顶B</td><td>724.67</td><td>805.37</td><td>1950.11</td></tr><tr><td>2015-09-15</td><td colspan=\"2\">南EB</td><td>391.78</td><td>419.74</td><td>891.21</td></tr><tr><td>2015-12-22</td><td colspan=\"2\">顶都</td><td>27.46</td><td>31.48</td><td>29.89</td></tr><tr><td>2016-01-28</td><td colspan=\"2\">B</td><td>265.92</td><td>142.04</td><td>99.04</td></tr><tr><td>2017-11-13</td><td colspan=\"2\">顶邮</td><td>222.65</td><td>191.97</td><td>254.29</td></tr><tr><td>2018-10-18</td><td colspan=\"2\">底B</td><td>568.83</td><td>1530.78</td><td>2009.05</td></tr><tr><td>2019-04-10</td><td colspan=\"2\">顶B</td><td>662.43</td><td>1465.46</td><td>1517.01</td></tr><tr><td>2020-07-13</td><td colspan=\"2\">顶都</td><td>8064.47</td><td>7902.07</td><td>6790.54</td></tr><tr><td></td><td colspan=\"2\"></td><td></td><td>2852.76</td><td>2849.22</td></tr><tr><td>2021-03-18</td><td colspan=\"2\">B</td><td>2009.72</td><td></td><td></td></tr><tr><td>2021-12-13</td><td colspan=\"2\">顶部</td><td>5341.16</td><td>3579.57</td><td>3065.12</td></tr><tr><td>2022-04-26</td><td colspan=\"2\">底部</td><td>2197.51</td><td>2264.42</td><td>3399.59</td></tr><tr><td>2022-07-04</td><td colspan=\"2\">顶邵</td><td>2340.62</td><td>3688.16</td><td>4627.22</td></tr><tr><td>2022-10-31</td><td colspan=\"2\">B</td><td>6162.69</td><td>6618.07</td><td>5339.34</td></tr></table>",
        "bbox": [
            578,
            305,
            800,
            523
        ],
        "page_idx": 33
    },
    {
        "type": "image",
        "img_path": "images/4ad94d577d04181d59f95250b4401c77b8f7c3c887a1383ff967f437af400f57.jpg",
        "image_caption": [
            "Figure 11: The rendered outputs for various types of Tables. "
        ],
        "image_footnote": [],
        "bbox": [
            199,
            546,
            800,
            835
        ],
        "page_idx": 33
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 33
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            137,
            58,
            862,
            73
        ],
        "page_idx": 33
    },
    {
        "type": "image",
        "img_path": "images/55fb73638465df195949309a4736c1396b24b30ee16c4334ddb49c22b3e8f2a5.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            176,
            107,
            823,
            832
        ],
        "page_idx": 34
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 34
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            140,
            58,
            864,
            74
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "A.1.3 Among Formula types ",
        "text_level": 1,
        "bbox": [
            137,
            107,
            359,
            123
        ],
        "page_idx": 35
    },
    {
        "type": "image",
        "img_path": "images/29a8c8c7f1cb9211d3f7756ced1fe087fcf1678dd4c0fa154fbbf4e2c6f8f7f0.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            196,
            140,
            495,
            309
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "Handwritten Formula ",
        "text_level": 1,
        "bbox": [
            225,
            323,
            382,
            338
        ],
        "page_idx": 35
    },
    {
        "type": "image",
        "img_path": "images/6516e3b151571840a2cc9e62b133c6dfcbd271f24ee92c84673f3cacb8b2e971.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            197,
            342,
            493,
            551
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "Blurred Formula",
        "text_level": 1,
        "bbox": [
            248,
            570,
            367,
            584
        ],
        "page_idx": 35
    },
    {
        "type": "image",
        "img_path": "images/32568d42a04a5ad04ee5cd86802f4dee85c10b5606da169a0fc6c2e0e909ea40.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            511,
            142,
            794,
            407
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "Matrices ",
        "text_level": 1,
        "bbox": [
            584,
            428,
            651,
            441
        ],
        "page_idx": 35
    },
    {
        "type": "image",
        "img_path": "images/b3c53df6e2917743491187674c68263128854150aacece7ef098f59e36527697.jpg",
        "image_caption": [
            "Figure 13: The rendered outputs for various types of Formulas. "
        ],
        "image_footnote": [],
        "bbox": [
            200,
            590,
            488,
            833
        ],
        "page_idx": 35
    },
    {
        "type": "image",
        "img_path": "images/b46854529cbc7ccd2a04428b557bbd04b7eec5d1f4189aab2963935aaf8b555e.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            513,
            448,
            794,
            832
        ],
        "page_idx": 35
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 35
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            137,
            58,
            864,
            74
        ],
        "page_idx": 35
    },
    {
        "type": "image",
        "img_path": "images/8d40ffa5c6736714d0b8e3fd7579a6f88518d9dd4864c132fd5ec1c2f0b0e2ca.jpg",
        "image_caption": [
            "Figure 14: The rendered outputs for various types of Formulas. "
        ],
        "image_footnote": [],
        "bbox": [
            137,
            18,
            864,
            815
        ],
        "page_idx": 36
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "A.2 Compare to Previous Versions ",
        "text_level": 1,
        "bbox": [
            137,
            106,
            457,
            123
        ],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "A.2.1 Table ",
        "bbox": [
            137,
            131,
            233,
            146
        ],
        "page_idx": 37
    },
    {
        "type": "image",
        "img_path": "images/ba711541e2a3d2f919c4d263f26692b2fd113d42135442642a73c9b2a20b83ed.jpg",
        "image_caption": [
            "Figure 15: Compare with Previous Version, MinerU2.5 performs better in rotated tables. "
        ],
        "image_footnote": [],
        "bbox": [
            184,
            131,
            818,
            875
        ],
        "page_idx": 37
    },
    {
        "type": "discarded",
        "text": "38 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 37
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            138,
            58,
            864,
            73
        ],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "Image ",
        "text_level": 1,
        "bbox": [
            310,
            113,
            367,
            131
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "MinerU2.5 ",
        "text_level": 1,
        "bbox": [
            619,
            114,
            715,
            130
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            240,
            156,
            462,
            170
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            233,
            185,
            464,
            191
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "拓宽视野",
        "text_level": 1,
        "bbox": [
            318,
            210,
            357,
            217
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            227,
            228,
            281,
            234
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            214,
            234,
            462,
            270
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "以在春天种树、种花。夏天很热。我可以在河里游泳。秋天是个不错的季节。我可以在秋天放风的季节，因为它们都很美。",
        "bbox": [
            215,
            270,
            460,
            297
        ],
        "page_idx": 38
    },
    {
        "type": "image",
        "img_path": "images/bd9976a464d501396ee018d20d9a6d824a6bb9bae2e2020e9c9ebad13a365f98.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            302,
            300,
            313,
            309
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "家长帮一帮",
        "text_level": 1,
        "bbox": [
            318,
            301,
            357,
            309
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "请按照下表所列内容与具体要求，复习本单元重点知识，不积跬步，无以至千里，小朋友加",
        "bbox": [
            223,
            310,
            462,
            324
        ],
        "page_idx": 38
    },
    {
        "type": "table",
        "img_path": "images/e6ff410f3f969cbdf306f77cfd8607b6d486642b622cc9ff69fabf167eba0a6a.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=1 colspan=1>项目与要求</td><td rowspan=1 colspan=1>内容</td><td rowspan=1 colspan=1>复习方式</td></tr><tr><td rowspan=1 colspan=1>三会（听说读）</td><td rowspan=1 colspan=1>autumn(秋天），winter(冬天），coo(凉爽的）.cold(寒冷的）</td><td rowspan=1 colspan=1>★大声期读★热读熟记</td></tr><tr><td rowspan=1 colspan=1>二会（听说）</td><td rowspan=1 colspan=1>sunny(阳光充足的），windy（有风的），cloudy（多云的），skate(溜冰),ice(冰)，outside(在外面），snowmen(雪人）</td><td rowspan=1 colspan=1>★听懂会说</td></tr><tr><td rowspan=1 colspan=1>句子（听说读）</td><td rowspan=1 colspan=1>I like(doing)..,i/o..Do youlike..,?Yes,Ido./NoIdon&#x27;t,</td><td rowspan=1 colspan=1>★大声读★热读热记</td></tr><tr><td rowspan=1 colspan=1>□语（听说）</td><td rowspan=1 colspan=1>In...（季节），I/Welike(doing)..</td><td rowspan=1 colspan=1>★指读★对话</td></tr><tr><td rowspan=1 colspan=1>语音（听读）</td><td rowspan=1 colspan=1>字母s,z的发音（见教材P29)</td><td rowspan=1 colspan=1>★大声朗读★热读热记</td></tr></table>",
        "bbox": [
            215,
            325,
            462,
            419
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "油哦！",
        "bbox": [
            215,
            319,
            232,
            324
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "关于四季有许多英语的美文。",
        "bbox": [
            227,
            219,
            305,
            227
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "教材全解二年级英语（第二学期）",
        "text_level": 1,
        "bbox": [
            531,
            142,
            614,
            148
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "begins to grow.We can fly kites in the park on windy days.And we can ride bicycles with our friends on sunny days. ",
        "bbox": [
            531,
            151,
            797,
            162
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Kitty:Yes,spring is beautiful ",
        "bbox": [
            531,
            166,
            604,
            172
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Alice: What about you? What’s your favourite season？ ",
        "bbox": [
            532,
            175,
            669,
            181
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Kity:Myfavourite seasonis winter.Anditsalsoanice season.lt often snowsin winter，and we cango skatingrmakeasnown. ",
        "bbox": [
            531,
            184,
            794,
            195
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Alice:Yeah,every seasonis beautiful.Ilove four seasons. ",
        "bbox": [
            531,
            198,
            679,
            204
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "拓宽视野",
        "text_level": 1,
        "bbox": [
            531,
            217,
            555,
            223
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "关于四季有许多英语的美文。",
        "bbox": [
            531,
            227,
            606,
            233
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "来读一读，学一学吧！",
        "bbox": [
            531,
            237,
            586,
            243
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Therearefourseasonsinayear.Theyare spring,summer,autumnand winter.like spring.Itis warm.lt oftenrains.Icanplant treesandflowers inspring.Summer is very hot.I can swimin theriver.Autumnisa good season.1canflykites inautumn.Icaneat many apples,too.Winter is cold.It oftensnows.Iwish1 canmake a snowman inwinter.I love all the seasons because they are beautiful. ",
        "bbox": [
            531,
            246,
            802,
            267
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "一年中有四个季节，它们是春，夏、秋、冬。我喜欢春天。（春天）天气暖和，经常下雨。我可以在春天种树。种花，夏天很热。我可以在河里游泳，秋天是个不错的季节，我可以在秋天放风筝。我也可以吃很多草果。冬天很冷。天经常下雪，我希望我可以在冬天堆雪人。我真欢所有的季节，因为它们都很美。",
        "bbox": [
            531,
            270,
            799,
            289
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "家长帮一帮",
        "text_level": 1,
        "bbox": [
            531,
            301,
            560,
            308
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "请按照下表所列内容与具体要求，复习本单元重点知识，不积陡步，无以至千里，小朋友加油哦",
        "bbox": [
            531,
            310,
            767,
            318
        ],
        "page_idx": 38
    },
    {
        "type": "table",
        "img_path": "images/73e7b83d7ead16cd0a4ab4396bd43c1d97ce8d8016eade1d9d54671ab647b9de.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=1 colspan=2>项目与要求</td><td rowspan=1 colspan=1>内容</td><td rowspan=1 colspan=1>复习方式</td></tr><tr><td rowspan=2 colspan=1>记</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>autumn（秋天),winter（冬天），cool(凉爽的），cold（寒冷的）</td><td rowspan=1 colspan=1>★大声朗读★熟读热记</td></tr><tr><td rowspan=1 colspan=1>二</td><td rowspan=1 colspan=1>sunny（阳光充足的），windy（有风的），cloudy（多云的），skate（溜冰),ice(冰），outside(在外面)，snowmen（雪人)</td><td rowspan=1 colspan=1>★听懂会说</td></tr><tr><td rowspan=1 colspan=2>句子（听说读）</td><td rowspan=1 colspan=1>1like(doing).in/on..Do youlie..?Yes,Ido./No,Idon’t.</td><td rowspan=1 colspan=1>★大声朗读★熟读热记</td></tr><tr><td rowspan=1 colspan=2>□语（听说）</td><td rowspan=1 colspan=1>In..(季节）,/Weliedoing</td><td rowspan=1 colspan=1>★指读★对话</td></tr><tr><td rowspan=1 colspan=2>语音（听读）</td><td rowspan=1 colspan=1>字母s，z的发音（见教材P29）</td><td rowspan=1 colspan=1>★大声朗读★熟读热记</td></tr></table>",
        "bbox": [
            531,
            320,
            803,
            410
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "62 ",
        "bbox": [
            531,
            412,
            539,
            419
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "MinerU2-pipeline ",
        "text_level": 1,
        "bbox": [
            259,
            464,
            411,
            482
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "MinerU2-VLM ",
        "text_level": 1,
        "bbox": [
            602,
            464,
            728,
            481
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "beginstogrow.Wecanfly kitesin thepark on windydays.And wecanride bicycleswith our friends on sunny days. ",
        "bbox": [
            196,
            503,
            465,
            515
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Kitty:Yes,spring is beautiful ",
        "bbox": [
            199,
            517,
            269,
            523
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Alice:What about you?What'syour favourite season? ",
        "bbox": [
            196,
            525,
            333,
            534
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Kity:My favouriteseason is winter.Anditsalsoa nice season.lt oftensnowsin winterand we can go skating ormake asnowman. ",
        "bbox": [
            196,
            536,
            450,
            547
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Alice:Yeah,every season is beautiful.llove four seasons. ",
        "bbox": [
            199,
            550,
            343,
            556
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "拓宽视野",
        "text_level": 1,
        "bbox": [
            199,
            569,
            245,
            579
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "关于四季有许多英语的美文。",
        "bbox": [
            199,
            587,
            271,
            593
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "来读一读，学一学吧！",
        "bbox": [
            199,
            597,
            253,
            603
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Thereare four seasonsina year.They are spring,summerautumnand winter.llike spring.ltis warm.lt oftenrains.lcanplant treesandflowersinspring.Summeris very hot.Ican swimintheriver.Autumnisagood season.Ican flykitesinautumn.lcaneatmany apples,too.Winteriscold.lt oftensnows.l wish Ican makeasnowman inwinter.llove allthe seasons because they are beautiful ",
        "bbox": [
            199,
            607,
            464,
            627
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "一年中有四个季节。它们是春，夏、秋、冬。我喜欢春天。（春天）天气暖和，经常下雨。我可以在春天种树、种花，夏天很热。我可以在河里游泳，秋天是个不错的季节，我可以在秋天放风筝。我也可以吃很多苹果。冬天很冷。天经常下雪。我希望我可以在冬天堆雪人。我喜欢所有的季节，因为它们都很美。",
        "bbox": [
            199,
            631,
            467,
            650
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "家长帮一帮",
        "text_level": 1,
        "bbox": [
            199,
            654,
            256,
            664
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Table Structure Error ",
        "text_level": 1,
        "bbox": [
            326,
            657,
            464,
            667
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "请按照下表所列内容与具体要求，复习本单元重点知识，不积跬步，无以至千里，小朋友加油哦！",
        "bbox": [
            197,
            671,
            437,
            678
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Table Structure Error ",
        "text_level": 1,
        "bbox": [
            656,
            489,
            794,
            501
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "begins to grow. We can fly kites in the park on windy days. And we can ride bicycles with our friends on sunny days. ",
        "bbox": [
            526,
            510,
            790,
            521
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Kitty: Yes, spring is beautiful. ",
        "bbox": [
            526,
            523,
            601,
            530
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Alice:What about you?What's your favourite season？ ",
        "bbox": [
            524,
            532,
            666,
            539
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "KityMy favourite seasonis winter.Anditsalsoa nice seasonlt often snows in winterand we cango skating or make asnowman ",
        "bbox": [
            524,
            542,
            787,
            553
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Alice:Yeah,every season is beautiful.Ilove four seasons. ",
        "bbox": [
            526,
            556,
            673,
            561
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "拓宽视野",
        "text_level": 1,
        "bbox": [
            526,
            565,
            573,
            575
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "关于四季有许多英语的美文。",
        "text_level": 1,
        "bbox": [
            524,
            583,
            602,
            589
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "来读一读，学一学吧！",
        "bbox": [
            526,
            593,
            576,
            599
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "There arefour seasonsina year.They are spring,summer,autumn and winter.Ilike spring.Itis warm.It oftenrains,Icanplanttreesandflowersinspring,Summerisvery hot,Icanswimintheriver.Autumnisa good season.Icanfly kitesinautumn.Icaneat many apples,too.Winteris cold.It often snows.1wish1 canmakea snowman inwinter.I love allthe seasonsbecause they are beautiful ",
        "bbox": [
            526,
            603,
            795,
            623
        ],
        "page_idx": 38
    },
    {
        "type": "image",
        "img_path": "images/2ccf536b2680747712bfc2290917624e59225d3b121f355911851aedd99bc982.jpg",
        "image_caption": [
            "Figure 16: Compare with Previous Version, MinerU2.5 performs better in tables with merged cells. "
        ],
        "image_footnote": [],
        "bbox": [
            191,
            679,
            480,
            800
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "一年中有四个季节。它们是春、夏、秋、冬。我喜欢春天。（春天)天气暖和，经常下雨。我可以在春天种树、种花。夏天很热。我可以在河里游泳。秋天是个不错的季节。我可以在秋天放风筝。我也可以吃很多苹果。冬天很冷，天经常下雪，我希望我可以在冬天堆雪人。我喜欢所有的季节，因为它们都很美。",
        "bbox": [
            526,
            626,
            792,
            645
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "家长帮一帮",
        "text_level": 1,
        "bbox": [
            526,
            648,
            583,
            659
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "请按照下表所到内容与具体要求复习本单元重点知识不积鞋步无以至千里小朋友加油哦！",
        "bbox": [
            527,
            666,
            746,
            672
        ],
        "page_idx": 38
    },
    {
        "type": "image",
        "img_path": "images/1d35ca26a817a5146c999c22ca6159e6600fa3df0e4ba465e8f6052ae3ab98de.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            519,
            676,
            802,
            801
        ],
        "page_idx": 38
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            930
        ],
        "page_idx": 38
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            138,
            58,
            864,
            73
        ],
        "page_idx": 38
    },
    {
        "type": "discarded",
        "text": "关注信公众号“教辅资种站”敬取更多学习资种",
        "bbox": [
            263,
            443,
            408,
            449
        ],
        "page_idx": 38
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            214,
            428,
            230,
            434
        ],
        "page_idx": 38
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            303,
            208,
            312,
            218
        ],
        "page_idx": 38
    },
    {
        "type": "discarded",
        "text": "关注微信公众号教辅资料站获取更多学习资料",
        "bbox": [
            529,
            422,
            648,
            429
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "A.2.2 Formula ",
        "text_level": 1,
        "bbox": [
            137,
            107,
            254,
            122
        ],
        "page_idx": 39
    },
    {
        "type": "image",
        "img_path": "images/83398281e2606e0f83974ac54b5388f225e0e662008fc20b36297323f397b58c.jpg",
        "image_caption": [
            "Figure 17: Compare with Previous Version, MinerU2.5 performs better in Formula with Chinese. "
        ],
        "image_footnote": [],
        "bbox": [
            179,
            113,
            818,
            848
        ],
        "page_idx": 39
    },
    {
        "type": "discarded",
        "text": "40 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 39
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            137,
            58,
            864,
            73
        ],
        "page_idx": 39
    },
    {
        "type": "image",
        "img_path": "images/c5359adf81d6872557fc5b47fd18eed2b505bd51a14b66fa7b651d4526d7e48d.jpg",
        "image_caption": [
            "Figure 18: Compare with Previous Version, MinerU2.5 performs better in multi-lines and complex Formula. "
        ],
        "image_footnote": [],
        "bbox": [
            179,
            95,
            821,
            814
        ],
        "page_idx": 40
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            138,
            58,
            864,
            74
        ],
        "page_idx": 40
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            506,
            931
        ],
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "A.2.3 Layout&OCR ",
        "text_level": 1,
        "bbox": [
            138,
            107,
            294,
            122
        ],
        "page_idx": 41
    },
    {
        "type": "image",
        "img_path": "images/91c5e353d589294d5b5fe74ac8d94564e6d557ec64ea53a32fa83ec8a50733ad.jpg",
        "image_caption": [
            "Figure 19: Compare with Previous Version, MinerU2.5 achieve finer layout detection. "
        ],
        "image_footnote": [],
        "bbox": [
            184,
            128,
            815,
            847
        ],
        "page_idx": 41
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            930
        ],
        "page_idx": 41
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            135,
            58,
            862,
            73
        ],
        "page_idx": 41
    },
    {
        "type": "image",
        "img_path": "images/614b8bf6c24994e8ccfbe0378cd37c366962a328696f7e9ddd1eced4eaa82020.jpg",
        "image_caption": [
            "page. "
        ],
        "image_footnote": [],
        "bbox": [
            183,
            79,
            818,
            829
        ],
        "page_idx": 42
    },
    {
        "type": "discarded",
        "text": "43 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 42
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            137,
            58,
            864,
            73
        ],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "A.3 Compare with Others ",
        "text_level": 1,
        "bbox": [
            138,
            104,
            382,
            123
        ],
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "A.3.1 Table ",
        "bbox": [
            137,
            131,
            233,
            146
        ],
        "page_idx": 43
    },
    {
        "type": "image",
        "img_path": "images/3ea48e0eaeb3346c77025fbef5c9d15a90600699c5813f6d1cbcd35dc7c8245e.jpg",
        "image_caption": [
            "Figure 21: Compare with others in Full-page table. "
        ],
        "image_footnote": [],
        "bbox": [
            183,
            141,
            815,
            871
        ],
        "page_idx": 43
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            930
        ],
        "page_idx": 43
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            137,
            58,
            864,
            74
        ],
        "page_idx": 43
    },
    {
        "type": "image",
        "img_path": "images/9fe1a33a4a7f7ac7a69f8c4a4d0072ff93723e496e56d625ade2ec3ccff5cca6.jpg",
        "image_caption": [
            "Figure 22: Compare with others in content dense table. "
        ],
        "image_footnote": [],
        "bbox": [
            184,
            93,
            816,
            811
        ],
        "page_idx": 44
    },
    {
        "type": "discarded",
        "text": "45 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 44
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            138,
            58,
            864,
            73
        ],
        "page_idx": 44
    },
    {
        "type": "image",
        "img_path": "images/a721d1ebeeaaa2f8b48734d24b97ba257187310ea5c930b929567b006b8de292.jpg",
        "image_caption": [
            "Figure 23: Compare with others in Colored table with many empty cells. "
        ],
        "image_footnote": [],
        "bbox": [
            181,
            82,
            816,
            814
        ],
        "page_idx": 45
    },
    {
        "type": "discarded",
        "text": "46 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 45
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            138,
            58,
            864,
            74
        ],
        "page_idx": 45
    },
    {
        "type": "image",
        "img_path": "images/57522ce9218de99190eec4ca8c23dd27c0bd791a5803719642ae35ec3c329745.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            181,
            84,
            821,
            832
        ],
        "page_idx": 46
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            930
        ],
        "page_idx": 46
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            138,
            58,
            862,
            73
        ],
        "page_idx": 46
    },
    {
        "type": "image",
        "img_path": "images/84e67554fed9a10d82373ac3bfabc858b1b8db70b8a810c7d2122c4acd327392.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            181,
            109,
            483,
            438
        ],
        "page_idx": 47
    },
    {
        "type": "image",
        "img_path": "images/4221a1875c9e086996d9c2749ac9b89a415b34b4bca1c9cc7024571f10307bc0.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            524,
            111,
            802,
            412
        ],
        "page_idx": 47
    },
    {
        "type": "image",
        "img_path": "images/c2629c0f4c54598421962831f478ada6a7a509cc57cbb49d58de6f73410eb8d1.jpg",
        "image_caption": [
            "Figure 25: Compare with others in Table with irregular merged cells. "
        ],
        "image_footnote": [],
        "bbox": [
            186,
            460,
            486,
            800
        ],
        "page_idx": 47
    },
    {
        "type": "image",
        "img_path": "images/4c0bc73ae89a675b8bfeecbb5a49f3b57aaaf0cfafc50a078e220fd130309500.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            514,
            460,
            813,
            800
        ],
        "page_idx": 47
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            930
        ],
        "page_idx": 47
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            138,
            58,
            864,
            74
        ],
        "page_idx": 47
    },
    {
        "type": "image",
        "img_path": "images/d1542d90b51e6ed59dada15986403a028bb07203aebae83e63e3722dd6024ddb.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            140,
            46,
            864,
            834
        ],
        "page_idx": 48
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            930
        ],
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "A.3.2 Formula ",
        "text_level": 1,
        "bbox": [
            137,
            107,
            254,
            122
        ],
        "page_idx": 49
    },
    {
        "type": "image",
        "img_path": "images/064cc0dc2327d0254b0617587bb958826494bb0a3af84e0a31993d818c5f1823.jpg",
        "image_caption": [
            "Figure 27: Compare with others in Nested conditional expressions. "
        ],
        "image_footnote": [],
        "bbox": [
            181,
            108,
            821,
            849
        ],
        "page_idx": 49
    },
    {
        "type": "discarded",
        "text": "50 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 49
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            137,
            58,
            864,
            73
        ],
        "page_idx": 49
    },
    {
        "type": "image",
        "img_path": "images/5e82bab073276ce50874acd57225fb244f39e9843cf1ff19b39e7f2f4a98d8ca.jpg",
        "image_caption": [
            "Figure 28: Compare with others in Complex matrix. "
        ],
        "image_footnote": [],
        "bbox": [
            179,
            98,
            818,
            804
        ],
        "page_idx": 50
    },
    {
        "type": "discarded",
        "text": "51 ",
        "bbox": [
            488,
            920,
            506,
            931
        ],
        "page_idx": 50
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            140,
            58,
            864,
            74
        ],
        "page_idx": 50
    },
    {
        "type": "image",
        "img_path": "images/a44a00240bf0e250afc746bd2948f131a2ed6a1981957b734ce2d0acc5a21d97.jpg",
        "image_caption": [
            "Figure 29: Compare with others in Nested matrix. "
        ],
        "image_footnote": [],
        "bbox": [
            178,
            90,
            821,
            825
        ],
        "page_idx": 51
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 51
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            138,
            58,
            864,
            74
        ],
        "page_idx": 51
    },
    {
        "type": "text",
        "text": "A.3.3 Layout&OCR ",
        "text_level": 1,
        "bbox": [
            137,
            107,
            294,
            122
        ],
        "page_idx": 52
    },
    {
        "type": "image",
        "img_path": "images/133cfd0821a9be331968a69cdb6f2f1888161b14a493b8ca0bdf3629f6a2f4f5.jpg",
        "image_caption": [
            "Figure 30: Compare with others in Academic literature with alternating text and images. "
        ],
        "image_footnote": [],
        "bbox": [
            187,
            132,
            813,
            854
        ],
        "page_idx": 52
    },
    {
        "type": "discarded",
        "text": "53 ",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 52
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            137,
            58,
            864,
            73
        ],
        "page_idx": 52
    },
    {
        "type": "image",
        "img_path": "images/2d0c7ffb96561ae89c68115f4eca87fb2ab84797970d96ee8a27052c33f2fad4.jpg",
        "image_caption": [
            "Figure 31: Compare with others in Financial Report with Few Frame Tables. "
        ],
        "image_footnote": [],
        "bbox": [
            187,
            99,
            816,
            810
        ],
        "page_idx": 53
    },
    {
        "type": "discarded",
        "text": "54 ",
        "bbox": [
            488,
            920,
            508,
            931
        ],
        "page_idx": 53
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing少线表检测和拆分粒度比较好 ",
        "bbox": [
            140,
            58,
            864,
            74
        ],
        "page_idx": 53
    },
    {
        "type": "image",
        "img_path": "images/644313f2b29f3ac85e35921833abf159b02b19970c128d5029dfce527a673c9d.jpg",
        "image_caption": [
            "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing水印干扰小 ",
            "Figure 32: Compare with others in Textbooks with watermarks. "
        ],
        "image_footnote": [],
        "bbox": [
            189,
            88,
            813,
            821
        ],
        "page_idx": 54
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "B Prompt Details ",
        "text_level": 1,
        "bbox": [
            137,
            103,
            341,
            125
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Here, we provide a detailed description of the different prompts used during the two-stage inference of MinerU2.5, along with their corresponding output formats. ",
        "bbox": [
            133,
            136,
            861,
            167
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "B.1 Layout Detection ",
        "text_level": 1,
        "bbox": [
            137,
            186,
            339,
            204
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "The layout detection output will include the relative coordinates, category, and rotation direction of each element in the document. Each element will be output in sequence, ensuring traceability for all layout data. The input image will be resized to a resolution of $1 0 3 6 \\times 1 0 3 6$ . ",
        "bbox": [
            137,
            212,
            862,
            257
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Output format: ",
        "text_level": 1,
        "bbox": [
            137,
            265,
            251,
            280
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "• Box Coordinates: x1, y1, x2, y2 • Document Element Category: title, text, image, etc. • Rotation Direction: up, down, left, right ",
        "bbox": [
            160,
            287,
            550,
            349
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Example: ",
        "text_level": 1,
        "bbox": [
            137,
            356,
            207,
            371
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "<| box_start $1 > 1 0 0$ 200 300 $4 0 0 < 1$ box_end $> < |$ ref_start $>$ title <| ref_end $| > < |$ rotate_up | > <| box_start $1 > 4 0 0$ 500 600 $7 0 0 < 1$ box_end $| > < |$ ref_start | > text <| ref_end $| > < |$ rotate_up | > ",
        "bbox": [
            135,
            377,
            769,
            402
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "B.2 Text Recognition ",
        "text_level": 1,
        "bbox": [
            137,
            429,
            336,
            446
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "The output will contain the recognized text results. The input image will retain its native resolution; however, the number of image tokens will be limited to the range of 4 to 2048. If this limit is exceeded, the image will be scaled accordingly. ",
        "bbox": [
            137,
            455,
            864,
            502
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Output format: ",
        "text_level": 1,
        "bbox": [
            137,
            508,
            251,
            523
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "• OCR Results: The raw OCR output ",
        "bbox": [
            161,
            530,
            437,
            546
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Example: ",
        "text_level": 1,
        "bbox": [
            137,
            554,
            207,
            569
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "The results of the analyses of the uncertainty of the field data and related assumptions are shown in Figs 13 and 14. ",
        "bbox": [
            137,
            575,
            861,
            599
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "B.3 Formula Recognition ",
        "text_level": 1,
        "bbox": [
            137,
            627,
            375,
            645
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Any formulas found in the image will be extracted and converted into LaTeX format. The input image will retain its native resolution; however, the number of image tokens will be limited to the range of 4 to 2048. If this limit is exceeded, the image will be scaled accordingly. ",
        "bbox": [
            137,
            652,
            862,
            699
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Output format: ",
        "text_level": 1,
        "bbox": [
            137,
            705,
            251,
            722
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "• LaTeX Format: The LaTeX representation of the formula ",
        "bbox": [
            160,
            728,
            581,
            744
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Example: ",
        "text_level": 1,
        "bbox": [
            137,
            752,
            207,
            767
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "\\ [   \n\\ hat { F } $=$ \\ operatorname { Concat }\\ left (\\ left [ F_ {1} , F_ {2} , \\ dots , F_ { n } \\ right ]\\ right ) \\ tag {2}   \n\\ ]   \n\\ [   \n$\\mathsf { ~ M ~ } =$ \\ sigma \\ bigl ( \\ mathrm { GELU }(\\ mathrm { BN }(\\ mathrm { Conv } _ { gate }(\\ hat { F }) ) ) \\ bigr ) \\ tag {3}   \n\\ ] ",
        "bbox": [
            135,
            773,
            848,
            871
        ],
        "page_idx": 55
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            490,
            920,
            508,
            931
        ],
        "page_idx": 55
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            137,
            58,
            864,
            74
        ],
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "B.4 Table Recognition ",
        "text_level": 1,
        "bbox": [
            137,
            104,
            349,
            123
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "The output will include the recognized tables, structured in an OTSL (Open Table Structure Language) format for easy data processing. The input image will retain its native resolution; however, the number of image tokens will be limited to the range of 4 to 2048. If this limit is exceeded, the image will be scaled accordingly. ",
        "bbox": [
            137,
            131,
            862,
            193
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "Output format: ",
        "text_level": 1,
        "bbox": [
            137,
            199,
            251,
            214
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "• OTSL Format: The table represented in OTSL format ",
        "bbox": [
            160,
            222,
            562,
            237
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "Example: ",
        "text_level": 1,
        "bbox": [
            137,
            244,
            207,
            260
        ],
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "< fcel $>$ Site < fcel > Cl < fcel > NO3 < fcel > SO4 < fcel > Na < fcel > Ca < fcel >K < fcel > Mg < fcel $>$ NH4 < fcel $>$ References < nl >   \n< fcel ${ > } \\mathsf { C } \\mathsf { 1 }$ dominance sites $<$ lcel $>$ < lcel > < lcel > < lcel > < lcel > < lcel > < lcel > < lcel > < lcel > < nl >   \n< fcel $>$ Comba < fcel >109.8 < fcel >12.1 < fcel >23.3 < fcel >86.8 < fcel >43.4 < fcel >4.8 < fcel >15.1 < fcel >13.2 < fcel > Present study < nl >   \n< fcel $>$ Alibagh < fcel >236 < fcel >9 < fcel >36 < fcel >220 < fcel >46 < fcel >5 < fcel >64 < fcel >8 < fcel $>$ Naik et al (2002) <nl >   \n< fcel > Goa < fcel >113.4 < fcel >5.5 < fcel >27.4 < fcel >97.2 < fcel >41.5 < fcel >2.5 < fcel >24.5 < fcel >5.5 < fcel $>$ Parashar et al . (2001) <nl >   \n< fcel $>$ Bombay $<$ fcel >138 < fcel > - < fcel >10 < fcel >115 < fcel >36 < fcel >3.6 < fcel >24 < fcel > - < fcel $>$ Sequeira (1976) <nl >   \n< fcel $>$ Na dominance sites < lcel > < lcel > < lcel > < lcel > < lcel > < lcel > < lcel > < lcel > < lcel > < nl >   \n< fcel $>$ Colaba < fcel >171 < fcel >34 < fcel >52 < fcel >179 < fcel >133 < fcel >6 < fcel >59 < fcel >12 < fcel > Naik et al . $( 2 0 0 2 ) < \\mathsf { n } 1 >$   \n< fcel $>$ Silent Valley < fcel >43.0 < fcel ${ > } 2 1$ .0 < fcel >20.0 < fcel >46.0 < fcel >43.0 < fcel >4.0 < fcel >14.0 < fcel ${ > } 3$ .0 < fcel > Rao et al . (1995) <nl >   \n< fcel $>$ Chembur < fcel >164.5 < fcel >29.5 < fcel >70.4 < fcel >168.2 < fcel >89.5 < fcel >6.9 < fcel >36.5 < fcel ${ > } 4 1$ .1 < fcel $>$ Khemani et al . (1994) <nl >   \n< fcel $>$ Bhubaneswar < fcel >18 < fcel >10 < fcel >19.1 < fcel >15 < fcel >20.2 < fcel >1.8 < fcel >5.2 < fcel >18.7 < fcel $>$ Das et al . $( 2 0 0 5 ) < \\mathsf { n } 1 >$ ",
        "bbox": [
            137,
            265,
            857,
            506
        ],
        "page_idx": 56
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            488,
            920,
            508,
            930
        ],
        "page_idx": 56
    },
    {
        "type": "discarded",
        "text": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing ",
        "bbox": [
            138,
            58,
            862,
            73
        ],
        "page_idx": 56
    }
]