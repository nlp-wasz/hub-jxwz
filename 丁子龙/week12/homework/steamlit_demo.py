import streamlit as st
import asyncio
from agents.mcp.server import MCPServerSse
import asyncio
import inspect
from agents import (Agent, Runner, AsyncOpenAI, FunctionTool,
                    OpenAIChatCompletionsModel,RunContextWrapper,Tool,AgentHooks,
                    SQLiteSession,StopAtTools,TContext)
from openai.types.responses import ResponseTextDeltaEvent
from agents.mcp import MCPServer
from agents import set_default_openai_api, set_tracing_disabled
from typing import Optional,Callable,List
set_default_openai_api("chat_completions")
set_tracing_disabled(True)

st.set_page_config(page_title="ä¼ä¸šèŒèƒ½æœºå™¨äºº")
session = SQLiteSession("conversation_123")

class MyAgentHooks(AgentHooks):
    async def on_tool_start(self, context, agent, tool):
        # âœ… è·å–å·¥å…·å
        tool_name = getattr(tool, 'name', 'unknown')
        print(f"â–¶ï¸ Agent å³å°†è°ƒç”¨å·¥å…·: {tool_name}")


    async def on_tool_end(self, context, agent, tool, result):
        tool_name = getattr(tool, 'name', 'unknown')
        print(f"âœ… å·¥å…·æ‰§è¡Œå®Œæˆ: {tool_name} â†’ {result[:100]}...")




class FilteredAgent(Agent[TContext]):
    def __init__(
            self,
            *args,
            allowed_mcp_tool_names: Optional[List[str]] = None,  # âœ… æ¨èï¼šæ˜ç¡®çš„ç™½åå•
            **kwargs,
    ):
        super().__init__(*args, **kwargs)
        self._allowed_mcp_tool_names = set(allowed_mcp_tool_names) if allowed_mcp_tool_names else None

    async def get_all_tools(self, run_context: RunContextWrapper[TContext]) -> List[Tool]:
        # è·å–æœ¬åœ°å·²å¯ç”¨çš„å·¥å…·ï¼ˆä¸å˜ï¼‰
        local_enabled = await self._get_enabled_local_tools(run_context)

        # è·å–åŸå§‹ MCP å·¥å…·
        mcp_tools = await self.get_mcp_tools(run_context)

        # âœ… æŒ‰ç™½åå•è¿‡æ»¤
        if self._allowed_mcp_tool_names is not None:
            mcp_tools = [
                tool for tool in mcp_tools
                if getattr(tool, 'name', None) in self._allowed_mcp_tool_names
            ]
            print("Filtered MCP tools:", [tool.name for tool in mcp_tools])

        return mcp_tools + local_enabled

    async def _get_enabled_local_tools(self, run_context: RunContextWrapper[TContext]) -> List[Tool]:
        # å¤ç”¨çˆ¶ç±»ä¸­çš„å¯ç”¨é€»è¾‘ï¼ˆå¦‚ä½ å‰é¢æºç æ‰€ç¤ºï¼‰
        async def _check_tool_enabled(tool: Tool) -> bool:
            if not isinstance(tool, FunctionTool):
                return True
            attr = tool.is_enabled
            if isinstance(attr, bool):
                return attr
            res = attr(run_context, self)
            if inspect.isawaitable(res):
                return bool(await res)
            return bool(res)

        results = await asyncio.gather(*(_check_tool_enabled(t) for t in self.tools))
        return [t for t, ok in zip(self.tools, results) if ok]

# ğŸ” å°è£…å¼‚æ­¥è°ƒç”¨ï¼ˆå…³é”®ï¼ï¼‰
async def _fetch_tools():
    from fastmcp import Client
    # æ³¨æ„ï¼šSSE ç«¯ç‚¹åº”ä¸º /mcp/sseï¼ˆæ ‡å‡† FastMCP è·¯å¾„ï¼‰
    # è‹¥ä½ æœåŠ¡æŒ‚è½½åœ¨ /sseï¼Œè¯·ç¡®ä¿ä¸åç«¯ä¸€è‡´
    async with Client("http://localhost:8900/sse") as client:
        tools = await client.list_tools()
        return [tool.name for tool in tools]
tool_names = []
with st.sidebar:
    st.title('èŒèƒ½AI+æ™ºèƒ½é—®ç­”')
    if 'API_TOKEN' in st.session_state and len(st.session_state['API_TOKEN']) > 1:
        st.success('API Tokenå·²ç»é…ç½®', icon='âœ…')
        key = st.session_state['API_TOKEN']
    else:
        key = ""

    key = st.text_input('è¾“å…¥Token:', type='password', value=key)

    st.session_state['API_TOKEN'] = key
    model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["qwen-flash", "qwen-max"])
    use_tool = st.checkbox("ä½¿ç”¨å·¥å…·")
    if use_tool:
        tool_names = asyncio.run(_fetch_tools())
        selected_list = st.multiselect(
            "é€‰æ‹©è¦ä½¿ç”¨çš„å·¥å…·ï¼Œå¯å¤šé€‰ ğŸ‘‡",
            options=tool_names,
            default=None
        )


# åˆå§‹åŒ–çš„å¯¹è¯
if "messages" not in st.session_state.keys():
    st.session_state.messages = [
        {"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä¼ä¸šèŒèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥AIå¯¹è¯ ä¹Ÿ å¯ä»¥è°ƒç”¨å†…éƒ¨å·¥å…·ã€‚"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])


def clear_chat_history():
    st.session_state.messages = [
        {"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä¼ä¸šèŒèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥AIå¯¹è¯ ä¹Ÿ å¯ä»¥è°ƒç”¨å†…éƒ¨å·¥å…·ã€‚"}]

    global session
    session = SQLiteSession("conversation_123")


st.sidebar.button('æ¸…ç©ºèŠå¤©', on_click=clear_chat_history)

async def get_model_response(prompt, model_name, use_tool):
    async with MCPServerSse(
            name="SSE Python Server",
            params={
                "url": "http://localhost:8900/sse",
            },
            client_session_timeout_seconds=20
    )as mcp_server:
        external_client = AsyncOpenAI(
            api_key=key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        if use_tool:
            # tools = asyncio.run(_fetch_tools())
            # abandon_tools = [tool for tool in tool_names if tool not in selected_list]

            agent = FilteredAgent(
                name="Assistant",
                instructions="",
                mcp_servers=[mcp_server],
                allowed_mcp_tool_names = selected_list,
                # stop_at_tool_names = abandon_tools,
                model=OpenAIChatCompletionsModel(
                    model=model_name,
                    openai_client=external_client,
                ),
                hooks = MyAgentHooks(),  # â† æ³¨å†Œé’©å­
            )
        else:
            agent = FilteredAgent(
                name="Assistant",
                instructions="",
                model=OpenAIChatCompletionsModel(
                    model=model_name,
                    openai_client=external_client,
                )
            )

        result = Runner.run_streamed(agent, input=prompt, session=session)
        async for event in result.stream_events():
            if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
                yield event.data.delta


if len(key) > 1:
    if prompt := st.chat_input():
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            with st.spinner("è¯·æ±‚ä¸­..."):
                try:
                    response_generator = get_model_response(prompt, model_name, use_tool)

                    async def stream_and_accumulate(generator):
                        accumulated_text = ""
                        async for chunk in generator:
                            accumulated_text += chunk
                            message_placeholder.markdown(accumulated_text + "â–Œ")
                        return accumulated_text

                    full_response = asyncio.run(stream_and_accumulate(response_generator))
                    message_placeholder.markdown(full_response)

                except Exception as e:
                    error_message = f"å‘ç”Ÿé”™è¯¯: {e}"
                    message_placeholder.error(error_message)
                    full_response = error_message
                    print(f"Error during streaming: {e}")

            # 4. å°†å®Œæ•´çš„åŠ©æ‰‹å›å¤æ·»åŠ åˆ° session state
            st.session_state.messages.append({"role": "assistant", "content": full_response})
